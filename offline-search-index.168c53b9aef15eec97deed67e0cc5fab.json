[{"body":"","excerpt":"","ref":"/docs/reference/api/","title":"API"},{"body":"Overview Developing and deploying an application to Verrazzano consists of:\n Packaging the application as a Docker image. Publishing the application’s Docker image to a container registry. Applying the application’s Verrazzano components to the cluster. Applying the application’s Verrazzano applications to the cluster.  This guide does not provide the full details for the first two steps. An existing example application Docker image has been packaged and published for use.\nVerrazzano supports application definition using Open Application Model (OAM). Verrrazzano applications are composed of components and application configurations. This document demonstrates creating OAM resources that define an application as well as the steps required to deploy those resources.\nWhat you need   About 10 minutes.\n  Access to an existing Kubernetes cluster with Verrazzano installed.\n  Access to the application’s image in GitHub Container Registry.\nConfirm access using this command to pull the example’s Docker image:\n$ docker pull ghcr.io/verrazzano/example-helidon-greet-app-v1:0.1.12-1-20210218160249-d8db8f3   Application development This guide uses an example application which was written with Java and Helidon. For the implementation details, see the Helidon MP tutorial. See the application source code in the Verrazzano examples repository.\nThe example application is a JAX-RS service and implements the following REST endpoints:\n /greet - Returns a default greeting message that is stored in memory. This endpoint accepts the GET HTTP request method. /greet/{name} - Returns a greeting message including the name provided in the path parameter. This endpoint accepts the GET HTTP request method. /greet/greeting - Changes the greeting message to be used in future calls to the other endpoints. This endpoint accepts the PUT HTTP request method and a JSON payload.  The following code shows a portion of the application’s implementation. The Verrazzano examples repository contains the complete implementation. An important detail here is that the application contains a single resource exposed on path /greet.\npackage io.helidon.examples.quickstart.mp; ... @Path(\"/greet\") @RequestScoped public class GreetResource { @GET @Produces(MediaType.APPLICATION_JSON) public JsonObject getDefaultMessage() { ... } @Path(\"/{name}\") @GET @Produces(MediaType.APPLICATION_JSON) public JsonObject getMessage(@PathParam(\"name\") String name) { ... } @Path(\"/greeting\") @PUT @Consumes(MediaType.APPLICATION_JSON) ... public Response updateGreeting(JsonObject jsonObject) { ... } } A Dockerfile is used to package the completed application JAR file into a Docker image. The following code shows a portion of the Dockerfile. The Verrazzano examples repository contains the complete Dockerfile. Note that the Docker container exposes a single port 8080.\nFROMghcr.io/oracle/oraclelinux:7-slim...CMD java -cp /app/helidon-quickstart-mp.jar:/app/* io.helidon.examples.quickstart.mp.MainEXPOSE8080Application deployment When you deploy applications with Verrazzano, the platform sets up connections, network policies, and ingresses in the service mesh, and wires up a monitoring stack to capture the metrics, logs, and traces. Verrazzano employs OAM components to define the functional units of a system that are then assembled and configured by defining associated application configurations.\nVerrazzano components A Verrazzano OAM component is a Kubernetes Custom Resource describing an application’s general composition and environment requirements. The following code shows the component for the example application used in this guide. This resource describes a component which is implemented by a single Docker image containing a Helidon application exposing a single endpoint.\napiVersion:core.oam.dev/v1alpha2kind:Componentmetadata:name:hello-helidon-componentnamespace:hello-helidonspec:workload:apiVersion:oam.verrazzano.io/v1alpha1kind:VerrazzanoHelidonWorkloadmetadata:name:hello-helidon-workloadlabels:app:hello-helidonspec:deploymentTemplate:metadata:name:hello-helidon-deploymentpodSpec:containers:- name:hello-helidon-containerimage:\"ghcr.io/verrazzano/example-helidon-greet-app-v1:0.1.10-3-20201016220428-56fb4d4\"ports:- containerPort:8080name:httpA brief description of each field of the component:\n apiVersion - Version of the component custom resource definition kind - Standard name of the component custom resource definition metadata.name - The name used to create the component’s custom resource metadata.namespace - The namespace used to create this component’s custom resource spec.workload.kind - VerrazzanoHelidonWorkload defines a stateless workload of Kubernetes spec.workload.spec.deploymentTemplate.podSpec.metadata.name - The name used to create the stateless workload of Kubernetes spec.workload.spec.deploymentTemplate.podSpec.containers - The implementation containers spec.workload.spec.deploymentTemplate.podSpec.containers.ports - Ports exposed by the container  Verrazzano application configurations A Verrazzano application configuration is a Kubernetes Custom Resource which provides environment specific customizations. The following code shows the application configuration for the example used in this guide. This resource specifies the deployment of the application to the hello-helidon namespace. Additional runtime features are specified using traits, or runtime overlays that augment the workload. For example, the ingress trait specifies the ingress host and path, while the metrics trait provides the Prometheus scraper used to obtain the application related metrics.\napiVersion:core.oam.dev/v1alpha2kind:ApplicationConfigurationmetadata:name:hello-helidon-appconfnamespace:hello-helidonannotations:version:v1.0.0description:\"Hello Helidon application\"spec:components:- componentName:hello-helidon-componenttraits:- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:MetricsTraitspec:scraper:verrazzano-system/vmi-system-prometheus-0- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:IngressTraitmetadata:name:hello-helidon-ingressspec:rules:- paths:- path:\"/greet\"pathType:PrefixA brief description of each field in the application configuration:\n apiVersion - Version of the ApplicationConfiguration custom resource definition kind - Standard name of the application configuration custom resource definition metadata.name - The name used to create this application configuration resource metadata.namespace - The namespace used for this application configuration custom resource spec.components - Reference to the application’s components leveraged to specify runtime configuration spec.components[].traits - The traits specified for the application’s components  To explore traits, we can examine the fields of an ingress trait:\n apiVersion - Version of the OAM trait custom resource definition kind - IngressTrait is the name of the OAM application ingress trait custom resource definition spec.rules.paths - The context paths for accessing the application  Deploy the application The following steps are required to deploy the example application. Steps similar to the apply steps would be used to deploy any application to Verrazzano.\n  Create a namespace for the example application and add labels identifying the namespace as managed by Verrazzano and enabled for Istio.\n$ kubectl create namespace hello-helidon $ kubectl label namespace hello-helidon verrazzano-managed=true istio-injection=enabled   Apply the application’s component.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-comp.yaml This step causes the validation and creation of the component resource. No other resources or objects are created as a result. Application configurations applied in the future may reference this component resource.\n  Apply the application configuration.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-app.yaml This step causes the validation and creation of the application configuration resource. This operation triggers the activation of a number of Verrazzano operators. These operators create other Kubernetes objects (for example, Deployments, ReplicaSets, Pods, Services, Ingresses) that collectively provide and support the application.\n  Configure the application’s DNS resolution.\nAfter deploying the application, configure DNS to resolve the application’s ingress DNS name to the application’s load balancer IP address. The generated host name is obtained by querying Kubernetes for the gateway:\n$ kubectl get gateway hello-helidon-hello-helidon-appconf-gw \\  -n hello-helidon \\  -o jsonpath='{.spec.servers[0].hosts[0]}' The load balancer IP is obtained by querying Kubernetes for the Istio ingress gateway status:\n$ kubectl get service \\  -n istio-system istio-ingressgateway \\  -o jsonpath='{.status.loadBalancer.ingress[0].ip}' DNS configuration steps are outside the scope of this guide. For DNS infrastructure that can be configured and used, see the Oracle Cloud Infrastructure DNS documentation. In some small non-production scenarios, DNS configuration using /etc/hosts or an equivalent may be sufficient.\n  Verify the deployment Applying the application configuration initiates the creation of several Kubernetes objects. Actual creation and initialization of these objects occurs asynchronously. The following steps provide commands for determining when these objects are ready for use.\nNote: Many other Kubernetes objects unrelated to the example application may also exist. Those have been omitted from the lists.\n  Verify the Helidon application pod is running.\n$ kubectl get pods -n hello-helidon | grep '^NAME\\|hello-helidon-deployment' NAME READY STATUS RESTARTS AGE hello-helidon-deployment-78468f5f9c-czmp4 3/3 Running 0 22h The parameter hello-helidon-deployment is from the component’s spec.workload.spec.deploymentTemplate.podSpec.metadata.name value.\n  Verify the Verrazzano application operator pod is running.\n$ kubectl get pods -n verrazzano-system | grep '^NAME\\|verrazzano-application-operator' NAME READY STATUS RESTARTS AGE verrazzano-application-operator-5485967588-lp6cw 1/1 Running 0 8d The namespace verrazzano-system is used by Verrazzano for non-application objects managed by Verrazzano. A single verrazzano-application-operator manages the life cycle of all OAM based applications within the cluster.\n  Verify the Verrazzano monitoring infrastructure is running.\n$ kubectl get pods -n verrazzano-system | grep '^NAME\\|vmi-system' NAME READY STATUS RESTARTS AGE vmi-system-es-master-0 2/2 Running 0 26h vmi-system-grafana-74bb7cdf65-k97pb 2/2 Running 0 26h vmi-system-kibana-85565975b5-7hfdf 2/2 Running 0 26h vmi-system-prometheus-0-7bf464d898-czq8r 4/4 Running 0 26h These pods in the verrazzano-system namespace constitute a monitoring stack created by Verrazzano for the deployed applications.\nThe monitoring infrastructure comprises several components:\n vmi-system-api - Internal API for configuring monitoring vmi-system-es - Elasticsearch for log collection vmi-system-kibana - Kibana for log visualization vmi-system-grafana - Grafana for metric visualization vmi-system-prometheus - Prometheus for metric collection     Diagnose failures.\nView the event logs of any pod not entering the Running state within a reasonable length of time, such as five minutes.\n$ kubectl describe pod -n hello-helidon hello-helidon-deployment-78468f5f9c-czmp4 Use the specific namespace and name for the pod being investigated.\n  Explore the application Follow these steps to explore the application’s functionality. If DNS was not configured, then use the alternative commands.\n  Save the host name and IP address of the load balancer exposing the application’s REST service endpoints for later.\n$ HOST=$(kubectl get gateway hello-helidon-hello-helidon-appconf-gw \\  -n hello-helidon \\  -o jsonpath='{.spec.servers[0].hosts[0]}') $ ADDRESS=$(kubectl get service \\  -n istio-system istio-ingressgateway \\  -o jsonpath='{.status.loadBalancer.ingress[0].ip}') NOTE:\n The value of ADDRESS is used only if DNS has not been configured. The following alternative commands may not work in conjunction with firewalls that validate HTTP Host headers.    Get the default message.\n$ curl -sk \\  -X GET \\  \"https://${HOST}/greet\" {\"message\":\"Hello World!\"} If DNS has not been configured, then use this command.\n$ curl -sk \\  -X GET \\  \"https://${HOST}/greet\" \\  --resolve ${HOST}:443:${ADDRESS}   Get a message for Robert.\n$ curl -sk \\  -X GET \\  \"https://${HOST}/greet/Robert\" {\"message\":\"Hello Robert!\"} If DNS has not been configured, then use this command.\n$ curl -sk \\  -X GET \"https://${HOST}/greet/Robert\" \\  --resolve ${HOST}:443:${ADDRESS}   Update the default greeting.\n$ curl -sk \\  -X PUT \\  \"https://${HOST}/greet/greeting\" \\  -H 'Content-Type: application/json' \\  -d '{\"greeting\" : \"Greetings\"}' If DNS has not been configured, then use this command.\n$ curl -sk \\  -X PUT \\  \"https://${HOST}/greet/greeting\" \\  -H 'Content-Type: application/json' \\  -d '{\"greeting\" : \"Greetings\"}' \\  --resolve ${HOST}:443:${ADDRESS}   Get the new message for Robert.\n$ curl -sk \\  -X GET \\  \"https://${HOST}/greet/Robert\" {\"message\":\"Greetings Robert!\"} If DNS has not been configured, then use this command.\n$ curl -sk \\  -X GET \\  \"https://${HOST}/greet/Robert\" \\  --resolve ${HOST}:443:${ADDRESS}   Access the application’s logs Deployed applications have log collection enabled. These logs are collected using Elasticsearch and can be accessed using Kibana. Elasticsearch and Kibana are examples of infrastructure Verrazzano creates in support of an application as a result of applying an application configuration.\nDetermine the URL to access Kibana:\n$ KIBANA_HOST=$(kubectl get ingress \\  -n verrazzano-system vmi-system-kibana \\  -o jsonpath='{.spec.rules[0].host}') $ KIBANA_URL=\"https://${KIBANA_HOST}\" $ echo \"${KIBANA_URL}\" $ open \"${KIBANA_URL}\" The user name to access Kibana defaults to verrazzano during the Verrazzano installation.\nDetermine the password to access Kibana:\n$ echo $(kubectl get secret \\  -n verrazzano-system verrazzano \\  -o jsonpath={.data.password} | base64 \\  --decode) Access the application’s metrics Deployed applications have metric collection enabled. Grafana can be used to access these metrics collected by Prometheus. Prometheus and Grafana are additional components Verrazzano creates as a result of applying an application configuration.\nDetermine the URL to access Grafana:\n$ GRAFANA_HOST=$(kubectl get ingress \\  -n verrazzano-system vmi-system-grafana \\  -o jsonpath='{.spec.rules[0].host}') $ GRAFANA_URL=\"https://${GRAFANA_HOST}\" $ echo \"${GRAFANA_URL}\" $ open \"${GRAFANA_URL}\" The user name to access Grafana is set to the default value verrazzano during the Verrazzano installation.\nDetermine the password to access Grafana:\n$ echo $(kubectl get secret \\  -n verrazzano-system verrazzano \\  -o jsonpath={.data.password} | base64 \\  --decode) Alternatively, metrics can be accessed directly using Prometheus. Determine the URL for this access:\n$ PROMETHEUS_HOST=$(kubectl get ingress \\  -n verrazzano-system vmi-system-prometheus \\  -o jsonpath='{.spec.rules[0].host}') $ PROMETHEUS_URL=\"https://${PROMETHEUS_HOST}\" $ echo \"${PROMETHEUS_URL}\" $ open \"${PROMETHEUS_URL}\" The user name and password for both Prometheus and Grafana are the same.\nRemove the application Run the following commands to delete the application configuration, and optionally the component and namespace.\n  Delete the application configuration.\n$ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-app.yaml The deletion of the application configuration will result in the destruction of all application-specific Kubernetes objects.\n  (Optional) Delete the application’s component.\n$ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-comp.yaml Note: This step is not required if other application configurations for this component will be applied in the future.\n  (Optional) Delete the namespace.\n$ kubectl delete namespace hello-helidon   ","excerpt":"Overview Developing and deploying an application to Verrazzano consists of:\n Packaging the application as a Docker image. Publishing the application’s Docker image to a container registry. Applying …","ref":"/docs/samples/application-deployment-guide/","title":"Application Deployment Guide"},{"body":"During application deployment, the oam-kubernetes-runtime and verrazzano-application-operator cooperate through the generation and update of Kubernetes resources. The oam-kubernetes-runtime processes the ApplicationConfiguration and Component resources provided by the user and generates workload and trait resources. The verrazzano-application-operator processes Verrazzano specific workload and trait resources. These are then used to generate additional child and related resources.\nTroubleshooting application deployments should follow three general steps:\n Review the status of the oam-kubernetes-runtime and verrazzano-application-operator operator pods. Review the logs of the oam-kubernetes-runtime and verrazzano-application-operator operator pods. Review the resources generated by the oam-kubernetes-runtime and the verrazzano-application-operator.  Review oam-kubernetes-runtime operator status For application deployment to succeed, the oam-kubernetes-runtime pod must have a status of Running.\nUse the following command to get the pod status.\n$ kubectl get pods \\  -n verrazzano-system \\  -l app.kubernetes.io/name=oam-kubernetes-runtime If the pod status is not Running, then see the following instructions for reviewing the oam-kubernetes-runtime pod logs.\nReview verrazzano-application-operator operator status For application deployment to succeed, the verrazzano-application-operator pod must have a status of Running.\nUse the following command to get the pod status.\n$ kubectl get pods \\  -n verrazzano-system \\  -l app=verrazzano-application-operator If the pod status is not Running, then see the following instructions for reviewing the verrazzano-application-operator logs.\nReview oam-kubernetes-runtime operator logs Review the oam-kubernetes-runtime pod logs for any indication that pod startup or the generation of workloads or traits has failed.\nUse the following command to get the logs.\n$ kubectl logs \\  -n verrazzano-system \\  -l app.kubernetes.io/name=oam-kubernetes-runtime Review verrazzano-application-operator logs Review the verrazzano-application-operator logs for any indication that pod startup or resource generation has failed.\nUse the following command to get the logs.\n$ kubectl logs \\  -n verrazzano-system \\  -l app=verrazzano-application-operator Review generated workload resources The processing of a Component reference within an ApplicationConfiguration results in the generation of workloads. For example, a referenced Component might result in the generation of a VerrazzanoHelidonWorkload workload resource. In turn, the VerrazzanoHelidonWorkload workload resource will be processed and result in the generation of related Deployment and Service resources.\nIf the expected workload resource, for example VerrazzanoHelidonWorkload, is missing, then review the oam-kubernetes-runtime logs. If the expected related resources, for example Deployment or Service, are missing, then review the verrazzano-application-operator logs.\nThe following commands are examples of checking for the resources related to a VerrazzanoHelidonWorkload deployment.\n$ kubectl get -n hello-helidon verrazzanohelidonworkload hello-helidon-workload $ kubectl get -n hello-helidon deployment hello-helidon-deployment $ kubectl get -n hello-helidon service hello-helidon-deployment Review generated trait resources The processing of traits embedded with an ApplicationConfiguration results in the generation of trait resources. For example, an IngressTrait embedded within an ApplicationConfiguration will result in the generation of an IngressTrait resource. In turn, the IngressTrait resource will be processed and result in the generation of related Certificate, Gateway, and VirtualService resources.\nIf the expected trait resource, for example IngressTrait, is missing, then review the oam-kubernetes-runtime logs. If the expected related resources, for example Certificate, Gateway, and VirtualService, are missing, then review the verrazzano-application-operator logs.\nThe following commands are examples of checking for the resources related to an IngressTrait.\n$ kubectl get -n hello-helidon ingresstrait hello-helidon-ingress $ kubectl get -n istio-system Certificate hello-helidon-hello-helidon-appconf-cert $ kubectl get -n hello-helidon gateway hello-helidon-hello-helidon-appconf-gw $ kubectl get -n hello-helidon virtualservice hello-helidon-ingress-rule-0-vs Check for RBAC privilege issues The use of generic Kubernetes resources as workloads and traits can result in deployment failures if privileges are insufficient. In this case, the oam-kubernetes-runtime logs will contain errors containing the term forbidden.\nThe following command shows how to query for this type of failure message.\n$ kubectl logs \\  -n verrazzano-system \\  -l app.kubernetes.io/name=oam-kubernetes-runtime | grep forbidden Check resource owners Kubernetes maintains the child to parent relationship within metadata fields.\nThe following example returns the parent of the IngressTrait, named hello-helidon-ingress, in the hello-helidon namespace.\n$ kubectl get IngressTrait \\  -n hello-helidon hello-helidon-ingress \\  -o jsonpath='{range .metadata.ownerReferences[*]}{.name}{\"\\n\"}{end}' The results of this command can help identify the lineage of a given resource.\nCheck related resources Some resources also record the related resources affected during their processing. For example, when processed, an IngressTrait will create related Gateway, VirtualService, and Certificate resources.\nThe following command is an example of how to obtain the related resources of an IngressTraits.\n$ kubectl get IngressTrait \\  -n hello-helidon hello-helidon-ingress \\  -o jsonpath='{range .status.resources[*]}{.kind}: {.name}{\"\\n\"}{end}' The results of this command can help identify which other resources, the given resource affected.\n","excerpt":"During application deployment, the oam-kubernetes-runtime and verrazzano-application-operator cooperate through the generation and update of Kubernetes resources. The oam-kubernetes-runtime processes …","ref":"/docs/troubleshooting/troubleshooting-application-deployment/","title":"Application Deployment"},{"body":"","excerpt":"","ref":"/docs/concepts/","title":"Concepts"},{"body":" NOTE You should install this developer preview release of Verrazzano only in a cluster that can be safely deleted when your evaluation is complete.  The following instructions show you how to install Verrazzano in a single Kubernetes cluster.\nPrerequisites Verrazzano requires the following:\n A Kubernetes cluster and a compatible kubectl. At least 2 CPUs, 100GB disk storage, and 16GB RAM available on the Kubernetes worker nodes. This is sufficient to install the development profile of Verrazzano. Depending on the resource requirements of the applications you deploy, this may or may not be sufficient for deploying your applications.  For a list of the open source components and versions installed with Verrazzano, see Software Versions.\nNOTE Verrazzano has been tested only on the following versions of Kubernetes: 1.17.x, 1.18.x, 1.19.x, and 1.20x. Other versions have not been tested and are not guaranteed to work.  Prepare for the install Before installing Verrazzano, see instructions on preparing Kubernetes platforms.\nNOTE: Verrazzano can create network policies that can be used to limit the ports and protocols that pods use for network communication. Network policies provide additional security but they are enforced only if you install a Kubernetes Container Network Interface (CNI) plug-in that enforces them, such as Calico. For instructions on how to install a CNI plug-in, see the documentation for your Kubernetes cluster.\nInstall the Verrazzano platform operator Verrazzano provides a platform operator to manage the life cycle of Verrazzano installations. Using the Verrazzano custom resource, you can install, uninstall, and upgrade Verrazzano installations.\nTo install the Verrazzano platform operator:\n  Deploy the Verrazzano platform operator.\n$ kubectl apply -f https://github.com/verrazzano/verrazzano/releases/download/v0.17.0/operator.yaml   Wait for the deployment to complete.\n$ kubectl -n verrazzano-install rollout status deployment/verrazzano-platform-operator deployment \"verrazzano-platform-operator\" successfully rolled out   Confirm that the operator pod is correctly defined and running.\n$ kubectl -n verrazzano-install get pods NAME READY STATUS RESTARTS AGE verrazzano-platform-operator-59d5c585fd-lwhsx 1/1 Running 0 114s   Perform the install Verrazzano supports the following installation profiles: development (dev), production (prod), and managed cluster (managed-cluster). For more information, see Installation Profiles.\nTo change profiles in any of the following commands, set the VZ_PROFILE environment variable to the name of the profile you want to install.\nFor a complete description of Verrazzano configuration options, see the Verrazzano Custom Resource Definition.\nAccording to your DNS choice, nip.io (wildcard DNS) or Oracle OCI DNS, install Verrazzano using one of the following methods:\n nip.io OCI DNS   Install using nip.io Run the following commands:\n$ kubectl apply -f - \u003c\u003cEOF apiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: my-verrazzano spec: profile: ${VZ_PROFILE:-dev} EOF $ kubectl wait \\  --timeout=20m \\  --for=condition=InstallComplete verrazzano/my-verrazzano   Install using OCI DNS Prerequisites\n  A DNS zone is a distinct portion of a domain namespace. Therefore, ensure that the zone is appropriately associated with a parent domain. For example, an appropriate zone name for parent domain v8o.example.com domain is us.v8o.example.com.\n  Create a public OCI DNS zone using the OCI CLI or the OCI Console.\nTo create an OCI DNS zone using the OCI CLI:\n$ oci dns zone create \\ -c \u003ccompartment ocid\u003e \\ --name \u003czone-name-prefix\u003e.v8o.example.com \\ --zone-type PRIMARY To create an OCI DNS zone using the OCI console, see Managing DNS Service Zones.\n  Create a secret in the default namespace. The secret is created using the script create_oci_config_secret.sh which reads an OCI configuration file to create the secret.\nDownload the create_oci_config_secret.sh script:\n$ curl \\ -o ./create_oci_config_secret.sh \\ https://raw.githubusercontent.com/verrazzano/verrazzano/master/platform-operator/scripts/install/create_oci_config_secret.sh Run the create_oci_config_secret.sh script:\n$ chmod +x create_oci_config_secret.sh $ export KUBECONFIG=\u003ckubeconfig-file\u003e $ ./create_oci_config_secret.sh \\ -o \u003coci-config-file\u003e \\ -s \u003cconfig-file-section\u003e \\ -k \u003csecret-name\u003e -o defaults to the OCI configuration file in ~/.oci/config -s defaults to the DEFAULT properties section within the OCI configuration file -k defaults to a secret named oci   NOTE The key_file value within the OCI configuration file must reference a .pem file that contains a RSA private key. The contents of a RSA private key file starts with -----BEGIN RSA PRIVATE KEY-----. If your OCI configuration file references a .pem file that is not of this form, then you must generate a RSA private key file. See Generating a RSA Private Key. After generating the correct form of the .pem file, make sure to change the reference within the OCI configuration file.  Installation\nInstalling Verrazzano using OCI DNS requires some configuration settings to create DNS records.\nDownload the sample Verrazzano custom resource install-oci.yaml for OCI DNS:\n$ curl \\ -o ./install-oci.yaml \\ https://raw.githubusercontent.com/verrazzano/verrazzano/master/platform-operator/config/samples/install-oci.yaml Edit the downloaded install-oci.yaml file and provide values for the following configuration settings:\n spec.environmentName spec.certificate.acme.emailAddress spec.dns.oci.ociConfigSecret spec.dns.oci.dnsZoneCompartmentOCID spec.dns.oci.dnsZoneOCID spec.dns.oci.dnsZoneName  For the full configuration information for an installation, see the Verrazzano Custom Resource Definition.\nWhen you use the OCI DNS installation, you need to provide a Verrazzano name in the Verrazzano custom resource (spec.environmentName) that will be used as part of the domain name used to access Verrazzano ingresses. For example, you could use sales as an environmentName, yielding sales.us.v8o.example.com as the sales-related domain (assuming the domain and zone names listed previously).\nRun the following commands:\n$ kubectl apply -f ./install-oci.yaml $ kubectl wait \\ --timeout=20m \\ --for=condition=InstallComplete verrazzano/my-verrazzano    To monitor the console log output of the installation:\n$ kubectl logs \\  -f $(kubectl get pod \\  -l job-name=verrazzano-install-my-verrazzano \\  -o jsonpath=\"{.items[0].metadata.name}\") Verify the install Verrazzano installs multiple objects in multiple namespaces. In the verrazzano-system namespaces, all the pods in the Running state, does not guarantee, but likely indicates that Verrazzano is up and running.\n$ kubectl get pods -n verrazzano-system coherence-operator-controller-manager-7557bc4c49-7w55p 1/1 Running 0 27h fluentd-fzmsl 1/1 Running 0 27h fluentd-r9wwf 1/1 Running 0 27h fluentd-zp2r2 1/1 Running 0 27h oam-kubernetes-runtime-6ff589f66f-r95qv 1/1 Running 0 27h verrazzano-api-669c7d7f66-rcnl8 1/1 Running 0 27h verrazzano-application-operator-b5b77d676-7w95p 1/1 Running 0 27h verrazzano-console-6b469dff9c-b2jwk 1/1 Running 0 27h verrazzano-monitoring-operator-54cb658774-f6jjm 1/1 Running 0 27h verrazzano-operator-7f4b99d7d-wg7qm 1/1 Running 0 27h vmi-system-es-master-0 2/2 Running 0 27h vmi-system-grafana-74bb7cdf65-k97pb 2/2 Running 0 27h vmi-system-kibana-85565975b5-7hfdf 2/2 Running 0 27h vmi-system-prometheus-0-7bf464d898-czq8r 4/4 Running 0 27h weblogic-operator-7db5cdcf59-qxsr9 1/1 Running 0 27h (Optional) Run the example applications Example applications are located here.\nTo get the consoles URLs and credentials, see Access Verrazzano. ","excerpt":" NOTE You should install this developer preview release of Verrazzano only in a cluster that can be safely deleted when your evaluation is complete.  The following instructions show you how to install …","ref":"/docs/setup/install/installation/","title":"Install Guide"},{"body":"The Verrazzano logging stack consists of Fluentd, Elasticsearch, and Kibana components.\n Fluentd: a log aggregator that collects, processes, and formats logs from Kubernetes clusters. Elasticsearch: a scalable search and analytics engine for storing Kubernetes logs. Kibana: a visualization layer that provides a user interface to query and visualize collected logs.  As shown in the following diagram, logs written to stdout by a container running on Kubernetes are picked up by the kubelet service running on that node and written to /var/log/containers.\nFluentd sidecar For components with multiple log streams or that cannot log to stdout, Verrazzano deploys a Fluentd sidecar which parses and translates the log stream. The resulting log is sent to stdout of the sidecar container and then written to /var/log/containers by the kubelet service.\nFor example, in a WebLogic deployment, AdminServer.log is consumed, translated, and written to stdout by the Fluentd sidecar. You can view these logs using kubectl on the container named fluentd-stdout-sidecar.\n$ kubectl logs tododomain-adminserver \\  -n todo-list \\  -c fluentd-stdout-sidecar Fluentd DaemonSet Verrazzano deploys a Fluentd DaemonSet which runs one Fluentd replica per node in the verrazzano-system namespace. Each instance pulls logs from the node’s /var/log/containers directory and writes them to the target Elasticsearch index. The index name is based on the namespace associated with the record, using this format: verrazzano-namespace-\u003crecord namespace\u003e.\nFor example, vmi-system-kibana logs written to /var/log/containers will be pulled by Fluentd and written to Elasticsearch. The index used is named verrazzano-namespace-verrazzano-system because the VMI runs in the verrazzano-system namespace.\nThe same approach is used for both system and application logs.\nElasticsearch Verrazzano creates an Elasticsearch deployment as the store and search engine for the logs processed by Fluentd. Records written by Fluentd can be queried using the Elasticsearch REST API.\nFor example, you can use curl to get all of the Elasticsearch indexes. First, you must get the password for the verrazzano user and the host for the Elasticsearch VMI.\n$ PASS=$(kubectl get secret \\  --namespace verrazzano-system verrazzano \\  -o jsonpath={.data.password} | base64 \\  --decode; echo) $ HOST=$(kubectl get ingress \\  -n verrazzano-system vmi-system-es-ingest \\  -o jsonpath={.spec.rules[0].host}) $ curl -ik \\  --user verrazzano:$PASS https://$HOST//_cat/indices To see all of the records for a specific index, do the following:\n$ INDEX=verrazzano-namespace-todo-list $ curl -ik \\  --user verrazzano:$PASS https://$HOST/$INDEX/_doc/_search?q=message:* Verrazzano provides support for Installation Profiles. The production profile (prod), which is the default, provides a 3-node Elasticsearch and persistent storage for the Verrazzano Monitoring Instance (VMI). The development profile (dev) provides a single node Elasticsearch and no persistent storage for the VMI. The managed-cluster profile does not install Elasticsearch or Kibana in the local cluster; all logs are forwarded to the admin cluster’s Elasticsearch instance.\nKibana Kibana is a visualization dashboard for the content indexed on an Elasticsearch cluster. Verrazzano creates a Kibana deployment to provide a user interface for querying and visualizing the log data collected in Elasticsearch.\nTo access the Kibana console, read Access Verrazzano.\nTo see the records of an Elasticsearch index through Kibana, create an index pattern to filter for records under the desired index.\nFor example, to see the log records of a WebLogic application deployed to the todo-list namespace, create an index pattern of verrazzano-namespace-todo-list.\n","excerpt":"The Verrazzano logging stack consists of Fluentd, Elasticsearch, and Kibana components.\n Fluentd: a log aggregator that collects, processes, and formats logs from Kubernetes clusters. Elasticsearch: a …","ref":"/docs/monitoring/logs/","title":"Logging"},{"body":"Verrazzano may be installed in a multicluster environment, consisting of an admin cluster and optionally, one or more managed clusters.\n The admin cluster is a central point from which Verrazzano applications in managed clusters can be deployed and monitored. Managed clusters are registered with an admin cluster. Verrazzano multicluster resources are used to target applications to any cluster in a multicluster Verrazzano environment.  The following diagram shows a high-level overview of how multicluster Verrazzano works. For a more detailed view, see the diagram here.\nAdmin cluster A Verrazzano admin cluster is a central management point for:\n Deploying and undeploying applications to the managed clusters registered with the admin cluster. Viewing logs and metrics for both Verrazzano components and applications that reside in the managed clusters.  You may register one or more managed clusters with the admin cluster by creating a VerrazzanoManagedCluster resource in the verrazzano-mc namespace of an admin cluster.\nNote: The admin cluster has a fully functional Verrazzano installation. You can locate applications on the admin cluster as well as on managed clusters.\nManaged clusters A Verrazzano managed cluster has a minimal footprint of Verrazzano, installed using the managed-cluster installation profile. A managed cluster has the following additional characteristics:\n It is registered with an admin cluster with a unique name. Logs for Verrazzano system components and Verrazzano multicluster applications are sent to Elasticsearch running on the admin cluster, and are viewable from that cluster. A Verrazzano multicluster Kubernetes resource, created on the admin cluster, will be retrieved and deployed to a managed cluster if all of the following are true:  The resource is in a namespace governed by a VerrazzanoProject. The VerrazzanoProject has a placement value that includes this managed cluster. The resource itself has a placement value that includes this managed cluster.    Verrazzano multicluster resources Verrazzano includes several multicluster resource definitions for resources that may be targeted for placement in one or more clusters: MultiClusterApplicationConfiguration, MultiClusterComponent, MultiClusterConfigMap, and MultiClusterSecret.\n Each multicluster resource type serves as a wrapper for an underlying resource type. A multicluster resource additionally allows the placement of the underlying resource to be specified as a list of names of the clusters in which the resource must be placed. Multicluster resources are created in the admin cluster, in a namespace that is part of a VerrazzanoProject, and targeted for placement in either the local admin cluster or a remote managed cluster. A multicluster resource is said to be part of a VerrazzanoProject if it is in a namespace that is governed by that VerrazzanoProject.  Managed cluster registration A managed cluster may be registered with an admin cluster using a two-step process:\nStep 1: Create a VerrazzanoManagedCluster resource in the verrazzano-mc namespace of the admin cluster.\nStep 2: Retrieve the Kubernetes manifest file generated in the VerrazzanoManagedCluster resource and apply it on the managed cluster to complete the registration.\nWhen a managed cluster is registered, the following will happen:\n After both steps of the registration are complete, the managed cluster begins polling the admin cluster for VerrazzanoProject resources and multicluster resources, which specify a placement in this managed cluster.  Any VerrazzanoProject resources placed in this managed cluster are retrieved, and the corresponding namespaces and security permissions (RoleBindings) are created in the managed cluster. Any multicluster resources that are placed in this managed cluster, and are in a VerrazzanoProject that is also placed in this managed cluster, are retrieved, and created or updated on the managed cluster. The underlying resource represented by the multicluster resource is unwrapped, and created or updated on the managed cluster. The managed cluster namespace of the multicluster resource and its underlying resource matches the admin cluster namespace of the multicluster resource.   When the managed cluster connects to the admin cluster, it updates the VerrazzanoManagedCluster resource for this managed cluster with:  The endpoint URL that the admin cluster should use to scrape Prometheus metrics from the managed cluster. The date and time of the most recent successful connection from the managed cluster to the admin cluster.   For MultiClusterApplicationConfigurations retrieved and unwrapped on a managed cluster, the application logs are sent to Elasticsearch on the admin cluster, and may be viewed from the Verrazzano-installed Kibana UI on the admin cluster. Likewise, application metrics will be scraped by the admin cluster and available from Verrazzano-installed Prometheus on the admin cluster.  Detailed view of multicluster Verrazzano This diagram shows a detailed view of how multicluster Verrazzano works.\nTry out multicluster Verrazzano For more information, see the API Documentation for the resources described here.\nTo try out multicluster Verrazzano, see the Multicluster examples.\n","excerpt":"Verrazzano may be installed in a multicluster environment, consisting of an admin cluster and optionally, one or more managed clusters.\n The admin cluster is a central point from which Verrazzano …","ref":"/docs/applications/multicluster/","title":"Multicluster"},{"body":"This document describes some common problems you might encounter when using multicluster Verrazzano, and how to troubleshoot them.\nIf you created multicluster resources in the admin cluster, and specified a placement value in a managed cluster, then those resources will get created in that managed cluster. If they do not get created in the managed cluster, then use the following steps to troubleshoot:\n Verify that the managed cluster is registered correctly and can connect to the admin cluster. Verify that the VerrazzanoProject for the resource’s namespace, also has a placement in that managed cluster. Check the multicluster resource’s status field on the admin cluster to know what the status of that resource is on each managed cluster to which it is targeted.  Verify managed cluster registration and connectivity You can verify that a managed cluster was successfully registered with an admin cluster by viewing the corresponding VerrazzanoManagedCluster (VMC) resource on the admin cluster. For example, to verify that a managed cluster named managed1 was successfully registered:\n# on the admin cluster $ kubectl get verrazzanomanagedcluster managed1 \\  -n verrazzano-mc \\  -o yaml Partial sample output from the previous command:\n status: conditions: - lastTransitionTime: \"2021-06-22T21:03:27Z\" message: Ready status: \"True\" type: Ready lastAgentConnectTime: \"2021-06-22T21:06:04Z\" ... other fields ... Check the lastAgentConnectTime in the status of the VMC resource. This is the last time at which the managed cluster connected to the admin cluster. If this value is not present, then the managed cluster named managed1 never successfully connected to the admin cluster. This could be due to several reasons:\n  The managed cluster registration process step of applying the registration YAML on the managed cluster, was not completed. For the complete setup instructions, see here.\n  The managed cluster does not have network connectivity to the admin cluster. The managed cluster will attempt to connect to the admin cluster at regular intervals, and any errors will be reported in the verrazzano-application-operator pod’s log on the managed cluster. View the logs using the following command.\n  # on the managed cluster $ kubectl logs \\  -n verrazzano-system \\  -l app=verrazzano-application-operator If these logs reveal that there is a connectivity issue, check the admin cluster Kubernetes server address that you provided during registration and ensure that it is correct, and that it is reachable from the managed cluster. If it is incorrect, then you will need to repeat the managed cluster registration process described in the setup instructions here.\nVerify VerrazzanoProject placement For Verrazzano to create an application namespace in a managed cluster, that namespace must be part of a VerrazzanoProject that:\n Includes that namespace. Has a placement value that includes that managed cluster.  View the details of the project that corresponds to your application’s namespace. In the example command that follows, the project name is assumed to be myproject. All projects are expected to be created in the verrazzano-mc namespace.\n# on the admin cluster $ kubectl get verrazzanoproject myproject \\  -n verrazzano-mc \\  -o yaml The following partial sample output is for a project that will result in the namespace mynamespace being created on the managed cluster managed1.\nspec: placement: clusters: - name: managed1 template: namespaces: - metadata: name: mynamespace ....other fields.... Check the multicluster resource status On the admin cluster, each multicluster resource’s status field is updated with the status of the underlying resource on each managed cluster in which it is placed.\nThe following example command shows how to view the status of a MultiClusterApplicationConfiguration named myapp, in the namespace mynamespace, that has a placement value that includes the managed cluster managed1\n$ kubectl get multiclusterapplicationconfiguration myapp \\  -n mynamespace \\  -o yaml The status of the underlying resource in each cluster specified in the placement is shown in the following partial sample output:\n status: clusters: - lastUpdateTime: \"2021-06-22T21:05:04Z\" message: OAM Application Configuration created name: managed1 state: Succeeded conditions: - lastTransitionTime: \"2021-06-22T21:03:58Z\" message: OAM Application Configuration created status: \"True\" type: DeployComplete state: Succeeded The status message contains additional information on the operation’s success or failure.\n","excerpt":"This document describes some common problems you might encounter when using multicluster Verrazzano, and how to troubleshoot them.\nIf you created multicluster resources in the admin cluster, and …","ref":"/docs/troubleshooting/troubleshooting-multicluster/","title":"Multicluster Verrazzano"},{"body":"","excerpt":"","ref":"/docs/setup/platforms/","title":"Platform Setup"},{"body":"A Verrazzano project provides a way to group application namespaces that are owned or administered by the same user or group of users.\nThe VerrazzanoProject resource A VerrazzanoProject resource is created by a Verrazzano admin user, and specifies the following:\n A list of namespaces that the project governs. One or more users, groups, or service accounts that will be granted the verrazzano-project-admin role for the VerrazzanoProject. Project admins may deploy or delete applications and related resources in the namespaces in the project. One or more users, groups, or service accounts that will be granted the verrazzano-project-monitor role for the VerrazzanoProject. Project monitors may view the resources in the namespaces in the project, but not modify or delete them. A list of network policies to apply to the namespaces in the project.  The creation of a VerrazzanoProject results in:\n The creation of the specified namespaces in the project, if those do not already exist. The creation of a Kubernetes RoleBindings in each of the namespaces, to set up the appropriate permissions for the project admins and project monitors of the project. The creation of the specified network policies for each of the namespaces.  ","excerpt":"A Verrazzano project provides a way to group application namespaces that are owned or administered by the same user or group of users.\nThe VerrazzanoProject resource A VerrazzanoProject resource is …","ref":"/docs/applications/projects/","title":"Projects"},{"body":"Verrazzano provides tooling which assists in troubleshooting issues in your environment:\n k8s-dump-cluster.sh verrazzano-analysis  Tools Setup These tools are available for Linux and Mac: https://github.com/verrazzano/verrazzano/releases/.\n Linux Mac   Linux Instructions Use these instructions to obtain the analysis tools on Linux machines.\nDownload the tooling:  $ wget https://github.com/verrazzano/verrazzano/releases/download/v0.17.0/k8s-dump-cluster.sh $ wget https://github.com/verrazzano/verrazzano/releases/download/v0.17.0/k8s-dump-cluster.sh.sha256 $ wget https://github.com/verrazzano/verrazzano/releases/download/v0.17.0/verrazzano-analysis-linux-amd64.tar.gz $ wget https://github.com/verrazzano/verrazzano/releases/download/v0.17.0/verrazzano-analysis-linux-amd64.tar.gz.sha256 Verify the downloaded files:  $ sha256sum -c k8s-dump-cluster.sh.sha256 $ sha256sum -c verrazzano-analysis-linux-amd64.tar.gz.sha256 Unpack the verrazzano-analysis binary:  $ tar xvf verrazzano-analysis-linux-amd64.tar.gz   Mac Instructions Use these instructions to obtain the analysis tools on Mac machines.\nDownload the tooling:  $ wget https://github.com/verrazzano/verrazzano/releases/download/v0.17.0/k8s-dump-cluster.sh $ wget https://github.com/verrazzano/verrazzano/releases/download/v0.17.0/k8s-dump-cluster.sh.sha256 $ wget https://github.com/verrazzano/verrazzano/releases/download/v0.17.0/verrazzano-analysis-darwin-amd64.tar.gz $ wget https://github.com/verrazzano/verrazzano/releases/download/v0.17.0/verrazzano-analysis-darwin-amd64.tar.gz.sha256 Verify the downloaded files:  $ shasum -a 256 -c k8s-dump-cluster.sh.sha256 $ shasum -a 256 -c verrazzano-analysis-darwin-amd64.tar.gz.sha256 Unpack the verrazzano-analysis binary:  $ tar xvf verrazzano-analysis-darwin-amd64.tar.gz    Use the k8s-dump-cluster.sh tool The k8s-dump-cluster.sh tool is a shell script which runs various kubectl and helm commands against a cluster.\nNote that the data captured by this script might include sensitive information. This data is under your control; you can choose whether to share it.\nThe directory structure created by the k8s-dump-cluster.sh tool, for a specific cluster dump, appears as follows:\n$ CAPTURE_DIR cluster-dump directory per namespace (a directory at this level is assumed to represent a namespace) acme-orders.json application-configurations.json certificate-requests.json cluster-role-bindings.json cluster-roles.json cluster-roles.json coherence.json components.json {CONFIGNAME}.configmap (a file at this level for each configmap in the namespace) daemonsets.json deployments.json events.json gateways.json ingress-traits.json jobs.json multicluster-application-configurations.json multicluster-components.json multicluster-config-maps.json multicluster-logging-scopes.json multicluster-secrets.json namespace.json persistent-volume-claims.json persistent-volumes.json pods.json replicasets.json replication-controllers.json role-bindings.json services.json verrazzano-managed-clusters.json verrazzano-projects.json verrazzano_resources.json virtualservices.json weblogic-domains.json directory per pod (a directory at this level is assumed to represent a specific pod) logs.txt (includes logs for all containers and initContainers) api-resources.out application-configurations.json cluster-issuers.txt coherence.json configmap_list.out crd.json es_indexes.out gateways.json helm-ls.json helm-version.out images-on-nodes.csv ingress.json ingress-traits.json kubectl-version.json namespace_list.out network-policies.json network-policies.txt nodes.json pv.json verrazzano_resources.out virtualservices.json  The script shows the kubectl and helm commands which are run. The basic structure, shown previously, is formed by running the command, $ kubectl cluster-info dump --all-namespaces, with additional data captured into that directory structure.\nTo perform a dump of a cluster into a directory named my-cluster-dump:\n$ sh k8s-dump-cluster.sh -d my-cluster-dump\nUse the verrazzano-analysis tool The verrazzano-analysis tool analyzes data from a cluster dump captured using k8s-dump-cluster.sh, reports the issues found, and prescribes related actions to take. These tools are continually evolving with regard to what may be captured, the knowledge base of issues and actions, and the types of analysis that can be performed.\nUsers, developers, and Continuous Integration (CI) can use this tooling to quickly identify the root cause of encountered problems, determine mitigation actions, and provide a sharable report with other users or tooling.\nThe data that the analysis examines follows the structure created by the corresponding capture tooling. For example, k8s-dump-cluster.sh dumps a cluster into a specific structure, which might contain data that you do not want to share. The tooling analyzes the data and provides you with a report, which identifies issues and provides you with actions to take.\nThe verrazzano-analysis tool will find and analyze all cluster dump directories found under a specified root directory. This lets you create a directory to hold the cluster dumps of related clusters into sub-directories which the tool can analyze.\nFor example:\nmy-cluster-dumps CAPTURE_DIR-1 cluster-dump ... CAPTURE_DIR-2 cluster-dump ...  The tool analyzes each cluster dump directory found; you need to provide only the single root directory.\nTo perform an analysis of the clusters:\n$ verrazzano-analysis my-cluster-dumps\nUsage information Usage: verrazzano-analysis [options] captured-data-directory    Parameter Definition Default     -actions Include actions in the report. true   -help Display usage help.    -info Include informational messages. true   -minConfidence Minimum confidence threshold to report for issues, 0-10. 0   -minImpact Minimum impact threshold to report for issues, 0-10. 0   -reportFile Name of report output file. Output to stdout.   -support Include support data in the report. true   -version Display tool version.     ","excerpt":"Verrazzano provides tooling which assists in troubleshooting issues in your environment:\n k8s-dump-cluster.sh verrazzano-analysis  Tools Setup These tools are available for Linux and Mac: …","ref":"/docs/troubleshooting/diagnostictools/verrazzanoanalysistool/","title":"Verrazzano Analysis Tools"},{"body":"Open Application Model (OAM) is a runtime-agnostic specification for defining cloud native applications; it allows developers to focus on the application instead of the complexities of a particular runtime infrastructure. OAM provides the specification for several file formats and rules for a runtime to interpret. Verrazzano uses OAM to enable the definition of a composite application abstraction and makes OAM constructs available within a VerrazzanoApplication YAML file. Verrazzano provides the flexibility to combine what you want into a multicloud enablement. It uses the VerrazzanoApplication as a means to encapsulate a set of components, scopes, and traits, and deploy them on a selected cluster.\nOAM’s workload concept makes it easy to use many different workload types. Verrazzano includes specific workload types with special handling to deploy and manage those types, such as WebLogic, Coherence, and Helidon. OAM’s flexibility lets you create a grouping that is managed as a unit, although each component can be scaled or updated independently.\nHow does OAM work? OAM has five core concepts:\n Workloads - Declarations of the kinds of resources supported by the platform and the OpenAPI schema for that resource. Most Kubernetes CRDs can be exposed as workloads. Standard Kubernetes resource types can also be used (for example, Deployment, Service, Pod, ConfigMap). Components - Wrap a workload resource’s specification data within OAM specific metadata. Application Configurations - Describe a collection of components that comprise an application. This is also where customization (such as, environmental) of each component is done. Customization is achieved using scopes and traits. Scopes - Apply customization to several components. Traits - Apply customization to a single component.  ","excerpt":"Open Application Model (OAM) is a runtime-agnostic specification for defining cloud native applications; it allows developers to focus on the application instead of the complexities of a particular …","ref":"/docs/concepts/verrazzanooam/","title":"Verrazzano and the Open Application Model"},{"body":"Review the following key concepts to understand multicluster Verrazzano:\n Admin cluster - A Kubernetes cluster that serves as the central management point for deploying and monitoring applications in managed clusters. Managed clusters - A Kubernetes cluster that has the following characteristics:  It is registered with an admin cluster with a unique name. Verrazzano multicluster applications may be deployed to the managed cluster from the admin cluster. Logs and metrics for Verrazzano system components and Verrazzano multicluster applications deployed on the managed cluster are viewable from the admin cluster.   Verrazzano multicluster resources - Custom Kubernetes resources defined by Verrazzano.  Each multicluster resource serves as a wrapper for an underlying resource type. A multicluster resource allows the placement of the underlying resource to be specified as a list of names of the clusters in which the resource must be placed.    For more details, see here.\n","excerpt":"Review the following key concepts to understand multicluster Verrazzano:\n Admin cluster - A Kubernetes cluster that serves as the central management point for deploying and monitoring applications in …","ref":"/docs/concepts/verrazzanomulticluster/","title":"Verrazzano in a Multicluster Environment"},{"body":"A project provides a way to group application namespaces that are owned or administered by the same user or group of users. When creating a project, you can specify the subjects: users, groups and/or service accounts, that are to be granted access to the namespaces governed by the project. Two types of subjects may be specified:\n Project admins, who have both read and write access to the project’s namespaces. Project monitors, who have read-only access to the project’s namespaces.  ","excerpt":"A project provides a way to group application namespaces that are owned or administered by the same user or group of users. When creating a project, you can specify the subjects: users, groups and/or …","ref":"/docs/concepts/verrazzanoproject/","title":"Verrazzano Projects"},{"body":"","excerpt":"","ref":"/docs/applications/workloads/","title":"Workloads"},{"body":"","excerpt":"","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/","title":"Analysis Advice"},{"body":"","excerpt":"","ref":"/docs/troubleshooting/diagnostictools/","title":"Diagnostic Tools"},{"body":"The IngressTrait custom resource contains the configuration of host and path rules for traffic routing to an application. Here is a sample ApplicationConfiguration that specifies an IngressTrait. To deploy an example application that demonstrates this IngressTrait, see Hello World Helidon.\napiVersion: core.oam.dev/v1alpha2 kind: ApplicationConfiguration metadata: name: hello-helidon-appconf namespace: hello-helidon annotations: version: v1.0.0 description: \"Hello Helidon application\" spec: components: - componentName: hello-helidon-component traits: - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: MetricsTrait spec: scraper: verrazzano-system/vmi-system-prometheus-0 - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: IngressTrait metadata: name: hello-helidon-ingress spec: rules: - paths: - path: \"/greet\" pathType: Prefix In the sample configuration, the IngressTrait hello-helidon-ingress is set on the hello-helidon-component application component and defines an ingress rule that configures a path and path type. This exposes a route for external access to the application. Note that because no hosts list is given for the IngressRule, a DNS host name is automatically generated.\nFor example, with the sample application configuration successfully deployed, the application will be accessible with the path specified in the IngressTrait and the generated host name.\n$ HOST=$(kubectl get gateway hello-helidon-hello-helidon-appconf-gw -n hello-helidon -o jsonpath={.spec.servers[0].hosts[0]}) $ echo $HOST hello-helidon-appconf.hello-helidon.11.22.33.44.nip.io $ curl -sk -X GET https://${HOST}/greet Alternatively, specific host names can be given in an IngressRule. Doing this implies that a secret and certificate have been created for the specific hosts and the secret name has been specified in the associated IngressSecurity secretName field.\nIngressTrait    Field Type Description Required     apiVersion string oam.verrazzano.io/v1alpha1 Yes   kind string IngressTrait Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec IngressTraitSpec The desired state of an ingress trait. Yes    IngressTraitSpec IngressTraitSpec specifies the desired state of an ingress trait.\n   Field Type Description Required     rules IngressRule array A list of ingress rules to for an ingress trait. Yes   tls IngressSecurity The security parameters for an ingress trait. This is required only if specific hosts are given in an IngressRule. No    IngressRule IngressRule specifies a rule for an ingress trait.\n   Field Type Description Required     hosts string array One or more hosts exposed by the ingress trait. Wildcard hosts or hosts that are empty are filtered out. If there are no valid hosts provided, then a DNS host name is automatically generated and used. No   paths IngressPath array The paths to be exposed for an ingress trait. Yes    IngressPath IngressPath specifies a specific path to be exposed for an ingress trait.\n   Field Type Description Required     path string If no path is provided, it defaults to /. No   pathType string Path type values are case-sensitive and formatted as follows: exact: exact string matchprefix: prefix-based matchregex: regex-based matchIf the provided ingress path doesn’t contain a pathType, it defaults to prefix if the path is / and exact otherwise. No    IngressSecurity IngressSecurity specifies the secret containing the certificate securing the transport for an ingress trait.\n   Field Type Description Required     secretName string The name of a secret containing the certificate securing the transport. The specification of a secret here implies that a certificate was created for specific hosts, as specified in an IngressRule. Yes    ","excerpt":"The IngressTrait custom resource contains the configuration of host and path rules for traffic routing to an application. Here is a sample ApplicationConfiguration that specifies an IngressTrait. To …","ref":"/docs/reference/api/oam/ingresstrait/","title":"IngressTrait Custom Resource Definition"},{"body":"","excerpt":"","ref":"/docs/setup/install/","title":"Install"},{"body":"This document describes built-in configuration profiles that you can use to simplify a Verrazzano installation. An installation profile is a well-known configuration of Verrazzano settings that can be referenced by name, which then can be customized as needed.\nThe following table describes the Verrazzano installation profiles.\n   Profile Description Characteristics     prod Full install, production configuration. Default profile:- Full installation.- Persistent storage. - Production Elasticsearch cluster topology.   dev Development or evaluation configuration. Lightweight installation:- For evaluation purposes.- No persistence.- Single-node Elasticsearch cluster topology.   managed-cluster A specialized installation for managed clusters in a multicluster topology. Minimal installation for a managed cluster:- Cluster must be registered with an admin cluster to use multicluster features.    Use an installation profile To use a profile to install Verrazzano, set the profile name in the profile field of your Verrazzano custom resource.\nFor example, to use the dev profile:\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: my-verrazzano spec: profile: dev To use a different profile, simply replace dev with prod or managed-cluster.\nCustomize an installation profile You can override the profile settings for any component regardless of the profile. The following example uses a customized dev profile to configure a small 8Gi persistent volume for the MySQL instance used by Keycloak to provide more stability for the Keycloak service:\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: custom-dev-example spec: profile: dev components: keycloak: mysql: volumeSource: persistentVolumeClaim: claimName: mysql volumeClaimSpecTemplates: - metadata: name: mysql spec: resources: requests: storage: 8Gi For details on how to customize Verrazzano components, see Customize an Installation.\nProfile configurations The following table lists the Verrazzano components that are installed with each profile. Note that you can customize any Verrazzano installation, regardless of the profile.\n   Component dev prod managed-cluster     Istio ✔️ ✔️ ✔️   NGINX ✔️ ✔️ ✔️   Cert-Manager ✔️ ✔️ ✔️   External-DNS ️ ️    Prometheus ✔️ ✔️ ✔️   Elasticsearch ✔️ ✔️    Console ✔️ ✔️    Kibana ✔️ ✔️    Grafana ✔️ ✔️    Rancher ✔️ ✔️    Keycloak ✔️ ✔️     Prometheus and Grafana configurations The following table describes the Prometheus and Grafana configurations in each profile.\n   Profile Prometheus Grafana     prod 1 replica (128M memory, 50Gi storage) 1 replica (48M memory, 50Gi storage)   dev 1 replica (128M memory, ephemeral storage) 1 replica (48M memory, ephemeral storage)   managed-cluster 1 replica (128M memory, 50Gi storage) Not installed    Kibana and Elasticsearch configurations The following table describes the Kibana and Elasticsearch cluster topology in each profile.\n   Profile Elasticsearch Kibana     prod 3 master replicas (1.4Gi memory, 50Gi storage each)1 ingest replica (2.5Gi memory, no storage)2 data replicas (4.8Gi memory, 50Gi storage each) 1 replica (192M memory, ephemeral storage)   dev 1 master/data/ingest replica (1Gi memory, ephemeral storage) 1 replica (192M memory, ephemeral storage)   managed-cluster Not installed Not installed    NOTE Elasticsearch containers are configured to use 75% of the configured request memory for the Java min/max heap settings.  Profile-independent defaults The following table shows the settings for components that are profile-independent (consistent across all profiles unless overridden).\n   Component Default     DNS Wildcard DNS provider nip.io.   Certificates Uses the cert-manager self-signed ClusterIssuer for certificates.   Ingress-type Defaults to LoadBalancer service type for the ingress.    For details on how to customize Verrazzano components, see Customizing an Installation.\n","excerpt":"This document describes built-in configuration profiles that you can use to simplify a Verrazzano installation. An installation profile is a well-known configuration of Verrazzano settings that can be …","ref":"/docs/setup/install/profiles/","title":"Installation Profiles"},{"body":"Verrazzano uses Kubernetes RBAC to protect Verrazzano resources.\nVerrazzano includes a set of roles that can be granted to users, enabling access to Verrazzano resources managed by Kubernetes. In addition, Verrazzano creates a number of roles that grant permissions needed by various Verrazzano system components (operators and third-party components).\nVerrazzano creates default role bindings during installation and for projects, at project creation or update.\nNOTE Kubernetes RBAC must be enabled in every cluster to which Verrazzano is deployed or access control will not work. RBAC is enabled by default in most Kubernetes environments.  Verrazzano user roles The following table lists the defined Verrazzano user roles. Each is a ClusterRole intended to be granted directly to users or groups. (In some scenarios, it may be appropriate to grant a user role to a service account.)\n   Verrazzano Role Binding Scope Description     verrazzano-admin Cluster Manage Verrazzano system components, clusters, and projects. Install/update Verrazzano.   verrazzano-monitor Cluster View/monitor Verrazzano system components, clusters, and projects.   verrazzano-project-admin Namespace Deploy/manage applications.   verrazzano-project-monitor Namespace View/monitor applications.    Kubernetes user roles Verrazzano roles do not include permissions for Kubernetes itself. Instead, it relies on the default user roles provided by Kubernetes. This allows Verrazzano to easily grant the Kubernetes access appropriate to a Verrazzano role, without having to maintain a long list of fine-grained Kubernetes permissions in the Verrazzano roles.\nThe following table shows the default Kubernetes roles that are granted by default for each Verrazzano role.\n   Verrazzano Role Kubernetes Role Binding Scope     verrazzano-admin admin Cluster   verrazzano-monitor view Cluster   verrazzano-project-admin admin Namespace   verrazzano-project-monitor view Namespace    Default role bindings Verrazzano creates role bindings for the system and for projects, binding Verrazzano ClusterRoles to one or more Kubernetes Subjects. By default, each role is bound to a Keycloak group, so all Keycloak users who are members of that group will be granted the role.\nAlso, Verrazzano creates role bindings for the corresponding Kubernetes user roles. The Kubernetes role appropriate for a given Verrazzano role is bound to the same set of Subjects as the corresponding Verrazzano role.\nThe default bindings can be overridden by specifying one or more Kubernetes Subjects to which the role should be bound. Any valid Subject can be specified (user, group, or service account), but two caveats should be kept in mind:\n It’s generally better to grant a role to a group, rather than a specific user, so that roles can be granted (or withdrawn) by editing a user’s group memberships, rather than deleting a role binding and creating a new one. If you do want to grant a role directly to a specific user, the user must be specified using its unique ID, not its user name. This is because the API proxy impersonates the “sub” (subject) field from the user’s token, which contains the ID. Keycloak user IDs are guaranteed to be unique, unlike user names.  Default system role bindings Verrazzano creates role bindings for system users during installation. The default role bindings are listed below.\n   Role Default Binding Subject     verrazzano-admin group: verrazzano-admins   verrazzano-monitor group: verrazzano-monitors    Default project role bindings Verrazzano creates role bindings for system users during installation. The default role bindings are listed below.\n   Role Default Binding Subject     verrazzano-project-admin group: verrazzano-project-\u003cproj_name\u003e-admins   verrazzano-project-monitor group: verrazzano-project-\u003cproj_name\u003e-monitors    NOTE The role bindings for project roles are created automatically, but the project-specific groups that they refer to are not automatically created. You must create those groups using the Keycloak console or API, or specify different binding subjects for the project.  Override default role bindings You can override the default role bindings that are created for system and project roles.\nOverride system role bindings To override the set of subjects that are bound to Verrazzano (and Kubernetes) roles during installation, add the Subjects to the Verrazzano CR you use to install Verrazzano, as shown in the following example:\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: my-verrazzano spec: ... security: adminSubjects: - name: admin-group kind: Group monitorSubjects: - name: view-group kind: Group ... You can specify multiple subjects for both admin and monitor roles. You can also specify a subject or subjects for one role, but not the other. If no subjects are specified for a role, then the default binding subjects will be used.\nOverride project role bindings To override the set of subjects that are bound to Verrazzano (and Kubernetes) roles for a project, add the Subjects to the VerrazzanoProject CR for the project, as shown in the example below.\nNote that the generated role bindings will be updated if you update the VerrazzanoProject CR and change the subjects specified for either role.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: VerrazzanoProject metadata: name: my-project spec: ... security: projectAdminSubjects: - name: my-project-admin-group kind: Group projectMonitorSubjects: - name: my-project-view-group kind: Group ... As with the system role bindings, you can specify multiple subjects for both project-admin and project-monitor roles. You can also specify a subject or subjects for one role, but not the other. If no subjects are specified for a role, then the default binding subjects will be used.\n","excerpt":"Verrazzano uses Kubernetes RBAC to protect Verrazzano resources.\nVerrazzano includes a set of roles that can be granted to users, enabling access to Verrazzano resources managed by Kubernetes. In …","ref":"/docs/security/rbac/rbac/","title":"Kubernetes RBAC"},{"body":"This guide describes how to move (“Lift-and-Shift”) an on-premises WebLogic Server domain to a cloud environment running Kubernetes using Verrazzano.\nOverview The Initial steps create a very simple on-premises domain that you will move to Kubernetes. The sample domain is the starting point for the lift and shift process; it contains one application (ToDo List) and one data source. First, you’ll configure the database and the WebLogic Server domain. Then, in Lift and Shift, you will move the domain to Kubernetes with Verrazzano. This guide does not include the setup of the networking that would be needed to access an on-premises database, nor does it document how to migrate a database to the cloud.\nWhat you need   The Git command-line tool and access to GitHub\n  MySQL Database 8.x - a database server\n  WebLogic Server 12.2.1.4.0 - an application server; Note that all WebLogic Server installers are supported except the Quick Installer.\n  Maven - to build the application\n  WebLogic Deploy Tooling (WDT) - v1.9.15 or later, to convert the WebLogic Server domain to and from metadata\n  WebLogic Image Tool (WIT) - v1.9.13 or later, to build the Docker image\n  Initial steps In the initial steps, you create a sample domain that represents your on-premises WebLogic Server domain.\nCreate a database using MySQL called tododb   Download the MySQL image from Docker Hub.\n$ docker pull mysql:latest   Start the container database (and optionally mount a volume for data).\n$ docker run --name tododb \\ -p 3306:3306 \\ -e MYSQL_USER=derek \\ -e MYSQL_PASSWORD=welcome1 \\ -e MYSQL_DATABASE=tododb \\ -e MYSQL_ROOT_PASSWORD=welcome1 \\ -d mysql:latest  NOTE You should use a more secure password.    Start a MySQL client to change the password algorithm to mysql_native_password.\n Assuming the database server is running, start a database CLI client. $ docker exec \\  -it tododb mysql \\  -uroot \\  -p  When prompted for the password, enter the password for the root user, welcome1 or whatever password you set when starting the container in the previous step. After being connected, run the ALTER command at the MySQL prompt. $ ALTER USER 'derek'@'%' IDENTIFIED WITH mysql_native_password BY 'welcome1';   NOTE You should use a more secure password.    Create a WebLogic Server domain   If you do not have WebLogic Server 12.2.1.4.0 installed, install it now.\n  Choose the GENERIC installer from WebLogic Server Downloads and follow the documented installation instructions.\n  Be aware of these domain limitations:\n There are two supported domain types, single server and single cluster. Domains must use:  The default value AdminServer for AdminServerName. WebLogic Server listen port for the Administration Server: 7001. WebLogic Server listen port for the Managed Server: 8001. Note that these are all standard WebLogic Server default values.      Save the installer after you have finished; you will need it to build the Docker image.\n  To make copying commands easier, define an environment variable for ORACLE_HOME that points to the directory where you installed WebLogic Server 12.2.1.4.0. For example:\n$ export ORACLE_HOME=$HOME/Oracle/Middleware/Oracle_Home     Use the Oracle WebLogic Server Configuration Wizard to create a domain called tododomain.\n Launch $ORACLE_HOME/oracle_common/common/bin/config.sh. Select Create a new domain. Specify a Domain Location of \u003coracle home\u003e/user_projects/domains/tododomain and click Next. Select the Basic WebLogic Server Domain [wlserver] template and click Next. Enter the password for the administrative user (the examples here assume a password of “welcome1”) and click Next. Accept the defaults for Domain Mode and JDK, and click Next. Select Administration Server and click Next. Ensure that the server name is AdminServer and click Next. Click Create. After it has completed, click Next, then Finish.    To start the newly created domain, run the domain’s start script.\n$ $ORACLE_HOME/user_projects/domains/tododomain/bin/startWebLogic.sh   Access the Console of the newly started domain with your browser, for example, http://localhost:7001/console, and log in using the administrator credentials you specified.\n  Add a data source configuration to access the database Using the WebLogic Server Administration Console, log in and add a data source configuration to access the MySQL database. During the data source configuration, you can accept the default values for most fields, but the following fields are required to match the application and database settings you used when you created the MySQL database.\n  In the left pane in the Console, expand Services and select Data Sources.\n  On the Summary of JDBC Data Sources page, click New and select Generic Data Source.\n  On the JDBC Data Sources page, enter or select the following information:\n Name: tododb JNDI Name: jdbc/ToDoDB Database Type: MySQL    Click Next and then click Next two more times.\n  On the Create a New JDBC Data Source page, enter the following information:\n Database Name: tododb Host name: localhost Database Port: 3306 Database User Name: derek Password: welcome1 (or whatever password you used) Confirm Password: welcome1    Click Next.\n  Select Test Configuration, and make sure you see “Connection Test Succeeded” in the Messages field of the Console.\n  Click Next.\n  On the Select Targets page, select AdminServer.\n  Click Finish to complete the configuration.\n  Build and deploy the application   Using Maven, build this project to produce todo.war.\nNOTE: You should clone this repo outside of $ORACLE_HOME or copy the WAR file to another location, as WDT may ignore it during the model creation phase.\n$ git clone https://github.com/verrazzano/examples.git $ cd examples/todo-list/ $ mvn clean package   Using the WebLogic Server Administration Console, deploy the ToDo List application.\n In the left pane in the Console, select Deployments and click Install. Use the navigation links or provide the file path to todo.war typically \u003crepo\u003e/todo-list/target. For example, if you cloned the examples repository in your $HOME directory, the location should be $HOME/examples/examples/todo-list/target/todo.war. Click Next twice, then Finish.  NOTE: The remaining steps assume that the application context is todo.\n  Initialize the database After the application is deployed and running in WebLogic Server, access the http://localhost:7001/todo/rest/items/init REST service to create the database table used by the application. In addition to creating the application table, the init service also will load four sample items into the table.\nIf you get an error here, go back to the Select Targets page in the WebLogic Server Administration Console and make sure that you selected AdminServer as the data source target.\nAccess the application  Access the application at http://localhost:7001/todo/index.html.   Add a few entries or delete some. After verifying the application and database, you may shut down the local WebLogic Server domain.  Lift and Shift steps The following steps will move the sample domain to Kubernetes with Verrazzano.\nCreate a WDT Model  If you have not already done so, download v1.9.15 or later of WebLogic Deploy Tooling (WDT) from GitHub. Unzip the installer weblogic-deploy.zip file so that you can access bin/discoverDomain.sh. To make copying commands easier, define an environment variable for WDT_HOME that points to the directory where you installed WebLogic Deploy Tooling. $ export WDT_HOME=/install/directory   For example, to get the latest version:\n$ curl -OL https://github.com/oracle/weblogic-deploy-tooling/releases/latest/download/weblogic-deploy.zip $ unzip weblogic-deploy.zip $ cd weblogic-deploy $ export WDT_HOME=$(pwd) To create a reusable model of the application and domain, use WDT to create a metadata model of the domain.\n First, create an output directory to hold the generated scripts and models. Then, run WDT discoverDomain. $ mkdir v8o $ $WDT_HOME/bin/discoverDomain.sh \\  -oracle_home $ORACLE_HOME \\  -domain_home /path/to/domain/dir \\  -model_file ./v8o/wdt-model.yaml \\  -archive_file ./v8o/wdt-archive.zip \\  -target vz \\  -output_dir v8o   You will find the following files in ./v8o:\n create_k8s_secrets.sh - A helper script with kubectl commands to apply the Kubernetes secrets needed for this domain vz-application.yaml - Verrazzano application configuration and component file vz_variable.properties - A set of properties extracted from the WDT domain model wdt-archive.zip - The WDT archive file containing the ToDo List application WAR file wdt-model.yaml - The WDT model of the WebLogic Server domain  If you chose to skip the Access the application step and did not verify that the ToDo List application was deployed, then you should verify that you see the todo.war file inside the wdt-archive.zip file. If you do not see the WAR file, there was something wrong in your deployment of the application on WebLogic Server that will require additional troubleshooting in your domain.\nCreate a Docker image At this point, the Verrazzano model is just a template for the real model. The WebLogic Image Tool will fill in the placeholders for you, or you can edit the model manually to set the image name and domain home directory.\n If you have not already done so, download WebLogic Image Tool (WIT) from GitHub. Unzip the installer imagetool.zip file so that you can access bin/imagetool.sh. To make copying commands easier, define an environment variable for WIT_HOME that points to the directory where you installed WebLogic Image Tool. $ export WIT_HOME=/install/directory   For example, to get the latest WIT tool:\n$ curl -OL https://github.com/oracle/weblogic-image-tool/releases/latest/download/imagetool.zip $ unzip imagetool.zip $ cd imagetool $ export WIT_HOME=$(pwd) You will need a Docker image to run your WebLogic Server domain in Kubernetes. To use WIT to create the Docker image, run imagetool create. Although WIT will download patches and PSUs for you, it does not yet download installers. Until then, you must download the WebLogic Server and Java Development Kit installer manually and provide their location to the imagetool cache addInstaller command.\n# The directory created previously to hold the generated scripts and models. $ cd v8o $ $WIT_HOME/bin/imagetool.sh cache addInstaller \\  --path /path/to/installer/jdk-8u231-linux-x64.tar.gz \\  --type jdk \\  --version 8u231 # The installer file name may be slightly different depending on # which version of the 12.2.1.4.0 installer that you downloaded, slim or generic. $ $WIT_HOME/bin/imagetool.sh cache addInstaller \\  --path /path/to/installer/fmw_12.2.1.4.0_wls_Disk1_1of1.zip \\  --type wls \\  --version 12.2.1.4.0 $ $WIT_HOME/bin/imagetool.sh cache addInstaller \\  --path /path/to/installer/weblogic-deploy.zip \\  --type wdt \\  --version latest # Paths for the files in this command assume that you are running it from the # v8o directory created during the `discoverDomain` step. $ $WIT_HOME/bin/imagetool.sh create \\  --tag your/repo/todo:1 \\  --version 12.2.1.4.0 \\  --jdkVersion 8u231 \\  --wdtModel ./wdt-model.yaml \\  --wdtArchive ./wdt-archive.zip \\  --wdtVariables ./vz_variable.properties \\  --resourceTemplates=./vz-application.yaml \\  --wdtModelOnly The imagetool create command will have created a local Docker image and updated the Verrazzano model with the domain home and image name. Check your Docker images for the tag that you used in the create command using docker images from the Docker CLI.\nIf everything worked correctly, it is time to push that image to the container registry that Verrazzano will use to access the image from Kubernetes. You can use the Oracle Cloud Infrastructure Registry (OCIR) as your repository for this example, but most Docker compliant registries should work.\nThe variables in the vz-application.yaml resource template should be resolved with information from the image tool build.\nVerify this by looking in the v8o/vz-application.yaml file to make sure that the image: {{{imageName}}} value has been set with the given --tag value.\nPush the image to your repo.\nNOTE: The image name must be the same as what is in the vz-application.yaml file under spec \u003e workload \u003e spec \u003e image for the tododomain-domain component.\n$ docker push your/repo/todo:1 Deploy to Verrazzano After the application image has been created, there are several steps required to deploy a the application into a Verrazzano environment.\nThese include:\n Creating and labeling the tododomain namespace. Creating the necessary secrets required by the ToDo List application. Deploying MySQL to the tododomain namespace. Updating the vz-application.yaml file to use the Verrazzano MySQL deployment and (optionally) expose the WLS Console. Applying the vz-application.yaml file.  The following steps assume that you have a Kubernetes cluster and that Verrazzano is already installed in that cluster.\nLabel the namespace Create the tododomain namespace, and add labels to allow the WebLogic Server Kubernetes Operator to manage it and enabled for Istio.\n$ kubectl create namespace tododomain $ kubectl label namespace tododomain verrazzano-managed=true istio-injection=enabled Create the required secrets If you haven’t already done so, edit and run the create_k8s_secrets.sh script to generate the Kubernetes secrets. WDT does not discover passwords from your existing domain. Before running the create secrets script, you will need to edit create_k8s_secrets.sh to set the passwords for the WebLogic Server domain and the data source. In this domain, there are a few passwords that you need to enter:\n Administrator credentials (for example, weblogic/welcome1) ToDo database credentials (for example, derek/welcome1) Runtime encryption secret (for example, welcome1)  For example:\n# Update \u003cadmin-user\u003e and \u003cadmin-password\u003e for weblogic-credentials $ create_paired_k8s_secret weblogic-credentials weblogic welcome1 # Update \u003cuser\u003e and \u003cpassword\u003e for tododomain-jdbc-tododb $ create_paired_k8s_secret jdbc-tododb derek welcome1 # Update \u003cpassword\u003e used to encrypt hashes $ create_k8s_secret runtime-encryption-secret welcome1 Then run the script:\n$ sh ./create_k8s_secrets.sh Verrazzano will need a credential to pull the image that you just created, so you need to create one more secret. The name for this credential can be changed in the vz-application.yaml file to anything you like, but it defaults to tododomain-registry-credentials.\nAssuming that you leave the name tododomain-registry-credentials, you will need to run a kubectl create secret command similar to the following:\n$ kubectl create secret docker-registry tododomain-registry-credentials \\  --docker-server=phx.ocir.io \\  --docker-email=your.name@company.com \\  --docker-username=tenancy/username \\  --docker-password='passwordForUsername' \\  --namespace=tododomain Update the application configuration Update the generated vz-application.yaml file for the todo application to:\n Update the tododomain-configmap component to use the in-cluster MySQL service URL jdbc:mysql://mysql.tododomain.svc.cluster.local:3306/tododb to access the database.  wdt_jdbc.yaml:| resources:JDBCSystemResource:'todo-ds':JdbcResource:JDBCDriverParams:# This is the URL of the database used by the WebLogic Server applicationURL:\"jdbc:mysql://mysql.tododomain.svc.cluster.local:3306/tododb\"The file vz-application-modified.yaml is an example of a modified vz-application.yaml file. A diff of these two sample files is shown:\n$ diff vz-application.yaml vz-application-modified.yaml 102c102 \u003c URL: \"jdbc:mysql://localhost:3306/tododb\" --- \u003e URL: \"jdbc:mysql://mysql.tododomain.svc.cluster.local:3306/tododb\" Deploy MySQL As noted previously, moving a production environment to Verrazzano would require migrating the data as well. While data migration is beyond the scope of this guide, we will still need to include a MySQL instance to be deployed with the application in the Verrazzano environment.\nTo do so, download the mysql-oam.yaml file.\nThen, apply the YAML file:\n$ kubectl apply -f mysql-oam.yaml Wait for the MySQL pod to reach the Ready state.\n$ kubectl get pod -n tododomain -w NAME READY STATUS RESTARTS AGE mysql-5cfd58477b-mg5c7 0/1 Pending 0 0s mysql-5cfd58477b-mg5c7 0/1 Pending 0 0s mysql-5cfd58477b-mg5c7 0/1 ContainerCreating 0 0s mysql-5cfd58477b-mg5c7 1/1 Running 0 2s Deploy the ToDo List application Finally, run kubectl apply to apply the Verrazzano component and Verrazzano application configuration files to start your domain.\n$ kubectl apply -f vz-application.yaml This will:\n Create the application component resources for the ToDo List application. Create the application configuration resources that create the instance of the ToDo List application in the Verrazzano cluster.  Wait for the ToDo List example application to be ready.\n$ kubectl wait pod --for=condition=Ready tododomain-adminserver -n tododomain pod/tododomain-adminserver condition met Verify the pods are in the Running state:\n$ kubectl get pod -n tododomain NAME READY STATUS RESTARTS AGE mysql-55bb4c4565-c8zf5 1/1 Running 0 8m tododomain-adminserver 4/4 Running 0 5m Access the application from your browser   Get the generated host name for the application.\n$ kubectl get gateway tododomain-tododomain-appconf-gw -n tododomain -o jsonpath={.spec.servers[0].hosts[0]} tododomain-appconf.tododomain.11.22.33.44.nip.io   Initialize the database by accessing the init URL.\nhttps://tododomain-appconf.tododomain.11.22.33.44.nip.io/todo/rest/items/init   Access the application\nhttp://tododomain-appconf.tododomain.11.22.33.44.nip.io/todo   Access the WebLogic Server Administration Console   Set up port forwarding.\n$ kubectl port-forward pods/tododomain-adminserver 7001:7001 -n tododomain   Access the WebLogic Server Administration Console from your browser.\nhttp://localhost:7001/console   NOTE It is recommended that the WebLogic Server Administration Console not be exposed publicly.  ","excerpt":"This guide describes how to move (“Lift-and-Shift”) an on-premises WebLogic Server domain to a cloud environment running Kubernetes using Verrazzano.\nOverview The Initial steps create a very simple …","ref":"/docs/samples/lift-and-shift/","title":"Lift-and-Shift Guide"},{"body":"The MetricsTrait custom resource contains the configuration information needed to enable metrics for an application component. Component workloads configured with a MetricsTrait are setup to emit metrics through an endpoint that are scraped by a given Prometheus deployment. Here is a sample ApplicationConfiguration that specifies a MetricsTrait. To deploy an example application that demonstrates a MetricsTrait, see Hello World Helidon.\nNote that if an ApplicationConfiguration does not specify a MetricsTrait, then a default MetricsTrait will be generated with values appropriate for the workload type.\napiVersion: core.oam.dev/v1alpha2 kind: ApplicationConfiguration metadata: name: hello-helidon-appconf namespace: hello-helidon annotations: version: v1.0.0 description: \"Hello Helidon application\" spec: components: - componentName: hello-helidon-component traits: - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: MetricsTrait spec: scraper: verrazzano-system/vmi-system-prometheus-0 - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: IngressTrait metadata: name: hello-helidon-ingress spec: rules: - paths: - path: \"/greet\" pathType: Prefix In the sample configuration, a MetricsTrait is specified for the hello-helidon-component application component.\nWith the sample application configuration successfully deployed, you can query for metrics from the application component.\n$ HOST=$(kubectl get ingress \\ -n verrazzano-system vmi-system-prometheus \\ -o jsonpath={.spec.rules[0].host}) $ echo $HOST prometheus.vmi.system.default.\u003cip\u003e.nip.io $ VZPASS=$(kubectl get secret \\ --namespace verrazzano-system verrazzano \\ -o jsonpath={.data.password} | base64 \\ --decode; echo) $ curl -sk \\ --user verrazzano:${VZPASS} \\ -X GET https://${HOST}/api/v1/query?query=vendor_requests_count_total {\"status\":\"success\",\"data\":{\"resultType\":\"vector\",\"result\":[{\"metric\":{\"__name__\":\"vendor_requests_count_total\",\"app\":\"hello-helidon\",\"app_oam_dev_component\":\"hello-helidon-component\",\"app_oam_dev_name\":\"hello-helidon-appconf\",\"app_oam_dev_resourceType\":\"WORKLOAD\",\"app_oam_dev_revision\":\"hello-helidon-component-v1\",\"containerizedworkload_oam_crossplane_io\":\"496df78f-ef8b-4753-97fd-d9218d2f38f1\",\"job\":\"hello-helidon-appconf_default_helidon-logging_hello-helidon-component\",\"namespace\":\"helidon-logging\",\"pod_name\":\"hello-helidon-workload-b7d9d95d8-ht7gb\",\"pod_template_hash\":\"b7d9d95d8\"},\"value\":[1616535232.487,\"4800\"]}]}} MetricsTrait    Field Type Description Required     apiVersion string oam.verrazzano.io/v1alpha1 Yes   kind string MetricsTrait Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec MetricsTraitSpec The desired state of a metrics trait. Yes    MetricsTraitSpec MetricsTraitSpec specifies the desired state of a metrics trait.\n   Field Type Description Required     port integer The HTTP port for the related metrics endpoint. Defaults to 8080. No   path string The HTTP path for the related metrics endpoint. Defaults to /metrics. No   secret string The name of an opaque secret (for example, user name and password) within the workload’s namespace for metrics endpoint access. No   scraper string The Prometheus deployment used to scrape the related metrics endpoints. Defaults to verrazzano-system/vmi-system-prometheus-0. No    ","excerpt":"The MetricsTrait custom resource contains the configuration information needed to enable metrics for an application component. Component workloads configured with a MetricsTrait are setup to emit …","ref":"/docs/reference/api/oam/metricstrait/","title":"MetricsTrait Custom Resource Definition"},{"body":"The MultiClusterApplicationConfiguration custom resource is used to distribute core.oam.dev/v1alpha2/ApplicationConfiguration resources in a multicluster environment. Here is a sample MultiClusterApplicationConfiguration that specifies an ApplicationConfiguration resource to create on the cluster named managed1. To deploy an example application that demonstrates a MultiClusterApplicationConfiguration, see Multicluster Hello World Helidon.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: MultiClusterApplicationConfiguration metadata: name: hello-helidon-appconf namespace: hello-helidon spec: template: metadata: annotations: version: v1.0.0 description: \"Hello Helidon application\" spec: components: - componentName: hello-helidon-component traits: - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: MetricsTrait spec: scraper: verrazzano-system/vmi-system-prometheus-0 - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: IngressTrait metadata: name: hello-helidon-ingress spec: rules: - paths: - path: \"/greet\" pathType: Prefix placement: clusters: - name: managed1 MultiClusterApplicationConfiguration A MultiClusterApplicationConfiguration is an envelope to create core.oam.dev/v1alpha2/ApplicationConfiguration resources on the clusters specified in the placement section.\n   Field Type Description Required     apiVersion string clusters.verrazzano.io/v1alpha1 Yes   kind string MultiClusterApplicationConfiguration Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec MultiClusterApplicationConfigurationSpec The desired state of a core.oam.dev/v1alpha2/ApplicationConfiguration resource. Yes   status MultiClusterResourceStatus The runtime status of a multicluster resource. No    MultiClusterApplicationConfigurationSpec MultiClusterApplicationConfigurationSpec specifies the desired state of a core.oam.dev/v1alpha2/ApplicationConfiguration resource.\n   Field Type Description Required     template ApplicationConfigurationTemplate The embedded core.oam.dev/v1alpha2/ApplicationConfiguration resource. Yes   placement Placement Clusters in which the resource is to be placed. Yes    ApplicationConfigurationTemplate ApplicationConfigurationTemplate has the metadata and spec of the core.oam.dev/v1alpha2/ApplicationConfiguration resource.\n   Field Type Description Required     metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec ApplicationConfigurationSpec An instance of the struct ApplicationConfigurationSpec defined in core_types.go. No    ","excerpt":"The MultiClusterApplicationConfiguration custom resource is used to distribute core.oam.dev/v1alpha2/ApplicationConfiguration resources in a multicluster environment. Here is a sample …","ref":"/docs/reference/api/multicluster/multiclusterapplicationconfiguration/","title":"MultiClusterApplicationConfiguration Custom Resource Definition"},{"body":"The MultiClusterComponent custom resource is used to distribute core.oam.dev/v1alpha2/Component resources in a multicluster environment. Here is a sample MultiClusterComponent that specifies a OAM component resource to create on the cluster named managed1. To deploy an example application that demonstrates a MultiClusterComponent, see Multicluster Hello World Helidon.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: MultiClusterComponent metadata: name: hello-helidon-component namespace: hello-helidon spec: template: spec: workload: apiVersion: oam.verrazzano.io/v1alpha1 kind: VerrazzanoHelidonWorkload metadata: name: hello-helidon-workload namespace: hello-helidon labels: app: hello-helidon spec: deploymentTemplate: metadata: name: hello-helidon-deployment podSpec: containers: - name: hello-helidon-container image: \"ghcr.io/verrazzano/example-helidon-greet-app-v1:0.1.12-1-20210409130027-707ecc4\" ports: - containerPort: 8080 name: http placement: clusters: - name: managed1 MultiClusterComponent A MultiClusterComponent is an envelope to create core.oam.dev/v1alpha2/Component resources on the clusters specified in the placement section.\n   Field Type Description Required     apiVersion string clusters.verrazzano.io/v1alpha1 Yes   kind string MultiClusterComponent Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec MultiClusterComponentSpec The desired state of a core.oam.dev/v1alpha2/Component resource. Yes   status MultiClusterResourceStatus The runtime status of a multicluster resource. No    MultiClusterComponentSpec MultiClusterComponentSpec specifies the desired state of a core.oam.dev/v1alpha2/Component resource.\n   Field Type Description Required     template ComponentTemplate The embedded core.oam.dev/v1alpha2/Component resource. Yes   placement Placement Clusters in which the resource is to be placed. Yes    ComponentTemplate ComponentTemplate has the metadata and spec of the core.oam.dev/v1alpha2/Component resource.\n   Field Type Description Required     metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec ComponentSpec An instance of the struct ComponentSpec defined in core_types.go. No    ","excerpt":"The MultiClusterComponent custom resource is used to distribute core.oam.dev/v1alpha2/Component resources in a multicluster environment. Here is a sample MultiClusterComponent that specifies a OAM …","ref":"/docs/reference/api/multicluster/multiclustercomponent/","title":"MultiClusterComponent Custom Resource Definition"},{"body":"The MultiClusterConfigMap custom resource is used to distribute Kubernetes ConfigMap resources in a multicluster environment. Here is a sample MultiClusterConfigMap that specifies a Kubernetes ConfigMap to create on the cluster named managed1.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: MultiClusterConfigMap metadata: name: mymcconfigmap namespace: multiclustertest spec: template: metadata: name: myconfigmap namespace: myns data: simple.key: \"simplevalue\" placement: clusters: - name: managed1 MultiClusterConfigMap A MultiClusterConfigMap is an envelope to create Kubernetes ConfigMap resources on the clusters specified in the placement section.\n   Field Type Description Required     apiVersion string clusters.verrazzano.io/v1alpha1 Yes   kind string MultiClusterConfigMap Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec MultiClusterConfigMapSpec The desired state of a Kubernetes ConfigMap. Yes   status MultiClusterResourceStatus The runtime status of a multicluster resource. No    MultiClusterConfigMapSpec MultiClusterConfigMapSpec specifies the desired state of a Kubernetes ConfigMap.\n   Field Type Description Required     template ConfigMapTemplate The embedded Kubernetes ConfigMap. Yes   placement Placement Clusters in which the ConfigMap is to be placed. Yes    ConfigMapTemplate ConfigMapTemplate has the metadata and spec of the Kubernetes ConfigMap.\n   Field Type Description Required     metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   immutable *bool Corresponds to the immutable field of the struct ConfigMap defined in types.go. No   data map[string]string Corresponds to the data field of the struct ConfigMap defined in types.go. No   binaryData map[string][]byte Corresponds to the binaryData field of the struct ConfigMap defined in types.go. No    ","excerpt":"The MultiClusterConfigMap custom resource is used to distribute Kubernetes ConfigMap resources in a multicluster environment. Here is a sample MultiClusterConfigMap that specifies a Kubernetes …","ref":"/docs/reference/api/multicluster/multiclusterconfigmap/","title":"MultiClusterConfigMap Custom Resource Definition"},{"body":"The MultiClusterResourceStatus subresource is shared by multicluster custom resources.\nMultiClusterResourceStatus MultiClusterResourceStatus specifies the status portion of a multicluster resource.\n   Field Type Description Required     conditions Condition array The current state of a multicluster resource. No   state string The state of the multicluster resource. State values are case-sensitive and formatted as follows: Pending: deployment to cluster is in progressSucceeded: deployment to cluster successfully completedFailed: deployment to cluster failed No   clusters ClusterLevelStatus array Array of status information for each cluster. No    Condition Condition describes current state of a multicluster resource across all clusters.\n   Field Type Description Required     type string The condition of the multicluster resource which can be checked with a kubectl wait command. Condition values are case-sensitive and formatted as follows: DeployComplete: deployment to all clusters completed successfullyDeployFailed: deployment to all clusters failed Yes   status ConditionStatus An instance of the type ConditionStatus that is defined in types.go. Yes   lastTransitionTime string The last time the condition transitioned from one status to another. No   message string A message with details about the last transition. No    ClusterLevelStatus ClusterLevelStatus describes the status of the multicluster resource on an individual cluster.\n   Field Type Description Required     name string Name of the cluster. Yes   state string The state of the multicluster resource. State values are case-sensitive and formatted as follows: Pending: deployment is in progressSucceeded: deployment successfully completedFailed: deployment failed No   message string Message with details about the status in this cluster. No   lastUpdateTime string The last time the resource state was updated. Yes    ","excerpt":"The MultiClusterResourceStatus subresource is shared by multicluster custom resources.\nMultiClusterResourceStatus MultiClusterResourceStatus specifies the status portion of a multicluster resource. …","ref":"/docs/reference/api/multicluster/multiclusterresourcestatus/","title":"MultiClusterResourceStatus Subresource"},{"body":"The MultiClusterSecret custom resource is used to distribute Kubernetes Secret resources in a multicluster environment. Here is a sample MultiClusterSecret that specifies a Kubernetes secret to create on the cluster named managed1.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: MultiClusterSecret metadata: name: mymcsecret namespace: multiclustertest spec: template: data: username: dmVycmF6emFubw== password: dmVycmF6emFubw== spec: placement: clusters: - name: managed1 MultiClusterSecret A MultiClusterSecret is an envelope to create Kubernetes Secret resources on the clusters specified in the placement section.\n   Field Type Description Required     apiVersion string clusters.verrazzano.io/v1alpha1 Yes   kind string MultiClusterSecret Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec MultiClusterSecretSpec The desired state of a Kubernetes Secret. Yes   status MultiClusterResourceStatus The runtime status of a multicluster resource. No    MultiClusterSecretSpec MultiClusterSecretSpec specifies the desired state of a Kubernetes Secret.\n   Field Type Description Required     template SecretTemplate The embedded Kubernetes Secret. Yes   placement Placement Clusters in which the Secret is to be placed. Yes    SecretTemplate SecretTemplate has the metadata and spec of the Kubernetes Secret.\n   Field Type Description Required     metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   data map[string][]byte Corresponds to the data field of the struct Secret defined in types.go. No   stringData map[string]string Corresponds to the stringData field of the struct Secret defined in types.go. No   type string Corresponds to the type field of the struct Secret defined in types.go. No    ","excerpt":"The MultiClusterSecret custom resource is used to distribute Kubernetes Secret resources in a multicluster environment. Here is a sample MultiClusterSecret that specifies a Kubernetes secret to create …","ref":"/docs/reference/api/multicluster/multiclustersecret/","title":"MultiClusterSecret Custom Resource Definition"},{"body":"Network traffic refers to the data flowing across the network. In the context of this document, it is useful to think of network traffic from two perspectives: traffic based on direction and traffic related to component types, system or applications. Traffic direction is either north-south traffic, which enters and leaves the cluster, or east-west traffic, which stays within the cluster.\nFirst is a description of getting traffic into the cluster, then how traffic flows after it is in the cluster.\nIngress Ingress is an overloaded term, so it needs to be understood in context. Sometimes the term means external access into the cluster, as in “ingress to the cluster.” The term also refers to the Kubernetes Ingress resource. In addition, it might be used to mean network ingress to a container in a Pod. Here, it’s used to refer to both general ingress into the cluster and the Kubernetes Ingress resource.\nDuring installation, Verrazzano creates the necessary network resources to access both system components and applications. The following ingress and load balancers description is in the context of a Verrazzano installation.\nLoadBalancer Services To reach Pods from outside a cluster, an external IP address must be exposed using a LoadBalancer or NodePort service. Verrazzano creates two LoadBalancer services, one for system component traffic and another for application traffic. The specifics of how the service gets traffic into the cluster depends on the underlying Kubernetes platform. With Oracle OKE, creating a LoadBalancer type service will result in an OCI load balancer being created and configured to load balance to a set of Pods.\nIngress for system components To provide ingress to system components, Verrazzano installs a NGINX Ingress Controller, which includes a NGINX load balancer. Verrazzano also creates Kubernetes Ingress resources to configure ingress for each system component that requires ingress. An Ingress resource is used is to specify HTTP/HTTPS routes to Kubernetes services, along with an endpoint hostname and a TLS certificate. An Ingress by itself doesn’t do anything; it is just a resource. An ingress controller is needed to watch Ingress resources and reconcile them, configuring the underlying Kubernetes load balancer to handle the service routing. The NGINX Ingress Controller processes Ingress resources and configures NGINX with the ingress route information, and such.\nThe NGINX Ingress Controller is a LoadBalancer service, as seen here:\n$ kubectl get service -n ingress-nginx ... ingress-controller-ingress-nginx-controller LoadBalancer Using the OKE example, traffic entering the OCI load balancer is routed to the NGINX load balancer, then routed from there to the Pods belonging to the services described in the Ingress.\nIngress for applications Verrazzano also provides ingress into applications, but uses an Istio ingress gateway, which is an Envoy proxy, instead of NGINX. Istio has a Gateway resource that provides load balancer information, such as hosts, ports, and certificates for traffic coming into the mesh. For more information, see Istio Gateway. Just as an Ingress needs a corresponding Ingress controller, the same is true for the Gateway resource, where there is a corresponding Istio ingress gateway controller. However, unlike the Ingress, the Gateway resource doesn’t have service routing information. That is handled by the Istio VirtualService resource. The combination of Gateway and VirtualService is basically a superset of Ingress, because the combination provides more features than Ingress. In summary, the Istio ingress gateway provides ingress to the cluster using information from both the Gateway and VirtualService resources.\nBecause Verrazzano doesn’t create any applications during installations, there is no need to create a Gateway and VirtualService at that time. However, during installation, Verrazzano does create the Istio ingress gateway, which is a LoadBalancer service, along with the Istio egress gateway, which is a ClusterIP service.\n$ kubectl get service -n istio-system ... istio-ingressgateway LoadBalancer Again, referring to the OKE use case, this means that there will another OCI load balancer created, routing traffic to the Istio ingress gateway Pod, for example, the Envoy proxy.\nExternal DNS When you install Verrazzano, you can optionally specify an external DNS for your domain. If you do that, Verrazzano will not only create the DNS records, using ExternalDNS, but also it will configure your host name in the Ingress resources. You can then use that host name to access the system components through the NGINX Ingress Controller.\nSystem traffic System traffic includes all traffic that enters and leaves system Pods.\nNorth-south system traffic North-south traffic includes all system traffic that enters or leaves a Kubernetes cluster.\nIngress The following lists the Verrazzano system components which are accessed through the NGINX Ingress Controller from a client external to the cluster:\n Elasticsearch Keycloak Kibana Grafana Prometheus Rancher Verrazzano Console Verrazzano API  Egress The following table shows Verrazzano system components that initiate requests to a destination outside the cluster.\n   Component Destination Description     cert-manager Let’s Encrypt Get signed certificate.   Elasticsearch Keycloak OIDC sidecar calls Keycloak for authentication, which includes redirects.   ExternalDNS External DNS Create and delete DNS entries in an external DNS.   Fluentd Elasticsearch Fluentd on the managed cluster calls Elasticsearch on the admin cluster.   Grafana Keycloak OIDC sidecar calls Keycloak for authentication, which includes redirects.   Kibana Keycloak OIDC sidecar calls Keycloak for authentication, which includes redirects.   Prometheus Prometheus Prometheus on the admin cluster scrapes metrics from Prometheus on the managed cluster.   Rancher Agent Rancher Rancher agent on the managed cluster sends requests to Rancher on the admin cluster.   Verrazzano API Proxy Keycloak API proxy on the managed cluster calls Keycloak on the admin cluster.   Verrazzano Platform Operator Kubernetes API server Multicluster agent on the managed cluster calls API server on the admin cluster.    East-west system traffic The following tables show Verrazzano system components that send traffic to a destination inside the cluster, with the following exceptions:\n Usage of CoreDNS: It can be assumed that any Pod in the cluster can access CoreDNS for name resolution. Envoy to Istiod: The Envoy proxies all make requests to the Istio control plane to get dynamic configuration, and such. This includes both the gateways and the mesh sidecar proxies. That traffic is not shown. Traffic within a component is not shown, for example, traffic between Elasticsearch Pods. Prometheus scraping traffic is shown in the second table.     Component Destination Description     cert-manager Kubernetes API server Perform CRUD operations on Kubernetes resources.   Elasticsearch Keycloak OIDC sidecar calls Keycloak for token authentication.   Fluentd Elasticsearch Fluentd sends data to Elasticsearch.   Grafana Prometheus UI for Prometheus data.   Grafana Keycloak OIDC sidecar calls Keycloak for token authentication.   Kibana Elasticsearch UI for Elasticsearch.   Kibana Keycloak OIDC sidecar calls Keycloak for token authentication.   NGINX Ingress Controller Kubernetes API server Perform CRUD operations on Kubernetes resources.   Istio Kubernetes API server Perform CRUD operations on Kubernetes resources.   Prometheus Keycloak OIDC sidecar calls Keycloak for token authentication.   Rancher Kubernetes API server Perform CRUD operations on Kubernetes resources.   Verrazzano API Proxy Kubernetes API server Perform CRUD operations on Kubernetes resources.   Verrazzano Application Operator Kubernetes API server Perform CRUD operations on Kubernetes resources.   Verrazzano Monitoring Operator Kubernetes API server Perform CRUD operations on Kubernetes resources.   Verrazzano Operator Kubernetes API server Perform CRUD operations on Kubernetes resources.   Verrazzano Platform Operator Kubernetes API server Perform CRUD operations on Kubernetes resources.   Verrazzano Platform Operator Rancher Register the managed cluster with Rancher.    Prometheus scraping traffic This table shows Prometheus traffic for each system component scrape target.\n   Target Description     cadvisor Kubernetes metrics.   Elasticsearch Envoy metrics.   Grafana Envoy metrics.   Istiod Istio control plane metrics.   Istiod Envoy metrics.   Istio egress gateway Envoy metrics.   Istio ingress gateway Envoy metrics.   Keycloak Envoy metrics.   Kibana Envoy metrics.   MySQL Envoy metrics.   NGINX Ingress Controller Envoy metrics.   NGINX Ingress Controller NGINX metrics.   NGINX default back end Envoy metrics.   Node exporter Node metrics.   Prometheus Envoy metrics.   Prometheus Prometheus metrics.   Verrazzano console Envoy metrics.   Verrazzano API Envoy metrics.   WebLogic operator Envoy metrics.    Webhooks Several of the system components are controllers, and some of those have webhooks. Webhooks are called by the Kubernetes API server on a component HTTPS port to validate or mutate API payloads before they reach the API server.\nThe following components use webhooks:\n cert-manager Coherence Operator Istio Rancher Verrazzano Application Operator Verrazzano Platform Operator  Application traffic Application traffic includes all traffic to and from Verrazzano applications.\nNorth-south application traffic After Verrazzano is installed, you can deploy applications into the Istio mesh. When doing so, you will likely need ingress into the application. As previously mentioned, this can be done with Istio using the Gateway and VirtualService resources. Verrazzano will create those resources for you when you use an IngressTrait in your ApplicationConfiguration. The Istio ingress gateway created during installation will be shared by all applications in the mesh, and the Gateway resource is bound to the Istio ingress gateway that was created during installation. This is done by the selector field in the Gateway:\n selector: istio: ingressgateway Verrazzano creates a Gateway/VirtualService pair for each IngressTrait. Following is an example of those two resources created by Verrazzano.\nHere is the Gateway; in this case both the host name and certificate were generated by Verrazzano.\napiVersion: v1 items: - apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: ... name: hello-helidon-hello-helidon-appconf-gw namespace: hello-helidon ... spec: selector: istio: ingressgateway servers: - hosts: - hello-helidon-appconf.hello-helidon.1.2.3.4.nip.io port: name: HTTPS number: 443 protocol: HTTPS tls: credentialName: hello-helidon-hello-helidon-appconf-cert-secret mode: SIMPLE Here is the VirtualService; notice that it refers back to the Gateway and that it contains the service routing information.\napiVersion: v1 items: - apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: ... name: hello-helidon-ingress-rule-0-vs namespace: hello-helidon spec: gateways: - hello-helidon-hello-helidon-appconf-gw hosts: - hello-helidon-appconf.hello-helidon.1.2.3.4.nip.io HTTP: - match: - uri: prefix: /greet route: - destination: host: hello-helidon port: number: 8080 East-west application traffic To manage east-west traffic, each service in the mesh should be routed using a VirtualService and an optional DestinationRule. You can still send east-west traffic without either of these resources, but you won’t get any custom routing or load balancing. Verrazzano doesn’t configure east-west traffic. Consider bobbys-front-end in the Bob’s Books example at bobs-books-comp.yaml. When deploying Bob’s Books, a VirtualService is created for bobbys-front-end, because of the IngressTrait, but there are no VirtualServices for the other services in the application. When bobbys-front-end sends requests to bobbys-helidon-stock-application, this east-west traffic still goes to bobbys-helidon-stock-application through the Envoy sidecar proxies in the source and destination Pods, but there is no VirtualService representing bobbys-helidon-stock-application, where you could specify a canary deployment or custom load balancing. This is something you could configure manually, but it is not configured by Verrazzano.\nProxies Verrazzano uses network proxies in multiple places. The two proxy products are Envoy and NGINX. The following table shows which proxies are used and in which Pod they run.\n   Usage Proxy Pod Namespace Description     System ingress NGINX ingress-controller-ingress-nginx-controller-* ingress-nginx Provides external access to Verrazzano system components.   OIDC proxy sidecar NGINX vmi-system-es-ingest-* verrazzano-system Elasticsearch authentication.   OIDC proxy sidecar NGINX vmi-system-kibana-* verrazzano-system Elasticsearch authentication.   OIDC proxy sidecar NGINX vmi-system-prometheus-* verrazzano-system Elasticsearch authentication.   OIDC proxy sidecar NGINX vmi-system-grafana-* verrazzano-system Elasticsearch authentication.   OIDC proxy sidecar NGINX verrazzano-api-* verrazzano-system Verrazzano API server that proxies to Kubernetes API server.   Application ingress Envoy istio-ingressgateway-* istio-system Provides external access to Verrazzano applications.   Application egress Envoy istio-egressgateway-* istio-system Provides control of application egress traffic.   Istio mesh sidecar Envoy ingress-controller-ingress-nginx-controller-* ingress-nginx NGINX Ingress Controller in the Istio mesh.   Istio mesh sidecar Envoy ingress-controller-ingress-nginx-defaultbackend-* ingress-nginx NGINX default backend in the Istio mesh.   Istio mesh sidecar Envoy fluentd-* verrazzano-system Fluentd in the Istio mesh.   Istio mesh sidecar Envoy keycloak-* keycloak Keycloak in the Istio mesh.   Istio mesh sidecar Envoy mysql-* keycloak MySQL used by Keycloak in the Istio mesh.   Istio mesh sidecar Envoy verrazzano-api-* verrazzano-system Verrazzano API in the Istio mesh.   Istio mesh sidecar Envoy verrazzano-console-* verrazzano-system Verrazzano console in the Istio mesh.   Istio mesh sidecar Envoy vmi-system-es-master-* verrazzano-system Elasticsearch in the Istio mesh.   Istio mesh sidecar Envoy vmi-system-es-data-* verrazzano-system Elasticsearch in the Istio mesh.   Istio mesh sidecar Envoy vmi-system-es-ingest-* verrazzano-system Elasticsearch in the Istio mesh.   Istio mesh sidecar Envoy vmi-system-kibana-* verrazzano-system Kibana in the Istio mesh.   Istio mesh sidecar Envoy vmi-system-prometheus-* verrazzano-system Prometheus in the Istio mesh.   Istio mesh sidecar Envoy vmi-system-grafana-* verrazzano-system Grafana in the Istio mesh.   Istio mesh sidecar Envoy weblogic-operator-* verrazzano-system WebLogic operator in the Istio mesh.    Multicluster Some Verrazzano components send traffic between Kubernetes clusters. Those components are the Verrazzano agent, Verrazzano API proxy, the OIDC proxy, and Prometheus.\nMulticluster egress The following table shows Verrazzano system components that initiate requests between the admin and managed clusters. All of these requests go through the NGINX Ingress Controller on the respective destination cluster.\n   Source Cluster Source Component Destination Cluster Destination Component Description     Admin Prometheus Managed Prometheus Scape metrics on managed clusters.   Admin Verrazzano Console Managed Verrazzano API proxy Console proxy sends requests to API proxy.   Managed Fluentd Admin Elasticsearch Fluentd sends logs to Elasticsearch.   Managed Elasticsearch Admin Keycloak OIDC sidecar sends requests to Keycloak.   Managed Kibana Admin Keycloak OIDC sidecar sends requests to Keycloak.   Managed Grafana Admin Keycloak OIDC sidecar sends requests to Keycloak.   Managed Prometheus Admin Keycloak OIDC sidecar sends requests to Keycloak.   Managed Rancher Agent Admin Rancher Rancher Agent sends requests Rancher.   Managed Verrazzano API proxy Admin Keycloak API proxy sends requests to Keycloak.   Managed Verrazzano Agent Admin Kubernetes API server Agent, in the platform operator, sends requests Kubernetes API server.    Verrazzano agent In the multicluster topology, the Verrazzano platform operator has an agent thread running on the managed cluster that sends requests to the Kubernetes API server on the admin cluster. The URL for the admin cluster Kubernetes API server is registered on the managed cluster by the user.\nVerrazzano API proxy In a multicluster topology, the Verrazzano API proxy runs on both the admin and managed clusters.\nOn the admin cluster, the API proxy connects to in-cluster Keycloak, using the Keycloak Service. On the managed cluster, the API proxy connects to Keycloak on the admin cluster through the NGINX Ingress controller running on the admin cluster.\nVerrazzano OIDC proxy The OIDC proxy runs as a sidecar in the Elasticsearch, Kibana, Prometheus, and Grafana Pods. This proxy also needs to send requests to Keycloak, either in-cluster or through the cluster ingress. When a request comes into the OIDC proxy without an authentication header, the proxy sends a request to Keycloak through the NGINX Ingress Controller, so the request exits the cluster. Otherwise, if OIDC is on the admin cluster, then the request is sent directly to Keycloak within the cluster. If OIDC is on the managed cluster, then it must send requests to Keycloak on the admin cluster.\nPrometheus A single Prometheus service in the cluster, scrapes metrics from Pods in system components and applications. It also scrapes Pods in the Istio mesh using HTTPS, and outside the mesh using HTTP. In the multicluster case, the Prometheus on the admin cluster, scrapes metrics from Prometheus on the managed cluster, through the NGINX Ingress Controller on the managed cluster.\n","excerpt":"Network traffic refers to the data flowing across the network. In the context of this document, it is useful to think of network traffic from two perspectives: traffic based on direction and traffic …","ref":"/docs/networking/traffic/net-traffic/","title":"Network Traffic"},{"body":"The Placement subresource is shared by multicluster custom resources.\nPlacement Placement contains the name of each cluster where this resource will be located.\n   Field Type Description Required     clusters Cluster array An array of cluster locations. Yes    Cluster Cluster contains the name of a single cluster.\n   Field Type Description Required     cluster string The name of a cluster. Yes    ","excerpt":"The Placement subresource is shared by multicluster custom resources.\nPlacement Placement contains the name of each cluster where this resource will be located.\n   Field Type Description Required …","ref":"/docs/reference/api/multicluster/placement/","title":"Placement Subresource"},{"body":"Prerequisites The Quick Start assumes that you have already installed a Kubernetes cluster. For instructions on preparing Kubernetes platforms for installing Verrazzano, see Platform Setup. For detailed installation instructions, see the Installation Guide.\nVerrazzano requires the following:\n A Kubernetes cluster and a compatible kubectl. At least 2 CPUs, 100GB disk storage, and 16GB RAM available on the Kubernetes worker nodes. This is sufficient to install the development profile of Verrazzano. Depending on the resource requirements of the applications you deploy, this may or may not be sufficient for deploying your applications.  For a list of the open source components and versions installed with Verrazzano, see Software versions.\nNOTE Verrazzano has been tested only on the following versions of Kubernetes: 1.17.x, 1.18.x, 1.19.x, and 1.20x. Other versions have not been tested and are not guaranteed to work.  Install the Verrazzano platform operator Verrazzano provides a Kubernetes operator to manage the life cycle of Verrazzano installations. The operator works with a custom resource defined in the cluster. You can install, uninstall, and update Verrazzano installations by updating the Verrazzano custom resource. The Verrazzano platform operator controller will apply the configuration from the custom resource to the cluster for you.\nTo install the Verrazzano platform operator:\n  Deploy the Verrazzano platform operator.\n$ kubectl apply -f https://github.com/verrazzano/verrazzano/releases/download/v0.17.0/operator.yaml   Wait for the deployment to complete.\n$ kubectl -n verrazzano-install rollout status deployment/verrazzano-platform-operator deployment \"verrazzano-platform-operator\" successfully rolled out   Confirm that the operator pod is correctly defined and running.\n$ kubectl -n verrazzano-install get pods NAME READY STATUS RESTARTS AGE verrazzano-platform-operator-59d5c585fd-lwhsx 1/1 Running 0 114s   Install Verrazzano You install Verrazzano by creating a Verrazzano custom resource in your Kubernetes cluster. Verrazzano currently supports a default production (prod) profile and a development (dev) profile suitable for evaluation.\nThe development profile has the following characteristics:\n Wildcard (nip.io) DNS Self-signed certificates Shared observability stack used by the system components and all applications Ephemeral storage for the observability stack (if the pods are restarted, you lose all of your logs and metrics) Single-node, reduced memory Elasticsearch cluster  To install Verrazzano:\n  Install Verrazzano with its dev profile.\n$ kubectl apply -f - \u003c\u003cEOF apiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: example-verrazzano spec: profile: dev EOF   Wait for the installation to complete.\n$ kubectl wait \\  --timeout=20m \\  --for=condition=InstallComplete \\  verrazzano/example-verrazzano   (Optional) View the installation logs.\nThe Verrazzano operator launches a Kubernetes job to install Verrazzano. You can view the installation logs from that job with the following command:\n$ kubectl logs -f \\  $( \\  kubectl get pod \\  -l job-name=verrazzano-install-example-verrazzano \\  -o jsonpath=\"{.items[0].metadata.name}\" \\  )   Deploy an example application The Hello World Helidon example application provides a simple Hello World REST service written with Helidon. For more information and the code of this application, see the Verrazzano Examples.\nTo deploy the Hello World Helidon example application:\n  Create a namespace for the example application and add labels identifying the namespace as managed by Verrazzano and enabled for Istio.\n$ kubectl create namespace hello-helidon $ kubectl label namespace hello-helidon verrazzano-managed=true istio-injection=enabled   Apply the hello-helidon resources to deploy the application.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-comp.yaml $ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-app.yaml   Wait for the application to be ready.\n$ kubectl wait \\  --for=condition=Ready pods \\  --all -n hello-helidon \\  --timeout=300s pod/hello-helidon-deployment-78468f5f9c-czmp4 condition met This creates the Verrazzano OAM component application resources for the example, waits for the pods in the hello-helidon namespace to be ready.\n  Save the host name of the load balancer exposing the application’s REST service endpoints.\n$ HOST=$(kubectl get gateway hello-helidon-hello-helidon-appconf-gw \\  -n hello-helidon \\  -o jsonpath='{.spec.servers[0].hosts[0]}')   Get the default message.\n$ curl -sk \\  -X GET \\  \"https://${HOST}/greet\" {\"message\":\"Hello World!\"}   Uninstall the example application To uninstall the Hello World Helidon example application:\n  Delete the Verrazzano application resources.\n$ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-comp.yaml $ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-app.yaml   Delete the example namespace.\n$ kubectl delete namespace hello-helidon namespace \"hello-helidon\" deleted   Verify that the hello-helidon namespace has been deleted.\n$ kubectl get ns hello-helidon Error from server (NotFound): namespaces \"hello-helidon\" not found   Uninstall Verrazzano To uninstall Verrazzano:\n  Delete the Verrazzano custom resource.\n$ kubectl delete verrazzano example-verrazzano  NOTE This command blocks until the uninstall has completed. To follow the progress, you can view the uninstall logs.    (Optional) View the uninstall logs.\nThe Verrazzano operator launches a Kubernetes job to delete the Verrazzano installation. You can view the uninstall logs from that job with the following command:\n$ kubectl logs -f \\  $( \\  kubectl get pod \\  -l job-name=verrazzano-uninstall-example-verrazzano \\  -o jsonpath=\"{.items[0].metadata.name}\" \\  )   Next steps See the Verrazzano Example Applications.\n","excerpt":"Prerequisites The Quick Start assumes that you have already installed a Kubernetes cluster. For instructions on preparing Kubernetes platforms for installing Verrazzano, see Platform Setup. For …","ref":"/docs/quickstart/","title":"Quick Start"},{"body":"The Verrazzano custom resource contains the configuration information for an installation. Here is a sample Verrazzano custom resource file that uses OCI DNS. See other examples here.\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: my-verrazzano spec: environmentName: env profile: prod components: certManager: certificate: acme: provider: letsEncrypt emailAddress: emailAddress@domain.com dns: oci: ociConfigSecret: oci dnsZoneCompartmentOCID: dnsZoneCompartmentOcid dnsZoneOCID: dnsZoneOcid dnsZoneName: my.dns.zone.name ingress: type: LoadBalancer VerrazzanoSpec    Field Type Description Required     environmentName string Name of the installation. This name is part of the endpoint access URLs that are generated. The default value is default. No   profile string The installation profile to select. Valid values are prod (production) and dev (development). The default is prod. No   version string The version to install. Valid versions can be found here. Defaults to the current version supported by the Verrazzano platform operator. No   components Components The Verrazzano components. No   defaultVolumeSource VolumeSource Defines the type of volume to be used for persistence for all components unless overridden, and can be one of either EmptyDirVolumeSource or PersistentVolumeClaimVolumeSource. If PersistentVolumeClaimVolumeSource is declared, then the claimName must reference the name of an existing VolumeClaimSpecTemplate declared in the volumeClaimSpecTemplates section. No   volumeClaimSpecTemplates VolumeClaimSpecTemplate Defines a named set of PVC configurations that can be referenced from components to configure persistent volumes. No    VolumeClaimSpecTemplate    Field Type Description Required     metadata ObjectMeta Metadata about the PersistentVolumeClaimSpec template. No   spec PersistentVolumeClaimSpec A PersistentVolumeClaimSpec template that can be referenced by a Component to override its default storage settings for a profile. At present, only a subset of the resources.requests object are honored depending on the component. No    Components    Field Type Description Required     certManager CertManagerComponent The cert-manager component configuration. No   dns DNSComponent The DNS component configuration. No   ingress IngressComponent The ingress component configuration. No   istio IstioComponent The Istio component configuration. No   keycloak KeycloakComponent The Keycloak component configuration. No   elasticsearch ElasticsearchComponent The Elasticsearch component configuration. No   prometheus PrometheusComponent The Prometheus component configuration. No   kibana KibanaComponent The Kibana component configuration. No   grafana GrafanaComponent The Grafana component configuration. No    CertManager Component    Field Type Description Required     certificate Certificate The certificate configuration. No    Certificate    Field Type Description Required     acme Acme The ACME configuration. Either acme or ca must be specified. No   ca CertificateAuthority The certificate authority configuration. Either acme or ca must be specified. No    Acme    Field Type Description Required     provider string Name of the Acme provider. Yes   emailAddress string Email address of the user. Yes    CertificateAuthority    Field Type Description Required     secretName string The secret name. Yes   clusterResourceNamespace string The secrete namespace. Yes    DNS Component    Field Type Description Required     wildcard DNS-Wilcard Wildcard DNS configuration. This is the default with a domain of nip.io. No   oci DNS-OCI OCI DNS configuration. No   external DNS-External External DNS configuration. No    DNS Wildcard    Field Type Description Required     domain string The type of wildcard DNS domain. For example, nip.io, sslip.io, and such. Yes    DNS OCI    Field Type Description Required     ociConfigSecret string Name of the OCI configuration secret. Generate a secret based on the OCI configuration profile you want to use. You can specify a profile other than DEFAULT and specify the secret name. See instructions by running ./install/create_oci_config_secret.sh. Yes   dnsZoneCompartmentOCID string The OCI DNS compartment OCID. Yes   dnsZoneOCID string The OCI DNS zone OCID. Yes   dnsZoneName string Name of OCI DNS zone. Yes    DNS External    Field Type Description Required     suffix string The suffix for DNS names. Yes    Ingress Component    Field Type Description Required     type string The ingress type. Valid values are LoadBalancer and NodePort. The default value is LoadBalancer. Yes   ingressNginxArgs NameValue list The list of argument names and values. No   ports PortConfig list The list port configurations used by the ingress. No    Port Config    Field Type Description Required     name string The port name. No   port string The port value. Yes   targetPort string The target port value. The default is same as the port value. Yes   protocol string The protocol used by the port. TCP is the default. No   nodePort string The nodePort value. No    Name Value    Field Type Description Required     name string The argument name. Yes   value string The argument value. Either value or valueList must be specified. No   valueList string list The list of argument values. Either value or valueList must be specified. No   setString Boolean Specifies if the value is a string. No    Istio Component    Field Type Description Required     istioInstallArgs NameValue list A list of Istio Helm chart arguments and values to apply during the installation of Istio. Each argument is specified as either a name/value or name/valueList pair. No    Fluentd Component    Field Type Description Required     extraVolumeMounts ExtraVolumeMount list A list of host path volume mounts in addition to /var/log into the Fluentd DaemonSet. The Fluentd component collects log files in the /var/log/containers directory of Kubernetes worker nodes. The /var/log/containers directory may contain symbolic links to files located outside the /var/log directory. If the host path directory containing the log files is located outside of /var/log, the Fluentd DaemonSet must have the volume mount of that directory to collect the logs. No    Extra Volume Mount    Field Type Description Required     source string The source host path. Yes   destination string The destination path on the Fluentd Container, defaults to the source host path. No   readOnly Boolean Specifies if the volume mount is read-only, defaults to true. No    Keycloak Component    Field Type Description Required     enabled Boolean If true, then Keycloak will be installed. No   keycloakInstallArgs NameValue list Allows providing custom Helm arguments to install Keycloak. No   mysql MySQLComponent Contains the MySQL component configuration needed for Keycloak. No    MySQL Component    Field Type Description Required     mysqlInstallArgs NameValue list Allows providing custom Helm arguments to install MySQL for Keycloak. No   volumeSource VolumeSource Defines the type of volume to be used for persistence for Keycloak/MySQL, and can be one of either EmptyDirVolumeSource or PersistentVolumeClaimVolumeSource. If PersistentVolumeClaimVolumeSource is declared, then the claimName must reference the name of a VolumeClaimSpecTemplate declared in the volumeClaimSpecTemplates section. No    Elasticsearch Component    Field Type Description Required     enabled Boolean If true, then Elasticsearch will be installed. No    Kibana Component    Field Type Description Required     enabled Boolean If true, then Kibana will be installed. No    Prometheus Component    Field Type Description Required     enabled Boolean If true, then Prometheus will be installed. No    Grafana Component    Field Type Description Required     enabled Boolean If true, then Grafana will be installed. No    ","excerpt":"The Verrazzano custom resource contains the configuration information for an installation. Here is a sample Verrazzano custom resource file that uses OCI DNS. See other examples here.\napiVersion: …","ref":"/docs/reference/api/verrazzano/verrazzano/","title":"Verrazzano Custom Resource Definition"},{"body":"VerrazzanoCoherenceWorkload The VerrazzanoCoherenceWorkload custom resource contains the configuration information for a Coherence workload within Verrazzano. Here is a sample component that specifies a VerrazzanoCoherenceWorkload. To deploy an example application that demonstrates this workload type, see Sock Shop.\napiVersion: core.oam.dev/v1alpha2 kind: Component metadata: name: carts namespace: sockshop spec: workload: apiVersion: oam.verrazzano.io/v1alpha1 kind: VerrazzanoCoherenceWorkload spec: template: metadata: name: carts-coh spec: cluster: SockShop role: Carts replicas: 1 image: ghcr.io/helidon-sockshop/carts-coherence:2.2.0 imagePullPolicy: Always application: type: helidon jvm: args: - \"-Dcoherence.k8s.operator.health.wait.dcs=false\" - \"-Dcoherence.metrics.legacy.names=false\" memory: heapSize: 2g coherence: logLevel: 9 ports: - name: http port: 7001 service: name: carts port: 80 serviceMonitor: enabled: true - name: metrics port: 7001 serviceMonitor: enabled: true VerrazzanoCoherenceWorkload    Field Type Description Required     apiVersion string oam.verrazzano.io/v1alpha1 Yes   kind string VerrazzanoCoherenceWorkload Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec VerrazzanoCoherenceWorkloadSpec The desired state of a Verrazzano Coherence workload. Yes    VerrazzanoCoherenceWorkloadSpec VerrazzanoCoherenceWorkloadSpec specifies the desired state of a Verrazzano Coherence workload.\n   Field Type Description Required     template RawExtension The metadata and spec for the underlying Coherence resource. Yes    VerrazzanoHelidonWorkload The VerrazzanoHelidonWorkload custom resource contains the configuration information for a Helidon workload within Verrazzano. Here is a sample component that specifies a VerrazzanoHelidonWorkload. To deploy an example application that demonstrates this workload type, see Hello World Helidon.\napiVersion: core.oam.dev/v1alpha2 kind: Component metadata: name: hello-helidon-component namespace: hello-helidon spec: workload: apiVersion: oam.verrazzano.io/v1alpha1 kind: VerrazzanoHelidonWorkload metadata: name: hello-helidon-workload labels: app: hello-helidon spec: deploymentTemplate: metadata: name: hello-helidon-deployment podSpec: containers: - name: hello-helidon-container image: \"ghcr.io/verrazzano/example-helidon-greet-app-v1:0.1.10-3-20201016220428-56fb4d4\" ports: - containerPort: 8080 name: http VerrazzanoHelidonWorkload    Field Type Description Required     apiVersion string oam.verrazzano.io/v1alpha1 Yes   kind string VerrazzanoHelidonWorkload Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec VerrazzanoHelidonWorkloadSpec The desired state of a Verrazzano Helidon workload. Yes    VerrazzanoHelidonWorkloadSpec VerrazzanoHelidonWorkloadSpec specifies the desired state of a Verrazzano Helidon workload.\n   Field Type Description Required     deploymentTemplate DeploymentTemplate The embedded deployment. Yes    DeploymentTemplate DeploymentTemplate specifies the metadata and pod spec of the underlying deployment.\n   Field Type Description Required     metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   strategy DeploymentStrategy The replacement strategy of the underlying deployment. No   podSpec PodSpec The pod spec of the underlying deployment. Yes    VerrazzanoWebLogicWorkload The VerrazzanoWebLogicWorkload custom resource contains the configuration information for a WebLogic Domain workload within Verrazzano. Here is a sample component that specifies a VerrazzanoWebLogicWorkload. To deploy an example application that demonstrates this workload type, see the ToDo List Lift-and-Shift application.\napiVersion: core.oam.dev/v1alpha2 kind: Component metadata: name: todo-domain namespace: todo-list spec: workload: apiVersion: oam.verrazzano.io/v1alpha1 kind: VerrazzanoWebLogicWorkload spec: template: metadata: name: todo-domain namespace: todo-list spec: domainUID: tododomain domainHome: /u01/domains/tododomain image: container-registry.oracle.com/verrazzano/example-todo:0.8.0 imagePullSecrets: - name: tododomain-repo-credentials domainHomeSourceType: \"FromModel\" includeServerOutInPodLog: true replicas: 1 webLogicCredentialsSecret: name: tododomain-weblogic-credentials configuration: introspectorJobActiveDeadlineSeconds: 900 model: configMap: tododomain-jdbc-config domainType: WLS modelHome: /u01/wdt/models runtimeEncryptionSecret: tododomain-runtime-encrypt-secret secrets: - tododomain-jdbc-tododb serverPod: env: - name: JAVA_OPTIONS value: \"-Dweblogic.StdoutDebugEnabled=false\" - name: USER_MEM_ARGS value: \"-Djava.security.egd=file:/dev/./urandom -Xms64m -Xmx256m \" - name: WL_HOME value: /u01/oracle/wlserver - name: MW_HOME value: /u01/oracle VerrazzanoWebLogicWorkload    Field Type Description Required     apiVersion string oam.verrazzano.io/v1alpha1 Yes   kind string VerrazzanoWebLogicWorkload Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec VerrazzanoWebLogicWorkloadSpec The desired state of a Verrazzano WebLogic workload. Yes    VerrazzanoWebLogicWorkloadSpec VerrazzanoWebLogicWorkloadSpec specifies the desired state of a Verrazzano WebLogic workload.\n   Field Type Description Required     template RawExtension The metadata and spec for the underlying WebLogic Domain resource. Yes    ","excerpt":"VerrazzanoCoherenceWorkload The VerrazzanoCoherenceWorkload custom resource contains the configuration information for a Coherence workload within Verrazzano. Here is a sample component that specifies …","ref":"/docs/reference/api/oam/workloads/","title":"Verrazzano Workload Custom Resource Definitions"},{"body":"The VerrazzanoManagedCluster custom resource is used to register a managed cluster with an admin cluster. Here is a sample VerrazzanoManagedCluster that registers the cluster named managed1. To deploy an example application that demonstrates a VerrazzanoManagedCluster, see Multicluster Hello World Helidon.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: VerrazzanoManagedCluster metadata: name: managed1 namespace: verrazzano-mc spec: description: \"Managed Cluster 1\" caSecret: ca-secret-managed1 VerrazzanoManagedCluster    Field Type Description Required     apiVersion string clusters.verrazzano.io/v1alpha1 Yes   kind string VerrazzanoManagedCluster Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec VerrazzanoManagedClusterSpec The managed cluster specification. Yes   status VerrazzanoManagedClusterStatus The runtime status this resource. No    VerrazzanoManagedClusterSpec VerrazzanoManagedClusterSpec specifies a managed cluster to associate with an admin cluster.\n   Field Type Description Required     description string The description of the managed cluster. No   caSecret string The name of a Secret that contains the CA certificate of the managed cluster. This is used to configure the admin cluster to scrape metrics from the Prometheus endpoint on the managed cluster. See the steps 3 and 4 in instructions for how to create this Secret. Yes   serviceAccount string The name of the ServiceAccount that was generated for the managed cluster. This field is managed by a Verrazzano Kubernetes operator. No   managedClusterManifestSecret string The name of the Secret containing generated YAML manifest file to be applied by the user to the managed cluster. This field is managed by a Verrazzano Kubernetes operator. No    VerrazzanoManagedClusterStatus    Field Type Description Required     conditions Condition array The current state of this resource. No   lastAgentConnectTime string The last time the agent from this managed cluster connected to the admin cluster. No   apiUrl string The Verrazzano API server URL for the managed cluster. No    Condition Condition describes current state of this resource.\n   Field Type Description Required     type string The condition of the multicluster resource which can be checked with a kubectl wait command. Condition values are case-sensitive and formatted as follows: Ready: the VerrazzanoManagedCluster is ready to be used and all resources needed have been generated. Yes   status ConditionStatus An instance of the type ConditionStatus that is defined in types.go. Yes   lastTransitionTime string The last time the condition transitioned from one status to another. No   message string A message with details about the last transition. No    ","excerpt":"The VerrazzanoManagedCluster custom resource is used to register a managed cluster with an admin cluster. Here is a sample VerrazzanoManagedCluster that registers the cluster named managed1. To deploy …","ref":"/docs/reference/api/multicluster/verrazzanomanagedcluster/","title":"VerrazzanoManagedCluster Custom Resource Definition"},{"body":"The VerrazzanoProject custom resource is used to create the application namespaces and their associated security settings on one or more clusters. The namespaces are always created on the admin cluster. Here is a sample VerrazzanoProject that specifies a namespace to create on the cluster named managed1.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: VerrazzanoProject metadata: name: hello-helidon namespace: verrazzano-mc spec: template: namespaces: - metadata: name: hello-helidon placement: clusters: - name: managed1 VerrazzanoProject    Field Type Description Required     apiVersion string clusters.verrazzano.io/v1alpha1 Yes   kind string VerrazzanoProject Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec VerrazzanoProjectSpec The project specification. Yes   status MultiClusterResourceStatus The runtime status of a multicluster resource. No    VerrazzanoProjectSpec VerrazzanoProjectSpec specifies the namespaces to create and on which clusters to create them.\n   Field Type Description Required     template ProjectTemplate The project template. Yes   placement Placement Clusters on which the namespaces are to be created. Yes    ProjectTemplate ProjectTemplate contains the list of namespaces to create and the optional security configuration for each namespace.\n   Field Type Description Required     namespaces NamespaceTemplate array The list of application namespaces to create for this project. Yes   security SecuritySpec The project security configuration. No   networkPolicies NetworkPolicyTemplate array The network policies applied to namespaces in the project. No    NamespaceTemplate NamespaceTemplate contains the metadata and specification of a Kubernetes namespace.\n   Field Type Description Required     metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec NamespaceSpec An instance of the struct NamespaceSpec defined in types.go. No    SecuritySpec SecuritySpec defines the security configuration for a project.\n   Field Type Description Required     projectAdminSubjects Subject The subject to bind to the verrazzano-project-admin role. Encoded as an instance of the struct Subject defined in types.go. No   projectMonitorSubjects Subject The subject to bind to the verrazzano-project-monitoring role. Encoded as an instance of the struct Subject defined in types.go. No    NetworkPolicyTemplate NetworkPolicyTemplate contains the metadata and specification of the underlying NetworkPolicy. NOTE To add application NetworkPolicy, see NetworkPolicies for applications.     Field Type Description Required     metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec NetworkPolicySpec An instance of the struct NetworkPolicySpec defined in types.go. No    ","excerpt":"The VerrazzanoProject custom resource is used to create the application namespaces and their associated security settings on one or more clusters. The namespaces are always created on the admin …","ref":"/docs/reference/api/multicluster/verrazzanoproject/","title":"VerrazzanoProject Custom Resource Definition"},{"body":"The Hello World Helidon example is a Helidon-based service that returns a “Hello World” response when invoked. The example application is specified using Open Application Model (OAM) component and application configuration YAML files, and then deployed by applying those files.\nThe example application has two endpoints, which differ in configuration source:\n /greet- uses a microprofile properties file. Deploy this application by using the instructions here. /config- uses a Kubernetes ConfigMap. Deploy this application by using the instructions here.  For more information and the code of this application, see the Verrazzano examples.\n","excerpt":"The Hello World Helidon example is a Helidon-based service that returns a “Hello World” response when invoked. The example application is specified using Open Application Model (OAM) component and …","ref":"/docs/samples/hello-world/","title":"Hello World Helidon"},{"body":"OKE Missing Security List Ingress Rules The install scripts perform a check, which attempts access through the ingress ports. If the check fails, then the install will exit and you will see error messages like this:\nERROR: Port 443 is NOT accessible on ingress(132.145.66.80)! Check that security lists include an ingress rule for the node port 31739.\nOn an OKE install, this may indicate that there is a missing ingress rule or rules. To verify and fix the issue, do the following:\n Get the ports for the LoadBalancer services.  Run kubectl get services -A. Note the ports for the LoadBalancer type services. For example 80:31541/TCP,443:31739/TCP.   Check the security lists in the OCI Console.  Go to Networking/Virtual Cloud Networks. Select the related VCN. Go to the Security Lists for the VCN. Select the security list named oke-wkr-.... Check the ingress rules for the security list. There should be one rule for each of the destination ports named in the LoadBalancer services. In the above example, the destination ports are 31541 \u0026 31739. We would expect the ingress rule for 31739 to be missing because it was named in the ERROR output. If a rule is missing, then add it by clicking Add Ingress Rules and filling in the source CIDR and destination port range (missing port). Use the existing rules as a guide.    ","excerpt":"OKE Missing Security List Ingress Rules The install scripts perform a check, which attempts access through the ingress ports. If the check fails, then the install will exit and you will see error …","ref":"/docs/troubleshooting/troubleshooting/","title":"Known Issues"},{"body":"Verrazzano manages and secures network traffic between Verrazzano system components and deployed applications. Verrazzano does not manage or secure traffic for the Kubernetes cluster itself, or for non-Verrazzano services or applications running in the cluster. Traffic is secured at two levels in the network stack:\n ISO Layer 3/4: Using NetworkPolicies to control IP access to Pods. ISO Layer 6: Using TLS and mTLS to provide authentication, confidentiality, and integrity for connections within the cluster, and for external connections.  NetworkPolicies By default, all Pods in a Kubernetes cluster have network access to all other Pods in the cluster. Kubernetes has a NetworkPolicy resource that provides network level 3 and 4 security for Pods, restricting both ingress and egress IP traffic for a set of Pods in a namespace. Verrazzano configures all system components with NetworkPolicies to control ingress. Egress is not restricted.\nNOTE: A NetworkPolicy resource needs a NetworkPolicy controller to implement the policy, otherwise the policy has no effect. You must install a Kubernetes CNI plug-in that provides a NetworkPolicy controller, such as Calico, before installing Verrazzano, or else the policies are ignored.\nNetworkPolicies for system components Verrazzano installs a set of NetworkPolicies for system components to control ingress into the Pods. A policy is scoped to a namespace and uses selectors to specify the Pods that the policy applies to, along with the ingress and egress rules. For example, the following policy applies to the Verrazzano API Pod in the verrazzano-system namespace. This policy allows network traffic from NGINX Ingress Controller on port 8775, and from Prometheus on port 15090. No other Pods can reach those ports or any other ports of the Verrazzano API Pod. Notice that namespace selectors need to be used; the NetworkPolicy resource does not support specifying the namespace name.\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy ... spec: PodSelector: matchLabels: app: verrazzano-api ingress: - from: - namespaceSelector: matchLabels: verrazzano.io/namespace: ingress-nginx PodSelector: matchLabels: app.kubernetes.io/instance: ingress-controller ports: - port: 8775 protocol: TCP - from: - namespaceSelector: matchLabels: verrazzano.io/namespace: verrazzano-system PodSelector: matchLabels: app: system-prometheus ports: - port: 15090 protocol: TCP The following table shows all of the ingresses that allow network traffic into system components. The ports shown are Pod ports, which is what NetworkPolicies require.\n   Component Pod Port From Description      Verrazzano Application Operator 9443 Kubernetes API Server Webhook entrypoint.    Verrazzano Platform Operator 9443 Kubernetes API Server Webhook entrypoint.    Verrazzano Console 8000 NGINX Ingress Access from external client.    Verrazzano Console 15090 Prometheus Prometheus scraping.    Verrazzano Proxy 8775 NGINX Ingress Access from external client.    Verrazzano Proxy 15090 Prometheus Prometheus scraping.    cert-manager 9402 Prometheus Prometheus scraping.    Coherence Operator 9443 Prometheus Webhook entrypoint.    Elasticsearch 8775 NGINX Ingress Access from external client.    Elasticsearch 8775 Fluentd Access from Fluentd.    Elasticsearch 9200 Kibana, Internal Elasticsearch data port.    Elasticsearch 9300 Internal Elasticsearch cluster port.    Elasticsearch 15090 Prometheus Envoy metrics scraping.    Istio control plane 15012 Envoy Envoy access to istiod.    Istio control plane 15014 Prometheus Prometheus scraping.    Istio control plane 15017 Kubernetes API Server Webhook entrypoint.    Istio ingress gateway 8443 External Application ingress.    Istio ingress gateway 15090 Prometheus Prometheus scraping.    Istio egress gateway 8443 Mesh services Application egress.    Istio egress gateway 15090 Prometheus Prometheus scraping.    Keycloak 8080 NGINX Ingress Access from external client.    Keycloak 15090 Prometheus Prometheus scraping.    MySql 15090 Prometheus Prometheus scraping.    MySql 3306 Keycloak Keycloak datastore.    Node exporter 9100 Prometheus Prometheus scraping.    Rancher 80 NGINX Ingress Access from external client.    Rancher 9443 Kubernetes API Server Webhook entrypoint.    Prometheus 8775 NGINX Ingress Access from external client.    Prometheus 9090 Grafana Acccess for Grafana UI.     NetworkPolicies for applications By default, applications do not have NetworkPolicies that restrict ingress into the application or egress from it. You can configure them for the application namespaces using the NetworkPolicy section of a Verrazzano project.\nNOTE Verrazzano requires specific ingress to and egress from application pods. If you add a NetworkPolicy for your application namespace or pods, you must add an additional policy to ensure that Verrazzano still has the required access it needs. The ingress policy is only needed if you restrict ingress. Likewise, the egress policy is only needed if you restrict egress. Following are the ingress and egress NetworkPolicies:\n ingress NetworkPolicies  ingress: - from: - namespaceSelector: matchLabels: verrazzano.io/namespace: istio-system podSelector: matchLabels: app: istiod - from: - namespaceSelector: matchLabels: verrazzano.io/namespace: istio-system podSelector: matchLabels: app: istio-ingressgateway - from: - namespaceSelector: matchLabels: verrazzano.io/namespace: verrazzano-system podSelector: matchLabels: app: system-prometheus - from: - namespaceSelector: matchLabels: verrazzano.io/namespace: verrazzano-system podSelector: matchLabels: app: coherence-operator - from: - namespaceSelector: matchLabels: verrazzano.io/namespace: verrazzano-system podSelector: matchLabels: app: weblogic-operator   egress NetworkPolicies  egress: - ports: - port: 15012 protocol: TCP to: - namespaceSelector: matchLabels: verrazzano.io/namespace: istio-system podSelector: matchLabels: app: istiod - to: - namespaceSelector: matchLabels: verrazzano.io/namespace: istio-system podSelector: matchLabels: app: istio-egressgateway - ports: - port: 53 protocol: TCP - port: 53 protocol: UDP to: - namespaceSelector: matchLabels: verrazzano.io/namespace: kube-system - ports: - port: 8000 protocol: TCP to: - namespaceSelector: matchLabels: verrazzano.io/namespace: verrazzano-system podSelector: matchLabels: app: coherence-operator   NetworkPolicies for Envoy sidecar proxies As mentioned, Envoy sidecar proxies run in both system component pods and application pods. Each proxy sends requests to the Istio control plane pod, istiod, for a variety of reasons. During installation, Verrazzano creates a NetworkPolicy named istiod-access in the istio-system namespace to give ingress to system component sidecar proxies. For applications, Verrazzano creates a per-application NetworkPolicy in the istio-system namespace to allow the same access to istiod. When the application is deleted, Verrazzano will delete the policy.\nmTLS Istio can be enabled to use mTLS between services in the mesh, and also between the Istio gateways and Envoy sidecar proxies. There are various options to customize mTLS usage, for example it can be disabled on a per-port level. The Istio control plane, Istiod, is a CA and provides key and certificate rotation for the Envoy proxies, both gateways and sidecars.\nVerrazzano configures Istio to have strict mTLS for the mesh. All components and applications put into the mesh will use mTLS, with the exception of Coherence clusters, which are not in the mesh. Also, all traffic between the Istio ingress gateway and mesh sidecars use mTLS, and the same is true between the proxy sidecars and the egress gateway.\nVerrazzano sets up mTLS during installation with the PeerAuthentication resource as follows:\napiVersion: v1 items: - apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication ... spec: mtls: mode: STRICT TLS TLS is used by external clients to access the cluster, both through the NGINX Ingress Controller and the Istio ingress gateway. The certificate used by these TLS connections vary; see Verrazzano security for details. All TLS connections are terminated at the ingress proxy. Traffic between the two proxies and the internal cluster Pods always uses mTLS, because those Pods are all in the Istio mesh.\nIstio mesh Istio provides extensive security protection for both authentication and authorization, as described in Istio Security. Access control and mTLS are two security features that Verrazzano configures. These security features are available in the context of a service mesh.\nA service mesh is an infrastructure layer that provides certain capabilities like security, observability, load balancing, and such, for services. Istio defines a service mesh here. In the context of Istio on Kubernetes, a service in the mesh is a Kubernetes Service. Consider the Bob’s Books example application, which has several OAM components defined. At runtime, there is a Kubernetes Service for each component, and each Service is in the mesh, with one or more Pods associated with the service. All services in the mesh have an Envoy proxy in front of their Pods, intercepting network traffic to and from the Pod. In Kubernetes, that proxy happens to be a sidecar running in each Pod.\nThere are various ways to put a service in the mesh. Verrazzano uses the namespace label, istio-injection: enabled, to designate that all Pods in a given namespace are in the mesh. When a Pod is created in that namespace, the Istio control plane mutating webhook, changes the Pod spec to add the Envoy proxy sidecar container, causing the Pod to be in the mesh.\nDisabling sidecar injection In certain cases, Verrazzano needs to disable sidecar injection for specific Pods in a namespace. This is done in two ways: first, during installation, Verrazzano modifies the istio-sidecar-injector ConfigMap using a Helm override file for the Istio chart. This excludes several components from the mesh, such as the Verrazzano application operator. Second, certain Pods, such as Coherence Pods, are labeled at runtime with sidecar.istio.io/inject=\"false\" to exclude them from the mesh.\nComponents in the mesh The following Verrazzano components are in the mesh and use mTLS for all service to service communication.\n Elasticsearch Fluentd Grafana Kibana Keycloak MySQL NGINX Ingress Controller Prometheus Verrazzano API Proxy Verrazzano Console WebLogic Operator  Some of these components, have mesh-related details that are worth noting.\nNGINX The NGINX Ingress Controller listens for HTTPS traffic, and provides ingress into the cluster. NGINX is configured to do TLS termination of client connections. All traffic from NGINX to the mesh services use mTLS, which means that traffic is fully encrypted from the client to the target back-end services.\nKeycloak and MySQL Keycloak and MySQL are also in the mesh and use mTLS for network traffic. Because all of the components that use Keycloak are in the mesh, there is end to end mTLS security for all identity management handled by Keycloak. The following components access Keycloak:\n Verrazzano API Proxy Verrazzano API Console Elasticsearch Prometheus Grafana Kibana  Prometheus Although Prometheus is in the mesh, it is configured to use the Envoy sidecar and mTLS only when communicating with Keycloak. All the traffic related to scraping metrics, bypasses the sidecar proxy, doesn’t use the service IP address, but rather connects to the scrape target using the Pod IP address. If the scrape target is in the mesh, then HTTPS is used; otherwise, HTTP is used. For Verrazzano multicluster, Prometheus also connects from the admin cluster to the Prometheus server in the managed cluster by using the managed cluster NGINX Ingress, using HTTPS. Prometheus in the managed cluster and never establishes connections to targets outside the cluster.\nBecause Prometheus is in the mesh, additional configuration is done to allow the Envoy sidecar to be bypassed when scraping Pods. This is done with the Prometheus Pod annotation traffic.sidecar.istio.io/includeOutboundIPRanges: \u003ckeycloak-service-ip\u003e. This causes traffic bound for Keycloak to go through the Envoy sidecar, and all other traffic to bypass the sidecar.\nWebLogic Kubernetes Operator When the WebLogic operator creates a domain, it needs to communicate with the Pods in the domain. Verrazzano puts the WebLogic operator in the mesh so that it can communicate with the domain Pods using mTLS. As a result, the WebLogic domain must be created in the mesh.\nApplications in the mesh Before you create a Verrazzano application, you should decide if it should be in the mesh. You control sidecar injection, for example, mesh inclusion, by labeling the application namespace with istio-injection=enabled or istio-injection=disabled. By default, applications will not be put in the mesh if that label is missing. If your application uses a Verrazzano project, then Verrazzano will label the namespaces in the project to enable injection. If the application is in the mesh, then mTLS will be used. You can change the PeerAuthentication mTLS mode as desired if you don’t want strict mTLS. Also, if you need to add mTLS port exceptions, you can do this with DestinationRules or by creating another PeerAuthentication resource in the application namespace. Consult the Istio documentation for more information.\nWebLogic When the WebLogic operator creates a domain, it needs to communicate with the Pods in the domain. Verrazzano puts the WebLogic operator in the mesh so that it can communicate with the domain Pods using mTLS. Because of that, the WebLogic domain must be created in the mesh. Also, because mTLS is used, do not configure WebLogic to use TLS. If you want to use a custom certificate for your application, you can specify that in the ApplicationConfiguration, but that TLS connection will be terminated at the Istio ingress gateway, which you configure using a Verrazzano IngressTrait.\nCoherence Coherence clusters are represented by the Coherence resource, and are not in the mesh. When Verrazzano creates a Coherence cluster in a namespace that is annotated to do sidecar injection, it disables injection of the Coherence resource using the sidecar.istio.io/inject=\"false\" label shown previously. Furthermore, Verrazzano will create a DestinationRule in the application namespace to disable mTLS for the Coherence extend port 9000. This allows a service in the mesh to call the Coherence extend proxy. For an example, see Bobs Books. Here is an example of a DestinationRule created for the Bob’s Books application which includes a Coherence cluster.\nAPI Version: networking.istio.io/v1beta1 Kind: DestinationRule ... Spec: Host: *.bobs-books.svc.cluster.local Traffic Policy: Port Level Settings: Port: Number: 9000 Tls: Tls: Mode: ISTIO_MUTUAL Istio access control Istio lets you control access to your workload in the mesh, using the AuthorizationPolicy resource. This lets you control which services or Pods can access your workloads. Some of these options require mTLS; for more information, see Authorization Policy.\nVerrazzano always creates AuthorizationPolicies for applications, but never for system components. During application deployment, Verrazzano creates the policy in the application namespace and configures it to allow access from the following:\n Other Pods in the application Istio ingress gateway Prometheus scraper  This prevents other Pods in the cluster from gaining network access to the application Pods.\nIstio uses a service identity to determine the identity of the request’s origin; for Kubernetes this identity is a service account. Verrazzano creates a per-application AuthorizationPolicy as follows:\nAuthorizationPolicy apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy ... spec: rules: - from: - source: principals: - cluster.local/ns/sales/sa/greeter - cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account - cluster.local/ns/verrazzano-system/sa/verrazzano-monitoring-operator WebLogic domain access For WebLogic applications, the WebLogic operator must have access to the domain Pods for two reasons. First, it must access the domain servers to get health status; second it must inject configuration into the Monitoring Exporter sidecar running in the domain server Pods. When a WebLogic domain is created, Verrazzano adds an additional source, cluster.local/ns/verrazzano-system/sa/weblogic-operator-sa to the principals section to permit that access.\n","excerpt":"Verrazzano manages and secures network traffic between Verrazzano system components and deployed applications. Verrazzano does not manage or secure traffic for the Kubernetes cluster itself, or for …","ref":"/docs/networking/security/net-security/","title":"Network Security"},{"body":"","excerpt":"","ref":"/docs/setup/","title":"Setup"},{"body":"To delete a Verrazzano installation:\n# Get the name of the Verrazzano custom resource $ kubectl get verrazzano # Delete the Verrazzano custom resource $ kubectl delete verrazzano \u003cname of custom resource\u003e To monitor the console log of the uninstall:\n$ kubectl logs \\ -f $(kubectl get pod \\ -l job-name=verrazzano-uninstall-my-verrazzano \\ -o jsonpath=\"{.items[0].metadata.name}\") ","excerpt":"To delete a Verrazzano installation:\n# Get the name of the Verrazzano custom resource $ kubectl get verrazzano # Delete the Verrazzano custom resource $ kubectl delete verrazzano \u003cname of custom …","ref":"/docs/setup/uninstall/uninstall/","title":"Uninstall"},{"body":"A Verrazzano installation consists of a stack of components, such as cert-manager, where each component has a specific release version that may be different from the overall Verrazzano version. The Verrazzano platform operator knows the versions of each component associated with the Verrazzano version. When you perform the initial Verrazzano installation, the appropriate version of each component is installed by the platform operator. Post installation, it may be necessary to update one or more of the component images or Helm charts. This update is also handled by the platform operator and is called an upgrade. Currently, Verrazzano does only patch-level upgrade, where a helm upgrade command can be issued for the component. Typically, patch-level upgrades simply replace component images with newer versions.\nIt is important to distinguish between updating the Verrazzano platform operator versus upgrading the Verrazzano installation. The platform operator contains the newer component charts and image versions, so it must be updated prior to upgrading the installation. Updating the platform operator has no effect on an existing installation until you initiate the Verrazzano installation upgrade. Currently, there is no way to roll back either the platform operator update or the Verrazzano installation upgrade. Upgrading will not have any impact on running applications.\nUpgrading an existing Verrazzano installation involves:\n Upgrading the Verrazzano platform operator to the Verrazzano release version to which you want to upgrade. Updating the version of your installed Verrazzano resource to the version supported by the upgraded operator.  NOTE: You may only change the version field during an upgrade; changes to other fields or component configurations are not supported at this time.\nUpgrade the Verrazzano platform operator In order to upgrade an existing Verrazzano installation, you must first upgrade the Verrazzano platform operator.\n  Upgrade the Verrazzano platform operator.\nNOTE: If you are using a private container registry, then to update the platform operator, follow the instructions at Use a Private Registry.\nTo upgrade to the latest version:\n$ kubectl apply -f https://github.com/verrazzano/verrazzano/releases/download/v0.17.0/operator.yaml To upgrade to a specific version, where \u003cversion\u003e is the desired version:\n$ kubectl apply -f https://github.com/verrazzano/verrazzano/releases/download/\u003cversion\u003e/operator.yaml For example:\n$ kubectl apply -f https://github.com/verrazzano/verrazzano/releases/download/v0.7.0/operator.yaml   Wait for the deployment to complete.\n$ kubectl -n verrazzano-install rollout status deployment/verrazzano-platform-operator deployment \"verrazzano-platform-operator\" successfully rolled out   Confirm that the operator pod is correctly defined and running.\n$ kubectl -n verrazzano-install get pods NAME READY STATUS RESTARTS AGE verrazzano-platform-operator-59d5c585fd-lwhsx 1/1 Running 0 114s   Upgrade Verrazzano To upgrade Verrazzano:\n  Update the Verrazzano resource to the desired version.\nTo upgrade the Verrazzano components, you must update the version field in your Verrazzano resource spec to match the version supported by the platform operator to which you upgraded and apply it to the cluster.\nThe value of the version field in the resource spec must be a Semantic Versioning value corresponding to a valid Verrazzano release version.\nYou can update the resource by doing one of the following:\na. Editing the YAML file you used to install Verrazzano and setting the version field to the latest version.\nFor example, to upgrade to v0.17.0, your YAML file should be edited to add or update the version field:\napiVersion:install.verrazzano.io/v1alpha1kind:Verrazzanometadata:name:my-verrazzanospec:profile:devversion:v0.17.0Then apply the resource to the cluster (if you have not edited the resource in-place using kubectl edit):\n$ kubectl apply -f my-verrazzano.yaml b. Editing the Verrazzano resource directly using kubectl and setting the version field directly, for example:\n$ kubectl edit verrazzano my-verrazzano # In the resource editor, add or update the version field to \"version: v0.17.0\", then save.   Wait for the upgrade to complete:\n$ kubectl wait \\  --timeout=10m \\  --for=condition=UpgradeComplete verrazzano/my-verrazzano   Verify the upgrade Check that all the pods in the verrazzano-system namespace are in the Running state. While the upgrade is in progress, you may see some pods terminating and restarting as newer versions of components are applied.\nFor example:\n$ kubectl get pods -n verrazzano-system coherence-operator-controller-manager-7557bc4c49-7w55p 1/1 Running 0 27h fluentd-fzmsl 1/1 Running 0 27h fluentd-r9wwf 1/1 Running 0 27h fluentd-zp2r2 1/1 Running 0 27h oam-kubernetes-runtime-6ff589f66f-r95qv 1/1 Running 0 27h verrazzano-api-669c7d7f66-rcnl8 1/1 Running 0 27h verrazzano-application-operator-b5b77d676-7w95p 1/1 Running 0 27h verrazzano-console-6b469dff9c-b2jwk 1/1 Running 0 27h verrazzano-monitoring-operator-54cb658774-f6jjm 1/1 Running 0 27h verrazzano-operator-7f4b99d7d-wg7qm 1/1 Running 0 27h vmi-system-es-master-0 2/2 Running 0 27h vmi-system-grafana-74bb7cdf65-k97pb 2/2 Running 0 27h vmi-system-kibana-85565975b5-7hfdf 2/2 Running 0 27h vmi-system-prometheus-0-7bf464d898-czq8r 4/4 Running 0 27h weblogic-operator-7db5cdcf59-qxsr9 1/1 Running 0 27h ","excerpt":"A Verrazzano installation consists of a stack of components, such as cert-manager, where each component has a specific release version that may be different from the overall Verrazzano version. The …","ref":"/docs/setup/upgrade/upgrade/","title":"Upgrade"},{"body":"A Verrazzano application can contain any number of Coherence component workloads, where each workload is a standalone Coherence cluster, independent from other Coherence clusters in the application.\nVerrazzano uses the standard Coherence operator to provision and manage clusters, as documented at Coherence Operator. The Coherence operator uses a CRD, coherence.oracle.com (Coherence resource), to represent a Coherence cluster. When a Verrazzano application with Coherence is provisioned, Verrazzano configures the default logging and metrics for the Coherence cluster. Logs are sent to Elasticsearch and metrics to Prometheus.\nYou can view this telemetry data using the Kibana and Grafana consoles.\nOAM component The custom resource YAML file for the Coherence cluster is specified as a VerrazzanoCoherenceWorkload custom resource. In the following example, everything under the spec: section is standard Coherence resource YAML that you would typically use to provision a Coherence cluster. Including this component reference in your ApplicationConfiguration will result in a new Coherence cluster being provisioned. You can have multiple clusters in the same application with no conflict.\napiVersion: core.oam.dev/v1alpha2 kind: Component metadata: name: orders namespace: sockshop spec: workload: apiVersion: oam.verrazzano.io/v1alpha1 kind: VerrazzanoCoherenceWorkload spec: template: metadata: name: orders-coh spec: cluster: SockShop ... Life cycle With Verrazzano, you manage the life cycle of applications using Component and ApplicationConfiguration resources. Typically, you would modify the Coherence cluster resource to make changes or to do lifecycle operations, like scale in and scale out. However, in the Verrazzano environment, the cluster resource is owned by the Verrazzano application operator and will be reconciled to match the component workload resource. Therefore, you need to manage the cluster configuration by modifying the resource, either by kubectl edit or applying a new YAML file. Verrazzano will notice that the Component resource changed and will update the Coherence resource as needed.\nProvisioning When you apply the component YAML file shown previously, Kubernetes will create a component.oam.verrazzano.io resource, but the Coherence cluster will not be created until you create the ApplicationConfiguration resource, which references the Coherence component. When the application is created, Verrazzano creates a Coherence custom resource for each cluster, which is subsequently processed by the Coherence operator, resulting in a new cluster. After a cluster is created, the Coherence operator will monitor the Coherence resource to reconcile the state of the cluster. You can add a new Coherence workload to a running application, or remove an existing workload, by modifying the ApplicationConfiguration resource, and adding or removing the Coherence component.\nScaling Scaling a Coherence cluster is done by modifying the replicas field in the Component resource. Verrazzano will modify the Coherence resource replicas field and the cluster will be scaled accordingly. The following example configuration shows the replicas field that specifies the number of pods in the cluster.\napiVersion: core.oam.dev/v1alpha2 kind: Component metadata: name: orders namespace: sockshop spec: workload: apiVersion: oam.verrazzano.io/v1alpha1 kind: VerrazzanoCoherenceWorkload spec: template: metadata: name: orders-coh spec: cluster: SockShop replicas: 3 ... NOTE: A Coherence cluster provisioned with Verrazzano does not support autoscaling with a Horizontal Pod Autoscaler.\nTermination You can terminate the Coherence cluster by removing the component from the ApplicationConfiguration or by deleting the ApplicationConfiguration resource entirely.\nNOTE Do not delete the Coherence component if the application is still using it.  Logging When a Coherence cluster is provisioned, Verrazzano configures it to send logs to Elasticsearch. This is done by injecting Fluentd sidecar configuration into the Coherence resource. The Coherence operator will create the pod with the Fluentd sidecar. This sidecar periodically copies the Coherence logs from /logs to stdout, enabling the Fluentd DaemonSet in the verrazzano-system namespace to send the logs to Elasticsearch. Note that the Fluend sidecar running in the Coherence pod never communicates with Elasticsearch or any other network endpoint.\nThe logs are placed in a per-namespace Elasticsearch index named verrazzano-namespace-\u003cnamespace\u003e, for example: verrazzano-namespace-sockshop. All logs from Coherence pods in the same namespace will go into the same index, even for different applications. This is standard behavior and there is no way to disable or change it.\nEach log record has some Coherence and application fields, along with the log message itself. For example:\n kubernetes.labels.coherenceCluster SockShop kubernetes.labels.app_oam_dev/name sockshop-appconf kubernetes.labels.app_oam_dev/component orders ... Metrics Verrazzano uses Prometheus to scrape metrics from Coherence cluster pods. Like logging, metrics scraping is also enabled during provisioning, however, the Coherence resource YAML file must have proper metrics configuration. For details, see Coherence Metrics. In summary, there are two ways to configure the Coherence metrics endpoint. Coherence has a default metrics endpoint that you can enable. If your application serves metrics from its own endpoint, such as a Helidon application, then do not use the native Coherence metrics endpoint. To see the difference, examine the socks-shop and bobs-books examples.\nBobs Books The bobs-books example uses the default Coherence metrics endpoint, so the configuration must enable this feature, shown in the following metrics section of the roberts-coherence component in the YAML file, bobs-books-comp.yaml.\ncoherence: metrics: enabled: true Sock Shop The sock-shop example, which is a Helidon application with embedded Coherence, explicitly specifies the metrics port 7001 and doesn’t enable Coherence metrics. Coherence metrics still will be scraped, but not at the default endpoint.\n ports: ... - name: metrics port: 7001 serviceMonitor: enabled: true Because sock-shop components are not using the default Coherence metrics port, you must add a MetricsTrait section to the ApplicationConfiguration for each component, specifying the metrics port as follows:\n - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: MetricsTrait metadata: name: carts-metrics spec: port: 7001 Prometheus configuration Prometheus is configured to scrape targets using the ConfigMaps in the verrazzano-system namespace. During application deployments, Verrazzano updates the vmi-system-prometheus-config ConfigMap and adds targets for the application pods. Verrazzano also annotates those pods to match the expected annotations in the ConfigMap. When the application is deleted, Verrazzano removes the targets from the ConfigMap. You do not need to manually modify the ConfigMap or annotate the application pods.\nHere is an example of thesock-shop Prometheus ConfigMap section for catalog. Notice that pods in the sock-shop namespace with labels app_oam_dev_name and app_oam_dev_component are targeted. Prometheus will find those pods and then look at the pod annotations, verrazzano_io/metricsEnabled, verrazzano_io/metricsPath, and verrazzano_io/metricsPort for scrape configuration.\n- job_name: sockshop-appconf_default_sockshop_catalog ... kubernetes_sd_configs: - role: pod namespaces: names: - sockshop relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_verrazzano_io_metricsEnabled, __meta_kubernetes_pod_label_app_oam_dev_name, __meta_kubernetes_pod_label_app_oam_dev_component] ... - source_labels: [__meta_kubernetes_pod_annotation_verrazzano_io_metricsPath] ... - source_labels: [__address__, __meta_kubernetes_pod_annotation_verrazzano_io_metricsPort] Here is the corresponding catalog pod labels and annotations.\nkind: Pod metadata: labels: ... app.oam.dev/component: catalog app.oam.dev/name: sockshop-appconf annotations: ... verrazzano.io/metricsEnabled: \"true\" verrazzano.io/metricsPath: /metrics verrazzano.io/metricsPort: \"7001\" Istio integration Verrazzano ensures that Coherence clusters are not included in an Istio mesh, even if the namespace has the istio-injection: enabled label. This is done by adding the sidecar.istio.io/inject: \"false\" annotation to the Coherence resource, resulting in Coherence pods being created with that label. However, other application components in the mesh using mutual TLS authentication (mTLS) may need to communicate with Coherence. To handle this case, Verrazzano automatically creates an Istio DestinationRule to disable TLS for the Coherence port. This policy disables mTLS for port 9000, which happens to be used as a Coherence extend port for Bob’s Books.\n trafficPolicy: portLevelSettings: - port: number: 9000 tls: {} ... Currently, port 9000 is the only port where TLS is disabled, so you need to use this as the Coherence extend port if other components in the mesh access Coherence over the extend protocol.\nSummary Verrazzano makes it easy to deploy and observe Coherence clusters in your application, providing seamless integration with other components in your application running in an Istio mesh.\n","excerpt":"A Verrazzano application can contain any number of Coherence component workloads, where each workload is a standalone Coherence cluster, independent from other Coherence clusters in the application. …","ref":"/docs/applications/workloads/coherence/coherence/","title":"Coherence Workload"},{"body":"Helidon is a collection of Java libraries for writing microservices. Helidon provides an open source, lightweight, fast, reactive, cloud native framework for developing Java microservices. It is available as two frameworks:\n Helidon SE is a compact toolkit that embraces the latest Java SE features: reactive streams, asynchronous and functional programming, and fluent-style APIs. Helidon MP implements and supports Eclipse MicroProfile, a baseline platform definition that leverages Java EE and Jakarta EE technologies for microservices and delivers application portability across multiple runtimes.  Helidon is designed and built with container-first philosophy.\n Small footprint, low memory usage and faster startup times. All 3rd party dependencies are stored separately to enable Docker layering. Provides readiness, liveness and customizable health information for container schedulers like Kubernetes.  Containerized Helidon applications are generally deployed as Deployment in Kubernetes.\nVerrazzano integration Verrazzano supports application definition using Open Application Model (OAM). Verrrazzano applications are composed of components and application configurations.\nHelidon applications are first class citizen in Verrazzano with specialized Helidon Workload support, for example, VerrazzanoHelidonWorkload. VerrazzanoHelidonWorkload is supported as part of verrazzano-application-operator in the Verrazzano installation and no additional operator setup or installation is required. VerrazzanoHelidonWorkload also supports all the traits and scopes defined by Verrazzano along with core ones defined by the OAM specification.\nVerrazzanoHelidonWorkload is modeled after ContainerizedWorkload, for example, it is used for long-running workloads in containers. However, VerrazzanoHelidonWorkload closely resembles and directly refers to Kubernetes Deployment schema. This enables an easy lift and shift of existing containerized Helidon applications.\nThe complete VerrazzanoHelidonWorkload API definition and description is available at VerrazzanoHelidonWorkload.\nVerrazzano Helidon application development With Verrazzano, you manage the life cycle of applications using Component and ApplicationConfiguration resources. A Verrazzano application can contain any number of VerrazzanoHelidonWorkload components, where each workload is a standalone containerized Helidon application, independent of any other in the application.\nIn the following example, everything under the spec: section is the custom resource YAML file for the containerized Helidon application, as defined by VerrazzanoHelidonWorkload custom resource. Including this component reference in your ApplicationConfiguration will result in a new containerized Helidon application being provisioned.\napiVersion:core.oam.dev/v1alpha2kind:Componentmetadata:name:hello-helidon-componentnamespace:hello-helidonspec:workload:apiVersion:oam.verrazzano.io/v1alpha1kind:VerrazzanoHelidonWorkloadmetadata:name:hello-helidon-workloadlabels:app:hello-helidonspec:deploymentTemplate:metadata:name:hello-helidon-deploymentpodSpec:containers:- name:hello-helidon-container......The Application Development Guide provides end-to-end instructions for developing and deploying the Verrazzano Helidon application.\nFor more Verrazzano Helidon application examples, see Examples.\nProvisioning When you apply the previous component YAML file, Kubernetes will create a component.oam.verrazzano.io resource, but the containerized Helidon application will not be created until you create the ApplicationConfiguration resource, which references the VerrazzanoHelidonWorkload component. When the application is created, Verrazzano creates a Deployment and Service resource for each containerized Helidon application.\nTypically, you would modify the Deployment and Service resource to make changes or to do lifecycle operations, like scale in and scale out. However, in the Verrazzano environment, the containerized Helidon application resource is owned by the verrazzano-application-operator and will be reconciled to match the component workload resource. Therefore, you need to manage the application configuration by modifying the VerrazzanoHelidonWorkload or ApplicationConfiguration resource, either by kubectl edit or applying new YAML file. Verrazzano will notice that the component resource change and will update the Deployment and Service resource as needed.\nYou can add a new VerrazzanoHelidonWorkload to a running application, or remove an existing workload, by modifying the ApplicationConfiguration resource and adding or removing the VerrazzanoHelidonWorkload component.\nScaling The recommended way to scale containerized Helidon application replicas is to specify ManualScalerTrait with VerrazzanoHelidonWorkload in ApplicationConfiguration. The following example configuration shows the replicaCount field that specifies the number of replicas for the application.\n...spec:components:- componentName:hello-helidon-componenttraits:- trait:apiVersion:core.oam.dev/v1alpha2kind:ManualScalerTraitspec:replicaCount:2...Verrazzano will modify the Deployment resource replicas field and the containerized Helidon application replicas will be scaled accordingly.\nNOTE Make sure the replicas defined on the VerrazzanoHelidonWorkload component and that replicaCount defined on ManualScalerTrait for that component matches, or else the DeploymentController in Kubernetes and OAM runtime in verrazzano-application-operator will compete to create a different number of Pods for same containerized Helidon application. To avoid confusion, we recommend that you specify replicaCount defined on ManualScalerTrait and leave replicas undefined on VerrazzanoHelidonWorkload (as it is optional).  Logging When a containerized Helidon application is provisioned on Verrazzano, Verrazzano will configure the default logging and send logs to Elasticsearch. Logs can be viewed using the Kibana console.\nThe logs are placed in a per-namespace Elasticsearch index named verrazzano-namespace-\u003cnamespace\u003e, for example: verrazzano-namespace-hello-helidon. All logs from containerized Helidon application pods in the same namespace will go into the same index, even for different applications. This is standard behavior and there is no way to disable or change it.\nMetrics Verrazzano uses Prometheus to scrape metrics from containerized Helidon application pods. Like logging, metrics scraping is also enabled during provisioning. Metrics can be viewed using the Grafana console.\nVerrazzano lets you to customize configuration information needed to enable metrics using MetricsTrait for an application component.\nIngress Verrazzano lets you to configure traffic routing to a containerized Helidon application, using IngressTrait for an application component.\nTroubleshooting Whenever you have a problem with your Verrazzano Helidon application, there are some basic techniques you can use to troubleshoot. Troubleshooting shows you some simple things to try when troubleshooting, as well as how to solve common problems you may encounter.\n","excerpt":"Helidon is a collection of Java libraries for writing microservices. Helidon provides an open source, lightweight, fast, reactive, cloud native framework for developing Java microservices. It is …","ref":"/docs/applications/workloads/helidon/helidon/","title":"Helidon Workload"},{"body":"Before you begin Install Verrazzano by following the installation instructions.\nNOTE: The Sock Shop example application deployment files are contained in the Verrazzano project located at \u003cVERRAZZANO_HOME\u003e/examples/sockshop, where \u003cVERRAZZANO_HOME\u003e is the root of the Verrazzano project.\nDeploy the Sock Shop application This example application provides a Helidon implementation of the Sock Shop Microservices Demo Application. It uses OAM resources to define the application deployment.\n  Create a namespace for the Sock Shop application and add a label identifying the namespace as managed by Verrazzano.\n$ kubectl create namespace sockshop $ kubectl label namespace sockshop verrazzano-managed=true   To deploy the application, apply the Sock Shop OAM resources.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/sock-shop/sock-shop-comp.yaml $ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/sock-shop/sock-shop-app.yaml   Wait for the Sock Shop application to be ready.\n$ kubectl wait \\ --for=condition=Ready pods \\ --all -n sockshop \\ --timeout=300s   Explore the application The Sock Shop microservices application implements REST API endpoints including:\n /catalogue - Returns the Sock Shop catalog. This endpoint accepts the GET HTTP request method. /register - POST { \"username\":\"xxx\", \"password\":\"***\", \"email\":\"foo@example.com\", \"firstName\":\"foo\", \"lastName\":\"bar\" } to create a user. This endpoint accepts the POST HTTP request method.  NOTE: The following instructions assume that you are using a Kubernetes environment, such as OKE. Other environments or deployments may require alternative mechanisms for retrieving addresses, ports, and such.\nFollow these steps to test the endpoints:\n  Get the generated host name for the application.\n$ HOST=$(kubectl get gateway \\ -n sockshop \\ -o jsonpath={.items[0].spec.servers[0].hosts[0]}) $ echo $HOST sockshop-appconf.sockshop.11.22.33.44.nip.io   Get the EXTERNAL_IP address of the istio-ingressgateway service.\n$ ADDRESS=$(kubectl get service \\ -n istio-system istio-ingressgateway \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}') $ echo $ADDRESS 11.22.33.44   Access the Sock Shop application:\n  Using the command line\n# Get catalogue $ curl -sk \\ -X GET \\ https://${HOST}/catalogue \\ --resolve ${HOST}:443:${ADDRESS} [{\"count\":115,\"description\":\"For all those leg lovers out there....\", ...}] # Add a new user (replace values of username and password) $ curl -i \\ --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"username\":\"foo\",\"password\":\"****\",\"email\":\"foo@example.com\",\"firstName\":\"foo\",\"lastName\":\"foo\"}' \\ -k https://${HOST}/register \\ --resolve ${HOST}:443:${ADDRESS} # Add an item to the user's cart $ curl -i \\ --header \"Content-Type: application/json\" \\ --request POST \\ --data '{\"itemId\": \"a0a4f044-b040-410d-8ead-4de0446aec7e\",\"unitPrice\": \"7.99\"}' \\ -k https://${HOST}/carts/{username}/items \\ --resolve ${HOST}:443:${ADDRESS} # Get cart items $ curl -i \\ -k https://${HOST}/carts/{username}/items \\ --resolve ${HOST}:443:${ADDRESS} If you are using nip.io, then you do not need to include --resolve.\n  Local testing with a browser\nTemporarily, modify the /etc/hosts file (on Mac or Linux) or c:\\Windows\\System32\\Drivers\\etc\\hosts file (on Windows 10), to add an entry mapping the host name to the ingress gateway’s EXTERNAL-IP address. For example:\n11.22.33.44 sockshop.example.com Then, you can access the application in a browser at https://sockshop.example.com/catalogue.\nIf you are using nip.io, then you can access the application in a browser using the HOST variable (for example, https://${HOST}/catalogue). If you are going through a proxy, you may need to add *.nip.io to the NO_PROXY list.\n  Using your own DNS name\n Point your own DNS name to the ingress gateway’s EXTERNAL-IP address. In this case, you would need to edit the sock-shop-app.yaml file to use the appropriate value under the hosts section (such as yourhost.your.domain), before deploying the Sock Shop application. Then, you can use a browser to access the application at https://\u003cyourhost.your.domain\u003e/catalogue.      A variety of endpoints associated with the deployed application, are available to further explore the logs, metrics, and such.\nAccessing them may require the following:\n  Run this command to get the password that was generated for the telemetry components:\n$ kubectl get secret \\ --namespace verrazzano-system verrazzano \\ -o jsonpath={.data.password} | base64 \\ --decode; echo The associated user name is verrazzano.\n  You will have to accept the certificates associated with the endpoints.\nYou can retrieve the list of available ingresses with following command:\n $ kubectl get ing -n verrazzano-system NAME CLASS HOSTS ADDRESS PORTS AGE verrazzano-ingress \u003cnone\u003e verrazzano.default.140.238.94.217.nip.io 140.238.94.217 80, 443 7d2h vmi-system-es-ingest \u003cnone\u003e elasticsearch.vmi.system.default.140.238.94.217.nip.io 140.238.94.217 80, 443 7d2h vmi-system-grafana \u003cnone\u003e grafana.vmi.system.default.140.238.94.217.nip.io 140.238.94.217 80, 443 7d2h vmi-system-kibana \u003cnone\u003e kibana.vmi.system.default.140.238.94.217.nip.io 140.238.94.217 80, 443 7d2h vmi-system-prometheus \u003cnone\u003e prometheus.vmi.system.default.140.238.94.217.nip.io 140.238.94.217 80, 443 7d2h Using the ingress host information, some of the endpoints available are:\n   Description Address Credentials     Kibana https://[vmi-system-kibana ingress host] verrazzano/telemetry-password   Grafana https://[vmi-system-grafana ingress host] verrazzano/telemetry-password   Prometheus https://[vmi-system-prometheus ingress host] verrazzano/telemetry-password        Troubleshooting   Verify that the application configuration, domain, and ingress trait all exist.\n$ kubectl get ApplicationConfiguration -n sockshop $ kubectl get Domain -n sockshop $ kubectl get IngressTrait -n sockshop   Verify that the Sock Shop service pods are successfully created and transition to the READY state. Note that this may take a few minutes and that you may see some of the services terminate and restart.\n $ kubectl get pods -n sockshop NAME READY STATUS RESTARTS AGE carts-coh-0 1/1 Running 0 41s catalog-coh-0 1/1 Running 0 40s orders-coh-0 1/1 Running 0 39s payment-coh-0 1/1 Running 0 37s shipping-coh-0 1/1 Running 0 36s users-coh-0 1/1 Running 0 35s   ","excerpt":"Before you begin Install Verrazzano by following the installation instructions.\nNOTE: The Sock Shop example application deployment files are contained in the Verrazzano project located at …","ref":"/docs/samples/sock-shop/","title":"Helidon Sock Shop"},{"body":"Get the consoles URLs Verrazzano installs several consoles. The endpoints for an installation are stored in the Status field of the installed Verrazzano Custom Resource. You can get the endpoints for these consoles by issuing the following command and examining the Status.Instance field:\n$ kubectl get vz -o yaml\nThe resulting output is similar to the following (abbreviated to show only the relevant portions):\n ... status: conditions: - lastTransitionTime: \"2021-06-30T03:10:00Z\" message: Verrazzano install in progress status: \"True\" type: InstallStarted - lastTransitionTime: \"2021-06-30T03:18:33Z\" message: Verrazzano install completed successfully status: \"True\" type: InstallComplete instance: consoleUrl: https://verrazzano.default.11.22.33.44.nip.io elasticUrl: https://elasticsearch.vmi.system.default.11.22.33.44.nip.io grafanaUrl: https://grafana.vmi.system.default.11.22.33.44.nip.io keyCloakUrl: https://keycloak.default.11.22.33.44.nip.io kibanaUrl: https://kibana.vmi.system.default.11.22.33.44.nip.io prometheusUrl: https://prometheus.vmi.system.default.11.22.33.44.nip.io rancherUrl: https://rancher.default.11.22.33.44.nip.io If you have jq installed, then you can use the following command to get the instance URLs more directly:\n$ kubectl get vz -o jsonpath=\"{.items[].status.instance}\" | jq .\nThe following is an example of the output:\n{ \"consoleUrl\": \"https://verrazzano.default.11.22.33.44.nip.io\", \"elasticUrl\": \"https://elasticsearch.vmi.system.default.11.22.33.44.nip.io\", \"grafanaUrl\": \"https://grafana.vmi.system.default.11.22.33.44.nip.io\", \"keyCloakUrl\": \"https://keycloak.default.11.22.33.44.nip.io\", \"kibanaUrl\": \"https://kibana.vmi.system.default.11.22.33.44.nip.io\", \"prometheusUrl\": \"https://prometheus.vmi.system.default.11.22.33.44.nip.io\", \"rancherUrl\": \"https://rancher.default.11.22.33.44.nip.io\" } Get consoles credentials You will need the credentials to access the consoles installed by Verrazzano.\nConsoles accessed by the same user name/password  Grafana Prometheus Kibana Elasticsearch  User: verrazzano\nTo get the password:\n$ kubectl get secret \\ --namespace verrazzano-system verrazzano \\ -o jsonpath={.data.password} | base64 \\ --decode; echo The Keycloak admin console User: keycloakadmin\nTo get the password:\n$ kubectl get secret \\ --namespace keycloak keycloak-http \\ -o jsonpath={.data.password} | base64 \\ --decode; echo The Rancher console User: admin\nTo get the password:\n$ kubectl get secret \\ --namespace cattle-system rancher-admin-secret \\ -o jsonpath={.data.password} | base64 \\ --decode; echo Change the Verrazzano password To change the Verrazzano password, first change the user password in Keycloak and then update the Verrazzano secret.\nChange the user in Keycloak\n  Navigate to the Keycloak admin console.\na. Obtaining the Keycloak admin console URL is described here.\nb. Obtaining the Keycloak admin console credentials is described here.\n  In the left pane, under Manage, select Users.\n  In the Users pane, search for verrazzano or click View all users.\n  For the verrazzano user, click the Edit action.\n  At the top, select the Credentials tab.\n  Specify the new password and confirm.\n  Specify whether the new password is a temporary password. A temporary password must be reset on next login.\n  Click Reset Password.\n  Confirm the password reset by clicking Reset password in the confirmation dialog.\n  Update the Verrazzano secret\nGet the base64 encoding for your new password:\n$ echo -n 'MyNewPwd' | base64\nUpdate the password in the secret:\n$ kubectl edit secret verrazzano -n verrazzano-system\nReplace the existing password value with the new base64 encoded value.\n","excerpt":"Get the consoles URLs Verrazzano installs several consoles. The endpoints for an installation are stored in the Status field of the installed Verrazzano Custom Resource. You can get the endpoints for …","ref":"/docs/operations/","title":"Access Verrazzano"},{"body":"Summary Analysis detected that there were pods that had issues due to failures to pull an image or images.\nThe analysis was not able to identify a specific root cause, however, it might have supplied data that is related to the failures.\nSteps  Review the analysis data. At a minimum, it will indicate which pods are being impacted and might give other clues on the root cause. If the service is experiencing an outage, then consult the specific service status page. For common service status pages, see Related information.  Related information  GitHub Status OCI Status Kubernetes Troubleshooting  ","excerpt":"Summary Analysis detected that there were pods that had issues due to failures to pull an image or images.\nThe analysis was not able to identify a specific root cause, however, it might have supplied …","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/imagepullbackoff/","title":"Image Pull Back Off"},{"body":"Summary Analysis detected that there were pods which had issues due to failures to pull an image or images where the root cause was that the image was not found.\nSteps  Review the analysis data; it enumerates the pods and related messages regarding which images had this issue. Confirm that the image name, digest, and tag are correctly specified.  Related information  Kubernetes Troubleshooting  ","excerpt":"Summary Analysis detected that there were pods which had issues due to failures to pull an image or images where the root cause was that the image was not found.\nSteps  Review the analysis data; it …","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/imagepullnotfound/","title":"Image Pull Not Found"},{"body":"Summary Analysis detected that there were pods which had issues due to failures to pull an image or images.\nThe root cause was rate limit exceeded errors while pulling images.\nSteps  Review the analysis data; it enumerates the pods and related messages regarding which images had this issue. The detailed messages might provide specific instructions for the registry that is involved. For example, it might provide a link to instructions on how to increase the limit.  Related information  Increase Rate Limits  ","excerpt":"Summary Analysis detected that there were pods which had issues due to failures to pull an image or images.\nThe root cause was rate limit exceeded errors while pulling images.\nSteps  Review the …","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/imagepullratelimit/","title":"Image Pull Rate Limit"},{"body":"Summary Analysis detected that there were pods which had issues due to failures to pull an image or images where the root cause was that the service was not available.\nThe service might be unreachable or might be incorrect.\nSteps  Review the analysis data; it enumerates the pods and related messages about which images had this issue. Confirm that the registry for the image is correct. The messages might identify a connectivity issue. If the service is experiencing an outage, then consult the specific service status page. For common service status pages, see Related information.  Related information  GitHub Status OCI Status Kubernetes Troubleshooting  ","excerpt":"Summary Analysis detected that there were pods which had issues due to failures to pull an image or images where the root cause was that the service was not available.\nThe service might be unreachable …","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/imagepullservice/","title":"Image Pull Service Issue"},{"body":"Summary Analysis detected that the Verrazzano installation failed while installing the NGINX Ingress Controller.\nThe root cause appears to be that the load balancer is either missing or unable to set the ingress IP address on the NGINX Ingress service.\nSteps Refer to the platform-specific environment setup for your platform here.\nRelated information  Platform Setup Kubernetes Troubleshooting  ","excerpt":"Summary Analysis detected that the Verrazzano installation failed while installing the NGINX Ingress Controller.\nThe root cause appears to be that the load balancer is either missing or unable to set …","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/ingressnoloadbalancerip/","title":"Ingress Controller No Load Balancer IP"},{"body":"Summary Analysis detected that the Verrazzano installation failed while installing the NGINX Ingress Controller.\nThe root cause appears to be that an OCI IP non-ephemeral address limit has been reached.\nSteps  Review the messages from the supporting details for the exact limit. Refer to the OCI documentation related to managing IP Addresses.  Related information  Public IP Addresses  ","excerpt":"Summary Analysis detected that the Verrazzano installation failed while installing the NGINX Ingress Controller.\nThe root cause appears to be that an OCI IP non-ephemeral address limit has been …","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/ingressociiplimitexceeded/","title":"Ingress Controller OCI IP Limit Exceeded"},{"body":"Summary Analysis detected that the Verrazzano installation has failed, however, it did not isolate the exact reason for the failure.\nSteps Review the analysis data, which can help identify the issue.\nRelated information  Installation Guide Kubernetes Troubleshooting  ","excerpt":"Summary Analysis detected that the Verrazzano installation has failed, however, it did not isolate the exact reason for the failure.\nSteps Review the analysis data, which can help identify the issue. …","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/installfailure/","title":"Install Failure"},{"body":"Summary Analysis detected that the Verrazzano installation has failed related to the NGINX Ingress Controller, however, it was unable to isolate the specific root cause.\nSteps Review the analysis data, which might help identify the issue.\nRelated information  Installation Guide Platform Setup Kubernetes Troubleshooting  ","excerpt":"Summary Analysis detected that the Verrazzano installation has failed related to the NGINX Ingress Controller, however, it was unable to isolate the specific root cause.\nSteps Review the analysis …","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/ingressinstallfailure/","title":"Install Ingress Controller Failure"},{"body":"Prerequisites Before you begin, read this document, Verrazzano in a multicluster environment.\nOverview To set up a multicluster Verrazzano environment, you will need two or more Kubernetes clusters. One of these clusters will the admin cluster; the others will be managed clusters.\nThe instructions here assume an admin cluster and a single managed cluster. For each additional managed cluster, simply repeat the managed cluster instructions.\nInstall Verrazzano Install Verrazzano on each Kubernetes cluster.\n On one cluster, install Verrazzano using the dev or prod profile; this will be the admin cluster. On the other cluster, install Verrazzano using the managed-cluster profile; this will be a managed cluster. The managed-cluster profile contains only the components that are required for a managed cluster.  For detailed instructions on how to install and customize Verrazzano on a Kubernetes cluster using a specific profile, see the Installation Guide and Installation Profiles.\nRegister the managed cluster with the admin cluster The following sections show you how to register the managed cluster with the admin cluster. As indicated, some of these steps are performed on the admin cluster and some on the managed cluster.\nPreregistration setup Before registering the managed cluster, first you’ll need to set up the following items:\n A Secret containing the managed cluster’s CA certificate. Note that the cacrt field in this secret can be empty if the managed cluster uses a well-known CA. It is required only if the managed cluster uses self-signed certificates. This CA certificate is used by the admin cluster to scrape metrics from the managed cluster, for both applications and Verrazzano components. A ConfigMap containing the externally reachable address of the admin cluster. This will be provided to the managed cluster during registration so that it can connect to the admin cluster.  Follow these preregistration setup steps:\n  If needed for the admin cluster, obtain the managed cluster’s CA certificate. The admin cluster scrapes metrics from the managed cluster’s Prometheus endpoint. If the managed cluster Verrazzano installation uses self-signed certificates, then the admin cluster will need the managed cluster’s CA certificate in order to make an https connection.\n Depending on whether the Verrazzano installation on the managed cluster uses self-signed certificates or certificates signed by a well-known certificate authority, choose the appropriate instructions. If you are unsure what type of certificates are used, check for the system-tls secret in the verrazzano-system namespace on the managed cluster. # On the managed cluster $ kubectl -n verrazzano-system get secret system-tls If this secret is present, then your managed cluster is using self-signed certificates. If it is not present, then your managed cluster is using certificates signed by a well-known certificate authority.  Well-known CA Self-Signed   In this case, create a file called managed1.yaml with an empty value for the cacrt field as follows:\n$ kubectl create secret generic \"ca-secret-managed1\" -n verrazzano-mc \\  --from-literal=cacrt=\"\" --dry-run=client -o yaml \u003e managed1.yaml;   If the managed cluster certificates are self-signed, create a file called managed1.yaml containing the CA certificate of the managed cluster as the value of the cacrt field. In the following commands, the managed cluster’s CA certificate is saved in an environment variable called MGD_CA_CERT. Then use the --dry-run option of the kubectl command to generate the managed1.yaml file.\n# On the managed cluster $ MGD_CA_CERT=$(kubectl get secret system-tls \\  -n verrazzano-system \\  -o jsonpath=\"{.data.ca\\.crt}\" | base64 --decode) $ kubectl create secret generic \"ca-secret-managed1\" \\  -n verrazzano-mc \\  --from-literal=cacrt=\"$MGD_CA_CERT\" \\  --dry-run=client \\  -o yaml \u003e managed1.yaml   \n    Create a Secret on the admin cluster that contains the CA certificate for the managed cluster. This secret will be used for scraping metrics from the managed cluster. The file managed1.yaml that was created in the previous step provides input to this step.\n# On the admin cluster $ kubectl apply -f managed1.yaml # Once the command succeeds, you may delete the managed1.yaml file $ rm managed1.yaml   Obtain the publicly accessible Kubernetes API server address for the admin cluster from its kubeconfig file, using the following instructions.\n# First list the contexts in the kubeconfig file - sample output is shown below $ kubectl config get-contexts -o=name my-admin-cluster my-managed-cluster # From the output, find the admin cluster's context name and use it in the next command # View the information for that context in your kubeconfig file $ kubectl --context [your-admin-cluster-context-name] config view --minify apiVersion: v1 kind: Config clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://127.0.0.1:46829 name: kind-managed1 contexts: .... .... # In the output of the above command, the address shown in the \"server\" field is the Kubernetes API server address. # Set the ADMIN_K8S_SERVER_ADDRESS environment variable to that value e.g. in the above sample output, the # value is https://127.0.0.1:46829 $ export ADMIN_K8S_SERVER_ADDRESS=\u003cthe server address from the config output\u003e   On the admin cluster, create a ConfigMap that contains the externally accessible admin cluster Kubernetes server address found in the previous step.\n# On the admin cluster $ kubectl apply -f \u003c\u003cEOF - apiVersion: v1 kind: ConfigMap metadata: name: verrazzano-admin-cluster namespace: verrazzano-mc data: server: \"${ADMIN_K8S_SERVER_ADDRESS}\" EOF   Registration steps Perform the first three registration steps on the admin cluster, and the last step, on the managed cluster. The cluster against which to run the command is indicated in each code block.\nOn the admin cluster   To begin the registration process for a managed cluster named managed1, apply the VerrazzanoManagedCluster object on the admin cluster.\n# On the admin cluster $ kubectl apply -f \u003c\u003cEOF - apiVersion: clusters.verrazzano.io/v1alpha1 kind: VerrazzanoManagedCluster metadata: name: managed1 namespace: verrazzano-mc spec: description: \"Test VerrazzanoManagedCluster object\" caSecret: ca-secret-managed1 EOF   Wait for the VerrazzanoManagedCluster resource to reach the Ready status. At that point, it will have generated a YAML file that must be applied on the managed cluster to complete the registration process.\n# On the admin cluster $ kubectl wait --for=condition=Ready \\  vmc managed1 -n verrazzano-mc   Export the YAML file created to register the managed cluster.\n# On the admin cluster $ kubectl get secret verrazzano-cluster-managed1-manifest \\  -n verrazzano-mc \\  -o jsonpath={.data.yaml} | base64 --decode \u003e register.yaml   On the managed cluster Apply the registration file exported in the previous step, on the managed cluster.\n# On the managed cluster $ kubectl apply -f register.yaml # Once the command succeeds, you may delete the register.yaml file $ rm register.yaml After this step, the managed cluster will begin connecting to the admin cluster periodically. When the managed cluster connects to the admin cluster, it will update the Status field of the VerrazzanoManagedCluster resource for this managed cluster, with the following information:\n The timestamp of the most recent connection made from the managed cluster, in the lastAgentConnectTime status field. The host address of the Prometheus instance running on the managed cluster, in the prometheusHost status field. This is then used by the admin cluster to scrape metrics from the managed cluster. The API address of the managed cluster, in the apiUrl status field. This is used by the admin cluster’s API proxy to route incoming requests for managed cluster information, to the managed cluster’s API proxy.  Verifying that managed cluster registration completed You can perform all the verification steps on the admin cluster.\n  Verify that the managed cluster can connect to the admin cluster. View the status of the VerrazzanoManagedCluster resource on the admin cluster, and check whether the lastAgentConnectTime, prometheusUrl and apiUrl fields are populated. This may take up to 2 minutes after completing the registration steps.\n# On the admin cluster $ kubectl get vmc managed1 -n verrazzano-mc -o yaml # Sample output showing the status field spec: .... .... status: apiUrl: https://verrazzano.default.172.18.0.211.nip.io conditions: - lastTransitionTime: \"2021-07-07T15:49:43Z\" message: Ready status: \"True\" type: Ready lastAgentConnectTime: \"2021-07-16T14:47:25Z\" prometheusHost: prometheus.vmi.system.default.172.18.0.211.nip.io   Verify that the managed cluster is successfully registered with Rancher. When you perform the registration steps, Verrazzano also registers the managed cluster with Rancher. View the Rancher UI on the admin cluster. If the registration with Rancher was successful, then your cluster will be listed in Rancher’s list of clusters, and should be in Active state. You can find the Rancher UI URL for your cluster by following the instructions for Accessing Verrazzano.\n  Run applications in multicluster Verrazzano The Verrazzano multicluster setup is now complete and you can deploy applications by following the Multicluster Hello World Helidon example application.\nUse the admin cluster UI The admin cluster serves as a central point from which to register and deploy applications to managed clusters.\nIn the Verrazzano UI on the admin cluster, you can view the following:\n The managed clusters registered with this admin cluster. VerrazzanoProjects located on this admin cluster, or any of its registered managed clusters, or both. Applications located on this admin cluster, or any of its registered managed clusters, or both.  ","excerpt":"Prerequisites Before you begin, read this document, Verrazzano in a multicluster environment.\nOverview To set up a multicluster Verrazzano environment, you will need two or more Kubernetes clusters. …","ref":"/docs/setup/install/multicluster/","title":"Install Multicluster Verrazzano"},{"body":"Summary Analysis detected that there were nodes reporting insufficient memory.\nSteps   Review the analysis data to identify the specific nodes involved.\n  Review the nodes to determine why they do not have sufficient memory.\na. Are the nodes sized correctly for the workload?\n For the minimum resources required for installing Verrazzano, see the Installation Guide. Refer to documentation for other applications that you are deploying for resource guidelines and take those into account.  b. Is something unexpected running on the nodes or consuming more memory than expected?\n  Related information  Installation Guide Kubernetes Troubleshooting  ","excerpt":"Summary Analysis detected that there were nodes reporting insufficient memory.\nSteps   Review the analysis data to identify the specific nodes involved.\n  Review the nodes to determine why they do not …","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/insufficientmemory/","title":"Insufficient Memory"},{"body":"Verrazzano can be deployed to a number of different hosted and on-premises Kubernetes environemts. Particularly in hosted environments, it may not be possible to choose the authentication providers configured for the Kubernetes API server, and Verrazzano may have no ability to view, manage, or authenticate users.\nVerrazzano installs Keycloak to provide a common user store across all Kubernetes environents. The Verrazzano admin user can create and manage user accounts in Keycloak, and Verrazzano can authenticate and authorize Keycloak users.\nAlso, you can configure Keycloak to delegate authentication to an external user store, such as Active Directory or an LDAP server.\nBecause Keycloak is not configured as an authentication provider for the Kubernetes API, authenticating Keycloak users to Kubernetes requires the use of a proxy that impersonates Keycloak users when making Kubernetes API requests. For more information about the Verrazzano API proxy, see Verrazzano Proxies.\nKeycloak is also used when authenticating to the Verrazzano console and the various Verrazzano Monitoring Instance (VMI) logging and metrics consoles. The Verrazzano console uses the OpenID Connect (OIDC) PKCE flow to authenticate users against Keycloak and obtain ID and access tokens. Authentication for VMI consoles is provided by the Verrazzano OIDC proxy, which also uses PKCE to authenticate users, validates the resulting tokens, and authorizes incoming requests. For more information about the Verrazzano OIDC proxy, see Verrazzano Proxies.\n","excerpt":"Verrazzano can be deployed to a number of different hosted and on-premises Kubernetes environemts. Particularly in hosted environments, it may not be possible to choose the authentication providers …","ref":"/docs/security/keycloak/keycloak/","title":"Keycloak and SSO"},{"body":"Prepare for the OCI install   Create the OKE cluster using the OCI Console or by some other means.\n  For SHAPE, an OKE cluster with 3 nodes of VM.Standard2.4 OCI compute instance shape has proven sufficient to install Verrazzano and deploy the Bob’s Books example application.\n  Follow the instructions provided by OKE to download the Kubernetes configuration file for your cluster, and set the following ENV variable:\n   $ export KUBECONFIG=\u003cpath to valid Kubernetes config\u003e  Optional, if your organization requires the use of a private registry to the Docker images installed by Verrazzano, see Use a Private Registry.  NOTE: Verrazzano can create network policies that can be used to limit the ports and protocols that pods use for network communication. Network policies provide additional security but they are enforced only if you install a Kubernetes Container Network Interface (CNI) plug-in that enforces them, such as Calico. For an example on OKE, see Installing Calico and Setting Up Network Policies.\nNext steps To continue, see the Installation Guide.\n","excerpt":"Prepare for the OCI install   Create the OKE cluster using the OCI Console or by some other means.\n  For SHAPE, an OKE cluster with 3 nodes of VM.Standard2.4 OCI compute instance shape has proven …","ref":"/docs/setup/platforms/oci/oci/","title":"OCI Container Engine for Kubernetes (OKE)"},{"body":"Summary Analysis detected that there were pods which were in a pending state without detecting other specific issues related to them.\nSteps Review the analysis data. At a minimum, this should indicate which pods are being impacted and it might give other clues on the root cause.\nRelated information  Kubernetes Troubleshooting  ","excerpt":"Summary Analysis detected that there were pods which were in a pending state without detecting other specific issues related to them.\nSteps Review the analysis data. At a minimum, this should indicate …","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/pendingpods/","title":"Pending Pods"},{"body":"Summary Analysis detected that there were pods which were not in a running, succeeded, or pending state.\nThe analysis was not able to determine a specific root cause, however, it might have supplied data that is related to the pods in question. The root cause might be obvious from the supporting data, but the analysis tool isn’t isolating the specific scenario yet.\nSteps Review the analysis data. At a minimum, it should indicate which pods are being impacted and it might give other clues on the root cause.\nRelated information  Kubernetes Troubleshooting  ","excerpt":"Summary Analysis detected that there were pods which were not in a running, succeeded, or pending state.\nThe analysis was not able to determine a specific root cause, however, it might have supplied …","ref":"/docs/troubleshooting/diagnostictools/analysisadvice/podproblemsnotreported/","title":"Problem Pods"},{"body":"Before you begin Install Verrazzano by following the installation instructions.\nNOTE: The Spring Boot example application deployment files are contained in the Verrazzano project located at \u003cVERRAZZANO_HOME\u003e/examples/springboot-app, where VERRAZZANO_HOME is the root of the Verrazzano project.\nDeploy the Spring Boot application This example provides a simple web application developed using Spring Boot. For more information and the source code of this application, see the Verrazzano Examples.\n  Create a namespace for the Spring Boot application and add a label identifying the namespace as managed by Verrazzano.\n$ kubectl create namespace springboot $ kubectl label namespace springboot verrazzano-managed=true istio-injection=enabled   To deploy the application, apply the Spring Boot OAM resources.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/springboot-app/springboot-comp.yaml $ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/springboot-app/springboot-app.yaml   Wait for the Spring Boot application to be ready.\n$ kubectl wait \\ --for=condition=Ready pods \\ --all \\ -n springboot \\ --timeout=300s   Explore the application   Get the generated host name for the application.\n$ HOST=$(kubectl get gateway \\ -n springboot \\ -o jsonpath={.items[0].spec.servers[0].hosts[0]}) $ echo $HOST springboot-appconf.springboot.11.22.33.44.nip.io   Get the EXTERNAL_IP address of the istio-ingressgateway service.\n$ ADDRESS=$(kubectl get service \\ -n istio-system istio-ingressgateway \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}') $ echo $ADDRESS 11.22.33.44   Access the application:\n  Using the command line\n$ curl -sk \\ https://${HOST} \\ --resolve ${HOST}:443:${ADDRESS} $ curl -sk \\ https://${HOST}/facts \\ --resolve ${HOST}:443:${ADDRESS} If you are using nip.io, then you do not need to include --resolve.\n  Local testing with a browser\nTemporarily, modify the /etc/hosts file (on Mac or Linux) or c:\\Windows\\System32\\Drivers\\etc\\hosts file (on Windows 10), to add an entry mapping the host name to the ingress gateway’s EXTERNAL-IP address. For example:\n11.22.33.44 springboot.example.com Then, you can access the application in a browser at https://springboot.example.com/ and https://springboot.example.com/facts.\nIf you are using nip.io, then you can access the application in a browser using the HOST variable (for example, https://${HOST}/facts). If you are going through a proxy, you may need to add *.nip.io to the NO_PROXY list.\n  Using your own DNS name\n  Point your own DNS name to the ingress gateway’s EXTERNAL-IP address.\n  In this case, you would need to have edited the springboot-app.yaml file to use the appropriate value under the hosts section (such as yourhost.your.domain), before deploying the Spring Boot application.\n  Then, you can use a browser to access the application at https://\u003cyourhost.your.domain\u003e/ and https://\u003cyourhost.your.domain\u003e/facts.\nThe actuator endpoint is accessible under the path /actuator and the Prometheus endpoint exposing metrics data in a format that can be scraped by a Prometheus server is accessible under the path /actuator/prometheus.\n      A variety of endpoints associated with the deployed application, are available to further explore the logs, metrics, and such.\nAccessing them may require the following:\n  Run this command to get the password that was generated for the telemetry components:\n$ kubectl get secret \\ --namespace verrazzano-system verrazzano \\ -o jsonpath={.data.password} | base64 \\ --decode; echo The associated user name is verrazzano.\n  You will have to accept the certificates associated with the endpoints.\n  You can retrieve the list of available ingresses with following command:\n$ kubectl get ingress -n verrazzano-system NAME CLASS HOSTS ADDRESS PORTS AGE verrazzano-ingress \u003cnone\u003e verrazzano.default.140.141.142.143.nip.io 140.141.142.143 80, 443 7d2h vmi-system-es-ingest \u003cnone\u003e elasticsearch.vmi.system.default.140.141.142.143.nip.io 140.141.142.143 80, 443 7d2h vmi-system-grafana \u003cnone\u003e grafana.vmi.system.default.140.141.142.143.nip.io 140.141.142.143 80, 443 7d2h vmi-system-kibana \u003cnone\u003e kibana.vmi.system.default.140.141.142.143.nip.io 140.141.142.143 80, 443 7d2h vmi-system-prometheus \u003cnone\u003e prometheus.vmi.system.default.140.141.142.143.nip.io 140.141.142.143 80, 443 7d2h Using the ingress host information, some of the endpoints available are:\n   Description Address Credentials     Kibana https://[vmi-system-kibana ingress host] verrazzano/telemetry-password   Grafana https://[vmi-system-grafana ingress host] verrazzano/telemetry-password   Prometheus https://[vmi-system-prometheus ingress host] verrazzano/telemetry-password      Undeploy the application   To undeploy the application, delete the Spring Boot OAM resources.\n$ kubectl delete -f springboot-app.yaml $ kubectl delete -f springboot-comp.yaml   Delete the namespace springboot after the application pod is terminated.\n$ kubectl get pods -n springboot $ kubectl delete namespace springboot   ","excerpt":"Before you begin Install Verrazzano by following the installation instructions.\nNOTE: The Spring Boot example application deployment files are contained in the Verrazzano project located at …","ref":"/docs/samples/spring-boot/","title":"Spring Boot"},{"body":"Verrazzano supports being able to install from a private Docker-compliant container registry, which requires:\n Loading all required Verrazzano container images into your own registry and repository. Installing the Verrazzano platform operator with the private registry and repository used to load the images.  To load the required Verrazzano images and prepare for an installation from your private registry, you must do the following:\n Download the TAR file containing the full set of images for the Verrazzano release from the Oracle Software Download Center. Extract all files from the TAR file locally and follow the instructions in the README file in the archive.  ","excerpt":"Verrazzano supports being able to install from a private Docker-compliant container registry, which requires:\n Loading all required Verrazzano container images into your own registry and repository. …","ref":"/docs/setup/private-registry/private-registry/","title":"Use a Private Registry"},{"body":"Verrazzano uses the OAM specification to provide a layered approach to describing and deploying applications. The Open Application Model (OAM) is a specification developed within the Cloud Native Computing Foundation (CNCF). Verrazzano is compliant with the OAM specification version 0.2.1.\nAn ApplicationConfiguration is a composition of Components. Components encapsulate application implementation details. Application deployers apply traits and scopes to customize the Components for the environment.\nThe OAM specification supports extensibility. The behavior of the platform can be extended by adding OAM compliant definitions and controllers. Specifically, new workload, trait, and scope definitions can be added. These definitions can be referenced by components and application configurations and are processed by custom controllers.\nApplication configurations An ApplicationConfiguration is a collection of references to Components. A set of Traits and Scopes can be applied to each Component reference. The platform uses these Components, Traits, and Scopes to generate the final application resources during deployment.\nThe following sample shows the high level structure of an ApplicationConfiguration.\napiVersion:core.oam.dev/v1alpha2kind:ApplicationConfiguration...spec:components:- componentName:example-component-1traits:...scopes:...- componentName:example-component-2...Components A Component wraps the content of a workload. The platform extracts the workload during deployment and creates new resources that result from the application of Traits and Scopes. Verrazzano and the OAM specification provide several workloads, for example VerrazzanoHelidonWorkload and ContainerizedWorkload. The workloads also can be any Kubernetes resource. For some Kubernetes resources, the oam-kubernetes-runtime operator may need to be granted additional permission.\nA Component can also be parameterized; this allows the workload content to be customized when referenced within an ApplicationConfiguration. See the OAM specification for details.\nThe following sample shows the high level structure of a Component.\napiVersion:core.oam.dev/v1alpha2kind:Component...spec:workload:...parameters:...Workloads Components contain an embedded workload. Verrazzano and the OAM specification provide several workloads, for example VerrazzanoWebLogicWorkload and ContainerizedWorkload. Workloads can also be any Kubernetes resource.\nThe following sample shows a VerrazzanoHelidonWorkload workload embedded within a Component.\napiVersion:core.oam.dev/v1alpha2kind:Component...spec:workload:apiVersion:oam.verrazzano.io/v1alpha1kind:VerrazzanoHelidonWorkloadspec:deploymentTemplate:podSpec:containers:- name:example-containerimage:......A workload can optionally have an associated WorkloadDefinition. This provides the platform with information about the schema of the workload. A WorkloadDefintion is typically provided by the platform, not an end user.\nTraits Traits customize component workloads and generate related resources during deployment. Verrazzano provides several traits, for example IngressTrait and MetricsTrait. The platform extracts traits contained within an ApplicationConfiguration during deployment. This processing is similar to the extraction of workload content from Component resources. Note that for some Kubernetes resources the oam-kubernetes-runtime operator may need to be granted create permission.\nA Kubernetes operator, for example verrazzano-application-operator, processes these extracted traits and may create additional related resources or may alter related workloads. Each trait implementation will behave differently.\nThe following sample shows an IngressTrait applied to a referenced Component.\napiVersion:core.oam.dev/v1alpha2kind:ApplicationConfiguration...spec:components:- componentName:example-componenttraits:- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:IngressTraitspec:rules:- paths:- path:\"/greet\"Each trait type optionally can have an associated TraitDefinition. This provides the platform with additional information about the trait’s schema and workloads to which the trait can be applied. A TraitDefintion is typically provided by the platform, not an end user.\nScopes Scopes customize component workloads and generate related resources during deployment. An ApplicationConfiguration contains scope references instead of the scope’s content being embedded. The platform will update the scopes with a reference to each applied component. This update triggers the related operator to process the scope.\nThe following sample shows a reference to a HealthScope named example-health-scope.\napiVersion:core.oam.dev/v1alpha2kind:ApplicationConfiguration...spec:components:- componentName:example-componentscopes:- scopeRef:apiVersion:core.oam.dev/v1alpha2kind:HealthScopename:example-health-scope...The following sample shows the configuration details of the referenced HealthScope.\napiVersion:core.oam.dev/v1alpha2kind:HealthScopemetadata:name:example-health-scopespec:probe-method:GETprobe-endpoint:/healthEach scope type can optionally have an associated ScopeDefinition. This provides the platform with additional information about processing the scope.\n The scope’s schema. The workload types to which the scope can be applied. The field within the scope used to record related component references.  A ScopeDefintion is typically provided by the platform, not an end user.\nVerrazzano workloads The Verrazzano platform provides several workload definitions and implementations:\n The VerrazzanoWebLogicWorkload is used for WebLogic workloads. The VerrazzanoCoherenceWorkload is used for Coherence workloads. See Coherence Workload. The VerrazzanoHelidonWorkload is used for Helidon workloads. See Helidon Workload.  OAM ContainerizedWorkload The ContainerizedWorkload should be used for long-running container workloads which are not covered by the workload types described previously. This workload type is similar to the Deployment workload. It is provided to ensure that OAM can be used for non-Kubernetes deployment environments. See the OAM specification.\nVerrazzano traits The Verrazzano platform provides several trait definitions and implementations:\n IngressTrait MetricsTrait  IngressTrait The IngressTrait provides a simplified integration with the Istio ingress gateway included in the Verrazzano platform. The verrazzano-application-operator processes each IngressTrait and generates related Gateway, VirtualService, and Certificate resources when processed. The Certificate is created in the istio-system namespace. The values used to create are either explicitly provided in the trait or are derived from the environment or associated component.\nThe following sample shows an IngressTrait that results in the application being accessible using the path /greet.\napiVersion:core.oam.dev/v1alpha2kind:ApplicationConfiguration...spec:components:- componentName:example-componenttraits:- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:IngressTraitspec:rules:- paths:- path:\"/greet\"See the API documentation for details.\nMetricsTrait The MetricsTrait provides a simplified integration with the Prometheus service included in the Verrazzano platform. The verrazzano-application-operator processes each MetricsTrait and does two things:\n Updates the workload’s annotations to provide metrics source information. Updates the Prometheus’ metrics scrape configuration with metrics scrape targets. The Verrazzano platform will automatically apply a MetricsTrait to every component with a supported workload.  The following sample shows the a MetricsTrait that was automatically applied.\napiVersion:core.oam.dev/v1alpha2kind:ApplicationConfiguration...spec:components:- componentName:example-componenttraits:- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:MetricsTraitSee the API documentation for details.\nKubernetes resources Verrazzano and OAM provide workloads and traits to define and customize applications. However, some situations may require resources beyond those provided. In this case, other existing Kubernetes resources can also be used. The todo-list example takes advantage of this capability in several components to support unique Service and ConfigMap requirements.\nMost Kubernetes resources can be embedded as a workload within a Component. The following sample shows how a Deployment can be embedded as a workload within a Component. The oam-kubernetes-runtime operator will process the Component and extract the Deployment to a separate resource during deployment.\napiVersion:core.oam.dev/v1alpha2kind:Component...spec:workload:kind:DeploymentapiVersion:apps/v1name:...spec:selector:...template:...Most Kubernetes resources can also be embedded as a trait within an ApplicationConfiguration. The following sample shows how an Ingress can be embedded as a trait within an ApplicationConfiguration. The oam-kubernetes-runtime operator will process the ApplicationConfiguration and extract the Ingress to a separate resource during deployment. In the following sample, note that the Ingress is the Kubernetes Ingress, not the IngressTrait provided by Verrazzano.\napiVersion:core.oam.dev/v1alpha2kind:ApplicationConfiguration...spec:components:- componentName:...traits:- trait:apiVersion:networking.k8s.io/v1beta1kind:Ingress...spec:rules:...The oam-kubernetes-runtime operator has a limited set of privileges by default. Your cluster administrator may need to grant the oam-kubernetes-runtime operator additional privileges to enable the use of some Kubernetes resources as workloads or traits. Create additional roles and role bindings for the specific resources to be embedded as workloads or traits. The following examples of ClusterRole and ClusterRoleBinding show how oam-kubernetes-runtime can be granted privileges to manage Ingress resources.\napiVersion:rbac.authorization.k8s.io/v1kind:ClusterRolemetadata:name:oam-kubernetes-runtime-ingressesrules:- apiGroups:- networking.k8s.io- extensionsresources:- ingressesverbs:- create- delete- get- list- patch- updateapiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:oam-kubernetes-runtime-ingressesroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:oam-kubernetes-runtime-ingressessubjects:- kind:ServiceAccountname:oam-kubernetes-runtimenamespace:verrazzano-systemDeployment An application deployment occurs in Verrazzano through a number of Kubernetes controllers, reading and writing various resources. Each controller processes application resources, and generates or updates other related resources. Different types of controllers process different levels of application resources.\nThe ApplicationConfiguration controller processes ApplicationConfiguration and Component resources. This controller extracts and stores workload for each Component referenced within ApplicationConfiguration resources. Verrazzano implements the ApplicationConfiguration controller within the oam-kubernetes-runtime operator. Similarly, the ApplicationConfiguration controller extracts and stores Trait resources associated with Component resources in the ApplicationConfiguration.\nThe workload controllers process workload resources created by the ApplicationConfiguration controller, for example ContainerizedWorkload or VerrazzanoWebLogicWorkload. This controller processes these workload resources and generates more specific runtime resources. For example, the ContainerizedWorkload controller processes a ContainerizedWorkload resource and generates a Deployment resource. The VerrazzanoWebLogicWorkload controller processes a VerrazzanoWebLogicWorkload resource and generates a Domain resource. These controllers may take into account traits and scopes that are applied to the workload’s Component references within the ApplicationConfiguration. Verrazzano implements these workload controllers in two operators. Verrazzano specific workloads, for example VerrazzanoHelidonWorkload, are processed by a controller within the verrazzano-application-operator. Workloads defined by OAM, for example ContainerizedWorkload, are processed by a controller with the oam-kubernetes-runtime operator.\nThe trait controllers process trait resources created by the ApplicationConfiguration controller, for example MetricsTrait. The ApplicationConfiguration controller records the Component to which it was applied within each extracted trait. The trait controllers process extracted trait resources, and generate or update other related resources. For example, the IngressTrait controller within the verrazzano-application-operator processes IngressTrait resources and generates related Gateway and VirtualService resources. The same operator contains a MetricsTrait controller which processes MetricsTrait resources and adds annotations to related resources such as Deployments.\nScope controllers read scope resources updated by the ApplicationConfiguration controller during deployment. The ApplicationConfiguration controller updates the scope resources with references to each Component to which the scope is applied.\nThe following diagram shows the relationships between the resources and controllers described previously. The following diagram, based on the hello-helidon example, shows the processing of resources from a Kubernetes operator perspective. Controllers within the oam-kubernetes-runtime process the ApplicationConfiguration and Component resources and generate VerrazzanoHelidonWorkload and IngressTrait. Then controllers within the verrazzano-application-operator process the VerrazzanoHelidonWorkload and IngressTrait resources to generate Deployment, VirtualService, and other resources.\n","excerpt":"Verrazzano uses the OAM specification to provide a layered approach to describing and deploying applications. The Open Application Model (OAM) is a specification developed within the Cloud Native …","ref":"/docs/applications/","title":"Applications"},{"body":"","excerpt":"","ref":"/docs/setup/install/customizing/","title":"Customize Installations"},{"body":"This example demonstrates using standard Kubernetes resources, in conjunction with OAM resources, to define and deploy an application. Several standard Kubernetes resources are used in this example, both as workloads and traits.\n Deployment is used as a workload within a Component. Service is used as a workload within a Component. Ingress is used as a trait within an ApplicationConfiguration.  Before you begin Install Verrazzano by following the installation instructions.\nGrant permissions The oam-kubernetes-runtime is not installed with privileges that allow it to create the Kubernetes Ingress resource used in this example. The following steps create a role that allows Ingress resource creation and binds that role to the oam-kubernetes-runtime service account. For this example to work, your cluster admin will need to run the following steps to create the ClusterRole and ClusterRoleBinding.\n$ kubectl apply -f - \u003c\u003cEOF apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: oam-kubernetes-runtime-ingresses rules: - apiGroups: - networking.k8s.io - extensions resources: - ingresses verbs: - create - delete - get - list - patch - update --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: oam-kubernetes-runtime-ingresses roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: oam-kubernetes-runtime-ingresses subjects: - kind: ServiceAccount name: oam-kubernetes-runtime namespace: verrazzano-system EOF Deploy the application This example provides an web application using a common example application image. When accessed, the application returns the configured text.\n  Create the application namespace and add a label identifying the namespace as managed by Verrazzano.\n$ kubectl create namespace oam-kube $ kubectl label namespace oam-kube verrazzano-managed=true istio-injection=enabled   Create a Component containing a Deployment workload.\n$ kubectl apply -f - \u003c\u003cEOF apiVersion: core.oam.dev/v1alpha2 kind: Component metadata: name: oam-kube-dep-comp namespace: oam-kube spec: workload: kind: Deployment apiVersion: apps/v1 name: oam-kube-dep spec: replicas: 1 selector: matchLabels: app: oam-kube-app template: metadata: labels: app: oam-kube-app spec: containers: - name: oam-kube-cnt image: hashicorp/http-echo args: - \"-text=hello\" EOF   Create a Component containing a Service workload.\n$ kubectl apply -f - \u003c\u003cEOF apiVersion: core.oam.dev/v1alpha2 kind: Component metadata: name: oam-kube-svc-comp namespace: oam-kube spec: workload: kind: Service apiVersion: v1 metadata: name: oam-kube-svc spec: selector: app: oam-kube-app ports: - port: 5678 # Default port for image EOF   Create an ApplicationConfiguration referencing both Components and configuring an ingress trait.\n$ kubectl apply -f - \u003c\u003cEOF apiVersion: core.oam.dev/v1alpha2 kind: ApplicationConfiguration metadata: name: oam-kube-appconf namespace: oam-kube spec: components: - componentName: oam-kube-dep-comp - componentName: oam-kube-svc-comp traits: - trait: apiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata: name: oam-kube-ing annotations: kubernetes.io/ingress.class: istio spec: rules: - host: oam-kube-app.example.com http: paths: - path: /example backend: serviceName: oam-kube-svc servicePort: 5678 EOF   Explore the application  Get the host name for the application. $ export HOST=$(kubectl get ingress \\  -n oam-kube oam-kube-ing \\  -o jsonpath='{.spec.rules[0].host}') $ echo \"HOST=${HOST}\"  Get the load balancer address of the ingress gateway. $ export LOADBALANCER=$(kubectl get ingress \\  -n oam-kube oam-kube-ing \\  -o jsonpath='{.status.loadBalancer.ingress[0].ip}') $ echo \"LOADBALANCER=${LOADBALANCER}\"  Access the application. $ curl http://${HOST}/example --resolve ${HOST}:80:${LOADBALANCER}   Undeploy the application To undeploy the application, delete the namespace created. This will result in the deletion of all explicitly and implicitly created resources in the namespace.\n$ kubectl delete namespace oam-kube If desired, the cluster admin also can remove the created ClusterRole and ClusterRoleBinding.\n$ kubectl delete oam-kubernetes-runtime-ingresses am-kubernetes-runtime-ingresses $ kubectl delete ClusterRole oam-kubernetes-runtime-ingresses ","excerpt":"This example demonstrates using standard Kubernetes resources, in conjunction with OAM resources, to define and deploy an application. Several standard Kubernetes resources are used in this example, …","ref":"/docs/samples/standard-kubernetes/","title":"Standard Kubernetes Resources"},{"body":"Before you begin  Install Verrazzano by following the installation instructions. To download the example image, you must first accept the license agreement.  In a browser, navigate to https://container-registry.oracle.com/ and sign in. Search for example-todo and select the image name in the results. Click Continue, then read and accept the license agreement.    NOTE: The ToDo List example application deployment files are contained in the Verrazzano project located at \u003cVERRAZZANO_HOME\u003e/examples/todo-list, where \u003cVERRAZZANO_HOME\u003e is the root of the Verrazzano project.\nAll files and paths in this document are relative to \u003cVERRAZZANO_HOME\u003e/examples/todo-list.\nDeploy the ToDo List application ToDo List is an example application containing a WebLogic component. For more information and the source code of this application, see the Verrazzano Examples.\n  Create a namespace for the ToDo List example and add a label identifying the namespace as managed by Verrazzano.\n$ kubectl create namespace todo-list $ kubectl label namespace todo-list verrazzano-managed=true istio-injection=enabled   Create a docker-registry secret to enable pulling the ToDo List example image from the registry.\n$ kubectl create secret docker-registry tododomain-repo-credentials \\ --docker-server=container-registry.oracle.com \\ --docker-username=YOUR_REGISTRY_USERNAME \\ --docker-password=YOUR_REGISTRY_PASSWORD \\ --docker-email=YOUR_REGISTRY_EMAIL \\ -n todo-list Replace YOUR_REGISTRY_USERNAME, YOUR_REGISTRY_PASSWORD, and YOUR_REGISTRY_EMAIL with the values you use to access the registry.\n  Create and label secrets for the WebLogic domain:\n # Replace the values of the WLS_USERNAME and WLS_PASSWORD environment variables as appropriate. $ export WLS_USERNAME=\u003cusername\u003e $ export WLS_PASSWORD=\u003cpassword\u003e $ kubectl create secret generic tododomain-weblogic-credentials \\ --from-literal=password=$WLS_PASSWORD \\ --from-literal=username=$WLS_USERNAME \\ -n todo-list $ kubectl create secret generic tododomain-jdbc-tododb \\ --from-literal=username=$WLS_USERNAME \\ --from-literal=password=$WLS_PASSWORD \\ -n todo-list $ kubectl -n todo-list label secret tododomain-jdbc-tododb weblogic.domainUID=tododomain Note that the ToDo List example application is preconfigured to use specific secret names. For the source code of this application, see the Verrazzano Examples.\n  To deploy the application, apply the example resources.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/todo-list/todo-list-components.yaml $ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/todo-list/todo-list-application.yaml   Wait for the ToDo List application to be ready. You may need to repeat this command several times before it is successful. The tododomain-adminserver pod may take a while to be created and Ready.\n$ kubectl wait pod \\ --for=condition=Ready tododomain-adminserver \\ -n todo-list   Get the generated host name for the application.\n$ HOST=$(kubectl get gateway \\ -n todo-list \\ -o jsonpath={.items[0].spec.servers[0].hosts[0]}) $ echo $HOST todo-appconf.todo-list.11.22.33.44.nip.io   Get the EXTERNAL_IP address of the istio-ingressgateway service.\n$ ADDRESS=$(kubectl get service \\ -n istio-system istio-ingressgateway \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}') $ echo $ADDRESS 11.22.33.44   Access the ToDo List application:\n  Using the command line\n$ curl -sk \\ https://${HOST}/todo/ \\ --resolve ${HOST}:443:${ADDRESS} If you are using nip.io, then you do not need to include --resolve.\n  Local testing with a browser\nTemporarily, modify the /etc/hosts file (on Mac or Linux) or c:\\Windows\\System32\\Drivers\\etc\\hosts file (on Windows 10), to add an entry mapping the host name to the ingress gateway’s EXTERNAL-IP address. For example:\n11.22.33.44 todo.example.com Then, you can access the application in a browser at https://todo.example.com/todo.\n  Using your own DNS name\n  Point your own DNS name to the ingress gateway’s EXTERNAL-IP address.\n  In this case, you would need to have edited the todo-list-application.yaml file to use the appropriate value under the hosts section (such as yourhost.your.domain), before deploying the ToDo List application.\n  Then, you can use a browser to access the application at https://\u003cyourhost.your.domain\u003e/todo/.\nAccessing the application in a browser opens the page, “Derek’s ToDo List”, with an edit field and an Add button that lets you add tasks.\n      A variety of endpoints associated with the deployed ToDo List application, are available to further explore the logs, metrics, and such. Accessing them may require the following:\n  Run this command to get the password that was generated for the telemetry components:\n$ kubectl get secret \\ --namespace verrazzano-system verrazzano \\ -o jsonpath={.data.password} | base64 \\ --decode; echo The associated user name is verrazzano.\n  You will have to accept the certificates associated with the endpoints.\n  You can retrieve the list of available ingresses with following command:\n$ kubectl get ingress -n verrazzano-system NAME CLASS HOSTS ADDRESS PORTS AGE verrazzano-ingress \u003cnone\u003e verrazzano.default.140.141.142.143.nip.io 140.141.142.143 80, 443 7d2h vmi-system-es-ingest \u003cnone\u003e elasticsearch.vmi.system.default.140.141.142.143.nip.io 140.141.142.143 80, 443 7d2h vmi-system-grafana \u003cnone\u003e grafana.vmi.system.default.140.141.142.143.nip.io 140.141.142.143 80, 443 7d2h vmi-system-kibana \u003cnone\u003e kibana.vmi.system.default.140.141.142.143.nip.io 140.141.142.143 80, 443 7d2h vmi-system-prometheus \u003cnone\u003e prometheus.vmi.system.default.140.141.142.143.nip.io 140.141.142.143 80, 443 7d2h Using the ingress host information, some of the endpoints available are:\n   Description Address Credentials     Kibana https://[vmi-system-kibana ingress host] verrazzano/telemetry-password   Grafana https://[vmi-system-grafana ingress host] verrazzano/telemetry-password   Prometheus https://[vmi-system-prometheus ingress host] verrazzano/telemetry-password      Access the WebLogic Server Administration Console   Set up port forwarding.\n$ kubectl port-forward pods/tododomain-adminserver 7001:7001 -n todo-list   Access the WebLogic Server Administration Console from your browser.\nhttp://localhost:7001/console   NOTE It is recommended that the WebLogic Server Administration Console not be exposed publicly.  Troubleshooting   Verify that the application configuration, domain, and ingress trait all exist.\n$ kubectl get ApplicationConfiguration -n todo-list NAME AGE todo-appconf 19h $ kubectl get Domain -n todo-list NAME AGE todo-domain 19h $ kubectl get IngressTrait -n todo-list NAME AGE todo-domain-trait-7cbd798c96 19h   Verify that the WebLogic Administration Server and MySQL pods have been created and are running. Note that this will take several minutes.\n$ kubectl get pods -n todo-list NAME READY STATUS RESTARTS AGE mysql-5c75c8b7f-vlhck 1/1 Running 0 19h tododomain-adminserver 2/2 Running 0 19h   ","excerpt":"Before you begin  Install Verrazzano by following the installation instructions. To download the example image, you must first accept the license agreement.  In a browser, navigate to …","ref":"/docs/samples/todo-list/","title":"ToDo List"},{"body":"Before you begin   Install Verrazzano by following the installation instructions.\n  To download the example image, you must first accept the license agreement.\n In a browser, navigate to https://container-registry.oracle.com/ and sign in. Search for example-bobbys-coherence, example-bobbys-front-end, example-bobs-books-order-manager, and example-roberts-coherence. For each, select the image name in the results, click Continue, then read and accept the license agreement.  NOTE: The Bob’s Books example application deployment files are contained in the Verrazzano project located at \u003cVERRAZZANO_HOME\u003e/examples/bobs-books, where \u003cVERRAZZANO_HOME\u003e is the root of the Verrazzano project.\n  Overview Bob’s Books consists of three main parts:\n A back-end “order processing” application, which is a Java EE application with REST services and a very simple JSP UI, which stores data in a MySQL database. This application runs on WebLogic Server. A front-end web store “Robert’s Books”, which is a general book seller. This is implemented as a Helidon microservice, which gets book data from Coherence, uses a Coherence cache store to persist data for the order manager, and has a React web UI. A front-end web store “Bobby’s Books”, which is a specialty children’s book store. This is implemented as a Helidon microservice, which gets book data from a (different) Coherence cache store, interfaces directly with the order manager, and has a JSF web UI running on WebLogic Server.  For more information and the source code of this application, see the Verrazzano Examples.\nDeploy the example application   Create a namespace for the example and add a label identifying the namespace as managed by Verrazzano.\n$ kubectl create namespace bobs-books $ kubectl label namespace bobs-books verrazzano-managed=true istio-injection=enabled   Create a docker-registry secret to enable pulling the example image from the registry.\n$ kubectl create secret docker-registry bobs-books-repo-credentials \\ --docker-server=container-registry.oracle.com \\ --docker-username=YOUR_REGISTRY_USERNAME \\ --docker-password=YOUR_REGISTRY_PASSWORD \\ --docker-email=YOUR_REGISTRY_EMAIL \\ -n bobs-books Replace YOUR_REGISTRY_USERNAME, YOUR_REGISTRY_PASSWORD, and YOUR_REGISTRY_EMAIL with the values you use to access the registry.\n  Create secrets for the WebLogic domains:\n# Replace the values of the WLS_USERNAME and WLS_PASSWORD environment variables as appropriate. $ export WLS_USERNAME=\u003cusername\u003e $ export WLS_PASSWORD=\u003cpassword\u003e $ kubectl create secret generic bobbys-front-end-weblogic-credentials \\ --from-literal=password=$WLS_PASSWORD \\ --from-literal=username=$WLS_USERNAME \\ -n bobs-books $ kubectl create secret generic bobs-bookstore-weblogic-credentials \\ --from-literal=password=$WLS_PASSWORD \\ --from-literal=username=$WLS_USERNAME \\ -n bobs-books $ kubectl create secret generic mysql-credentials \\ --from-literal=username=$WLS_USERNAME \\ --from-literal=password=$WLS_PASSWORD \\ --from-literal=url=jdbc:mysql://mysql.bobs-books.svc.cluster.local:3306/books \\ -n bobs-books Note that the example application is preconfigured to use specific secret names. For the source code of this application, see the Verrazzano Examples. If you want to use secret names that are different from what is specified in the source code, you will need to update the corresponding YAML file and rebuild the Docker images for the example application.\n  To deploy the application, apply the example resources.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/bobs-books/bobs-books-comp.yaml $ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/bobs-books/bobs-books-app.yaml   Wait for all of the pods in the Bob’s Books example application to be ready. You may need to repeat this command several times before it is successful. The WebLogic Server and Coherence pods may take a while to be created and Ready.\n$ kubectl wait \\ --for=condition=Ready pods \\ --all -n bobs-books \\ --timeout=600s   Get the EXTERNAL_IP address of the istio-ingressgateway service.\n$ kubectl get service \\ -n \"istio-system\" \"istio-ingressgateway\" \\ -o jsonpath={.status.loadBalancer.ingress[0].ip} 11.22.33.44   Get the generated host name for the application.\n$ kubectl get gateway bobs-books-bobs-books-gw \\ -n bobs-books \\ -o jsonpath={.spec.servers[0].hosts[0]} bobs-books.bobs-books.11.22.33.44.nip.io   Access the application. To access the application in a browser, you will need to do one of the following:\n  Option 1: If you are using nip.io, then you can access the application using the generated host name. For example:\n  Robert’s Books UI at https://bobs-books.bobs-books.11.22.33.44.nip.io/.\n  Bobby’s Books UI at https://bobs-books.bobs-books.11.22.33.44.nip.io/bobbys-front-end/.\n  Bob’s order manager UI at https://bobs-books.bobs-books.11.22.33.44.nip.io/bobs-bookstore-order-manager/orders.\n    Option 2: Temporarily, modify the /etc/hosts file (on Mac or Linux) or c:\\Windows\\System32\\Drivers\\etc\\hosts file (on Windows 10), to add an entry mapping the host used by the application to the external IP address assigned to your gateway. For example:\n11.22.33.44 bobs-books.example.com Then, you can use a browser to access the application, as shown:\n  Robert’s Books UI at https://bobs-books.example.com/.\n  Bobby’s Books UI at https://bobs-books.example.com/bobbys-front-end/.\n  Bob’s order manager UI at https://bobs-books.example.com/bobs-bookstore-order-manager/orders.\n    Option 3: Alternatively, point your own DNS name to the load balancer’s external IP address. In this case, you would need to have edited the bobs-books-app.yaml file to use the appropriate values under the hosts section for the application (such as your-roberts-books-host.your.domain), before deploying the application. Then, you can use a browser to access the application, as shown:\n  Robert’s Books UI at https://\u003cyour-roberts-books-host.your.domain\u003e/.\n  Bobby’s Books UI at https://\u003cyour-bobbys-books-host.your.domain\u003e/bobbys-front-end/.\n  Bob’s order manager UI at https://\u003cyour-bobs-orders-host.your.domain\u003e/.\n      Access the applications using the WLS Administration Console Use the WebLogic Server Administration Console to access the applications as follows.\nNOTE It is recommended that the WebLogic Server Administration Console not be exposed publicly.  Access bobs-bookstore   Set up port forwarding.\n$ kubectl port-forward pods/bobs-bookstore-adminserver 7001:7001 -n bobs-books   Access the WebLogic Server Administration Console from your browser.\nhttp://localhost:7001/console   Access bobbys-front-end   Set up port forwarding.\n$ kubectl port-forward pods/bobbys-front-end-adminserver 7001:7001 -n bobs-books   Access the WebLogic Server Administration Console from your browser.\nhttp://localhost:7001/console   Troubleshooting   Verify that the application configuration, domains, Coherence resources, and ingress trait all exist.\n$ kubectl get ApplicationConfiguration -n bobs-books $ kubectl get Domain -n bobs-books $ kubectl get Coherence -n bobs-books $ kubectl get IngressTrait -n bobs-books   Verify that the service pods are successfully created and transition to the READY state. Note that this may take a few minutes and that you may see some of the services terminate and restart.\n$ kubectl get pods -n bobs-books NAME READY STATUS RESTARTS AGE bobbys-helidon-stock-application-868b5965c8-dk2xb 3/3 Running 0 19h bobbys-coherence-0 2/2 Running 0 19h bobbys-front-end-adminserver 3/3 Running 0 19h bobbys-front-end-managed-server1 3/3 Running 0 19h bobs-bookstore-adminserver 3/3 Running 0 19h bobs-bookstore-managed-server1 3/3 Running 0 19h mysql-669665fb54-9m8wq 2/2 Running 0 19h robert-helidon-96997fcd5-kzjkf 3/3 Running 0 19h robert-helidon-96997fcd5-nlswm 3/3 Running 0 19h roberts-coherence-0 2/2 Running 0 17h roberts-coherence-1 2/2 Running 0 17h   ","excerpt":"Before you begin   Install Verrazzano by following the installation instructions.\n  To download the example image, you must first accept the license agreement.\n In a browser, navigate to …","ref":"/docs/samples/bobs-books/","title":"Bob's Books"},{"body":"A Verrazzano instance is comprised of both Verrazzano components and several third party products. Collectively, these components are called the Verrazzano system components. In addition, after Verrazzano is installed, a Verrazzano instance can include applications deployed by the user. Applications can also be referred to as components, not to be confused with OAM Components.\nAll of the system components and applications use the network to some degree. Verrazzano configures networking to provide network security and traffic management. Network settings are configured both at installation and during runtime as applications as are deployed into the Kubernetes cluster.\nHigh-level overview The following diagram shows the high-level overview of Verrazzano networking using ExternalDNS and Let’s Encrypt for certificates. ExternalDNS and cert-manager both run outside the mesh and connect to external services using TLS. This diagram does not show Prometheus scraping.\nVerrazzano system traffic enters a platform load balancer over TLS and is routed to the NGINX Ingress Controller, where TLS is terminated. From there, the traffic is routed to one of the system components in the mesh over mTLS, or using HTTP to a system component, outside the mesh.\nApplication traffic enters a second OCI load balancer over TLS and is routed to the Istio ingress gateway, where TLS is terminated. From there, the traffic is routed to one of several applications using mTLS.\nNOTE: Applications can be deployed outside the mesh, but the Istio ingress gateway will send traffic to them using plaintext. You need to do some additional configuration to enable TLS passthrough, as described at Istio Gateway Passthrough.\nHigh-level network diagram Platform network connectivity A Kubernetes cluster is installed on a platform, such as Oracle OKE, an on-premises installation, a hybrid cloud topology, or such. Verrazzano interfaces only with Kubernetes; it has no knowledge of platform topology or network security. You must ensure that there is network connectivity. For example, the ingresses might use a platform load balancer that provides the entry point into the cluster for Verrazzano consoles and applications. These load balancer IP addresses must be accessible for your users. In the multicluster case, clusters might be on different platform technologies with firewalls between them. Again, you must ensure that the clusters have network connectivity.\nNetwork configuration during installation A summary of the network-related configuration follows.\nVerrazzano does the following as it relates to networking:\n Installs and configures NGINX Ingress Controller. Creates Ingress resources for system components. Installs and configures Istio. Enables strict mTLS for the mesh by creating an Istio PeerAuthentication resource. Creates an Istio egress gateway service. Creates an Istio ingress gateway service. Configures several Verrazzano system components to be in the mesh. Optionally, installs ExternalDNS and creates DNS records. Creates certificates required by TLS, used by system components. Creates certificates required by Kubernetes API server to call webhook. Creates NetworkPolicies for all of the system components.  Network configuration during application life cycle Verrazzano does the following as it relates to applications being deployed and terminated:\n Optionally, creates an Istio Gateway and VirtualService resources. Creates Istio AuthorizationPolicies, as needed. Creates Istio DestinationRules, as needed. Optionally, creates a self-signed certificate for the application. Optionally, creates DNS records using ExternalDNS.  ","excerpt":"A Verrazzano instance is comprised of both Verrazzano components and several third party products. Collectively, these components are called the Verrazzano system components. In addition, after …","ref":"/docs/networking/","title":"Networking"},{"body":"Prepare for the OCLNE install Oracle Linux Cloud Native Environment can be installed in several different types of environments. These range from physical, on-premises hardware to virtualized cloud infrastructure. The Oracle Linux Cloud Native Environment installation instructions assume that networking and compute resources already exist. The basic infrastructure requirements are a network with a public and private subnet and a set of hosts connected to those networks.\nOCI example The following is an example of OCI infrastructure that can be used to evaluate Verrazzano installed on Oracle Linux Cloud Native Environment. If other environments are used, the capacity and configuration should be similar.\nYou can use the VCN Wizard of the OCI Console to automatically create most of the described network infrastructure. Additional security lists/rules, as detailed below, need to be added manually. All CIDR values provided are examples and can be customized as required.\nVirtual Cloud Network (for example, CIDR 10.0.0.0/16) Public Subnet (for example, CIDR 10.0.0.0/24)\nSecurity List / Ingress Rules\n   Stateless Destination Protocol Source Ports Destination Ports Type \u0026 Code Description     No 0.0.0.0/0 ICMP   3, 4 ICMP errors   No 10.0.0.0/16 ICMP   3 ICMP errors   No 0.0.0.0/0 TCP All 22  SSH   No 0.0.0.0/0 TCP All 80  HTTP load balancer   No 0.0.0.0/0 TCP All 443  HTTPS load balancer    Security List / Egress Rules\n   Stateless Destination Protocol Source Ports Destination Ports Type \u0026 Code Description     No 10.0.1.0/24 TCP All 22  SSH   No 10.0.1.0/24 TCP All 30080  HTTP load balancer   No 10.0.1.0/24 TCP All 30443  HTTPS load balancer   No 10.0.1.0/24 TCP All 31380  HTTP load balancer   No 10.0.1.0/24 TCP All 31390  HTTPS load balancer    Private Subnet (for example, CIDR 10.0.1.0/24)\nSecurity List / Ingress Rules\n   Stateless Destination Protocol Source Ports Destination Ports Type \u0026 Code Description     No 0.0.0.0/0 ICMP   3, 4 ICMP errors   No 10.0.0.0/16 ICMP   3 ICMP errors   No 10.0.0.0/16 TCP All 22  SSH   No 10.0.0.0/24 TCP All 30080  HTTP load balancer   No 10.0.0.0/24 TCP All 30443  HTTPS load balancer   No 10.0.0.0/24 TCP All 31380  HTTP load balancer   No 10.0.0.0/24 TCP All 31390  HTTPS load balancer   No 10.0.1.0/24UDP All 111  NFS    No 10.0.1.0/24 TCP All 111  NFS   No 10.0.1.0/24 UDP All 2048  NFS   No 10.0.1.0/24 TCP All 2048-2050  NFS   No 10.0.1.0/24 TCP All 2379-2380  Kubernetes etcd   No 10.0.1.0/24 TCP All 6443  Kubernetes API Server   No 10.0.1.0/24 TCP All 6446  MySQL   No 10.0.1.0/24 TCP All 8090-8091  OLCNE Platform Agent   No 10.0.1.0/24 UDP All 8472  Flannel   No 10.0.1.0/24 TCP All 10250-10255  Kubernetes Kublet    Security List / Egress Rules\n   Stateless Destination Protocol Source Ports Destination Ports Type and Code Description     No 10.0.0.0/0 TCP    All egress traffic    DHCP Options\n   DNS Type     Internet and VCN Resolver    Route Tables\nPublic Subnet Route Table Rules\n   Destination Target     0.0.0.0/0 Internet Gateway    Private Subnet Route Table Rules\n   Destination Target     0.0.0.0/0 NAT Gateway   All OCI Services Service Gateway    Internet Gateway\nNAT Gateway\nService Gateway\nThe following compute resources adhere to the guidelines provided in the Oracle Linux Cloud Native Environment Getting Started guide. The attributes indicated (for example, Subnet, RAM, Shape, and Image) are recommendations that have been tested. Other values can be used if required.\nCompute Instances\n   Role Subnet Suggested RAM Compatible VM Shape Compatible VM Image     SSH Jump Host Public 8GB VM.Standard.E2.1 Oracle Linux 7.8   OLCNE Operator Host Private 16GB VM.Standard.E2.2 Oracle Linux 7.8   Kubernetes Control Plane Node Private 32GB VM.Standard.E2.4 Oracle Linux 7.8   Kubernetes Worker Node 1 Private 32GB VM.Standard.E2.4 Oracle Linux 7.8   Kubernetes Worker Node 2 Private 32GB VM.Standard.E2.4 Oracle Linux 7.8   Kubernetes Worker Node 3 Private 32GB VM.Standard.E2.4 Oracle Linux 7.8    Do the OLCNE install Deploy Oracle Linux Cloud Native Environment with the Kubernetes module, following instructions from the Getting Started guide.\n Use a single Kubernetes control plane node. Skip the Kubernetes API load balancer (3.4.3). Use private CA certificates (3.5.3).  Prepare for the Verrazzano install A Verrazzano Oracle Linux Cloud Native Environment deployment requires:\n A default storage provider that supports “Multiple Read/Write” mounts. For example, an NFS service like:  Oracle Cloud Infrastructure File Storage Service. A hardware-based storage system that provides NFS capabilities.   Load balancers in front of the worker nodes in the cluster. DNS records that reference the load balancers.  Examples for meeting these requirements follow.\nPrerequisites Details  Storage Load Balancers DNS   Storage Verrazzano requires persistent storage for several components. This persistent storage is provided by a default storage class. A number of persistent storage providers exist for Kubernetes. This guide will focus on pre-allocated persistent volumes. In particular, the provided samples will illustrate the use of OCI’s NFS File System.\nOCI example Before storage can be exposed to Kubernetes, it must be created. In OCI, this is done using File System resources. Using the OCI Console, create a new File System. Within the new File System, create an Export. Remember the value used for Export Path as it will be used later. Also note the Mount Target’s IP Address for use later.\nAfter the exports have been created, referenced persistent volume folders (for example, /example/pv0001) will need to be created. In OCI, this can be done by mounting the export on one of the Kubernetes worker nodes and creating the folders. In the following example, the value /example is the Export Path and 10.0.1.8 is the Mount Target’s IP Address. The following command should be run on one of the Kubernetes worker nodes. This will result in the creation of nine persistent volume folders. The reason for nine persistent volume folders is covered in the next section.\n$ sudo mount 10.0.1.8:/example /mnt $ for x in {0001..0009}; do sudo mkdir -p /mnt/pv${x} \u0026\u0026 sudo chmod 777 /mnt/pv${x}; done Persistent Volumes A default Kubernetes storage class is required by Verrazzano. When using pre-allocated PersistentVolumes, for example NFS, persistent volumes should be declared as following. The value for name may be customized but will need to match the PersistentVolume storageClassName value later.\n Create a default StorageClass $ cat \u003c\u003c EOF | kubectl apply -f - apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: example-nfs annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer EOF  Create the required number of PersistentVolume resources. The Verrazzano system requires five persistent volumes for itself. The following command creates nine persistent volumes. The value for storageClassName must match the above StorageClass name. The values for name may be customized. The value for path must match the Export Path of the Export from above, combined with the persistent volume folder from above. The value for server must be changed to match the location of your file system server. $ for n in {0001..0009}; do cat \u003c\u003c EOF | kubectl apply -f - apiVersion: v1 kind: PersistentVolume metadata: name: pv${n} spec: storageClassName: example-nfs accessModes: - ReadWriteOnce - ReadWriteMany capacity: storage: 50Gi nfs: path: /example/pv${n} server: 10.0.1.8 volumeMode: Filesystem persistentVolumeReclaimPolicy: Recycle EOF    Load Balancers Verrazzano on Oracle Linux Cloud Native Environment uses external load balancer services. These will not automatically be provided by Verrazzano or Kubernetes. Two load balancers must be deployed outside of the subnet used for the Kubernetes cluster. One load balancer is for management traffic and the other for application traffic.\nSpecific steps will differ for each load balancer provider, but a generic configuration and an OCI example follow.\nGeneric configuration:  Target Host: Host names of Kubernetes worker nodes Target Ports: See table External Ports: See table Distribution: Round-robin Health Check: TCP     Traffic Type Service Name Type Suggested External Port Target Port     Application istio-ingressgateway TCP 80 31380   Application istio-ingressgateway TCP 443 31390   Management ingress-controller-nginx-ingress-controller TCP 80 30080   Management ingress-controller-nginx-ingress-controller TCP 443 30443    OCI example The following details can be used to create OCI load balancers for accessing application and management user interfaces, respectively. These load balancers will route HTTP/HTTPS traffic from the Internet to the private subnet. If load balancers are desired, then they should be created now even though the application and management endpoints will be installed later.\n Application Load Balancer: Public Subnet  Listeners  HTTP Listener: Protocol TCP, Port 80 HTTPS Listener: Protocol TCP, Port 443   Backend Sets  HTTP Backend Sets:  Health Check: Protocol TCP, Port 31380 Backends: Kubernetes Worker Nodes, Port 31380, Distribution Policy Weighted Round Robin   HTTPS Backend Sets  Health Check: Protocol TCP, Port 31390 Backends: Kubernetes Worker Nodes, Port 31390, Distribution Policy Weighted Round Robin       Management Load Balancer: Public Subnet  Listeners  HTTP Listener: Protocol TCP, Port 80 HTTPS Listener: Protocol TCP, Port 443   Backend Sets  HTTP Backend Sets:  Health Check: Protocol TCP, Port 30080 Backends: Kubernetes Worker Nodes, Port 30080, Distribution Policy Weighted Round Robin   HTTPS Backend Sets  Health Check: Protocol TCP, Port 30443 Backends: Kubernetes Worker Nodes, Port 30443, Distribution Policy Weighted Round Robin         DNS When using the spec.dns.external DNS type, the installer searches the DNS zone you provide for two specific A records. These are used to configure the cluster and should refer to external addresses of the load balancers in the previous step. The A records will need to be created manually.\nNOTE: At this time, the only supported deployment for Oracle Linux Cloud Native Environment is the external DNS type.\n   Record Use     ingress-mgmt Set as the .spec.externalIPs value of the ingress-controller-nginx-ingress-controller service.   ingress-verrazzano Set as the .spec.externalIPs value of the istio-ingressgateway service.    For example:\n198.51.100.10 A ingress-mgmt.myenv.mydomain.com. 203.0.113.10 A ingress-verrazzano.myenv.mydomain.com. Verrazzano installation will result in a number of management services that need to point to the ingress-mgmt address.\nkeycloak.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. rancher.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. grafana.vmi.system.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. prometheus.vmi.system.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. kibana.vmi.system.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. elasticsearch.vmi.system.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. For simplicity, an administrator may want to create wildcard DNS records for the management addresses:\n*.system.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. OR\n*.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. OCI example DNS is configured in OCI by creating DNS zones in the OCI Console. When creating a DNS zone, use these values:\n Method: Manual Zone Name: \u003cdns-suffix\u003e Zone Type: Primary  The value for \u003cdns-suffix\u003e excludes the environment (for example, use the mydomain.com portion of myenv.mydomain.com).\nDNS A records must be manually added to the zone and published using values described above. DNS CNAME records, in the same way.\n  During the Verrazzano install, these steps should be performed on the Oracle Linux Cloud Native Environment operator node.\nEdit the sample Verrazzano custom resource install-olcne.yaml file and provide these configuration settings for your OLCNE environment:\n The value for spec.environmentName is a unique DNS subdomain for the cluster (for example, myenv in myenv.mydomain.com). The value for spec.dns.external.suffix is the remainder of the DNS domain (for example, mydomain.com in myenv.mydomain.com). Under spec.ingress.verrazzano.nginxInstallArgs, the value for controller.service.externalIPs is the IP address of ingress-mgmt.\u003cmyenv\u003e.\u003cmydomain.com\u003e configured during DNS set up. Under spec.ingress.application.istioInstallArgs, the value for gateways.istio-ingressgateway.externalIPs is the IP address of ingress-verrazzano.\u003cmyenv\u003e.\u003cmydomain.com\u003e configured during DNS set up.  You will install Verrazzano using the external DNS type (the example custom resource for OLCNE is already configured to use spec.dns.external).\nSet the following environment variable:\nThe value for \u003cpath to valid Kubernetes config\u003e is typically ${HOME}/.kube/config.\n$ export KUBECONFIG=$VERRAZZANO_KUBECONFIG Next steps To continue, see the Installation Guide.\n","excerpt":"Prepare for the OCLNE install Oracle Linux Cloud Native Environment can be installed in several different types of environments. These range from physical, on-premises hardware to virtualized cloud …","ref":"/docs/setup/platforms/olcne/olcne/","title":"Oracle Linux Cloud Native Environment (OLCNE)"},{"body":"Verrazzano supports Kubernetes Role-Based Access Control (RBAC) for Verrazzano resources, and integrates with Keycloak to enable Single Sign-On (SSO) across the Verrazzano console and the Verrazzano Monitoring Instance (VMI) logging and metrics consoles. Verrazzano provides proxies that enable SSO and Kubernetes API access for Keycloak user accounts.\nFor information on how Verrazzano secures network traffic, see Network Security.\n","excerpt":"Verrazzano supports Kubernetes Role-Based Access Control (RBAC) for Verrazzano resources, and integrates with Keycloak to enable Single Sign-On (SSO) across the Verrazzano console and the Verrazzano …","ref":"/docs/security/","title":"Security"},{"body":"Verrazzano installs a curated set of open source components. The following table lists each open source component with its version and a brief description.\n   Component Version Description     cert-manager 1.2.0 Automates the management and issuance of TLS certificates.   Coherence Operator 3.1.5 Assists with deploying and managing Coherence clusters.   Elasticsearch 7.6.1 Provides a distributed, multitenant-capable full-text search engine.   ExternalDNS 0.7.1 Synchronizes exposed Kubernetes Services and ingresses with DNS providers.   Grafana 6.4.4 Tool to help you study, analyze, and monitor metrics.   Istio 1.7.3 Service mesh that layers transparently onto existing distributed applications.   Keycloak 10.0.1 Provides single sign-on with Identity and Access Management.   Kibana 7.6.1 Provides search and data visualization capabilities for data indexed in Elasticsearch.   MySQL 8.0.20 Open source relational database management system used by Keycloak.   NGINX Ingress Controller 0.46.0 Traffic management solution for cloud‑native applications in Kubernetes.   Node Exporter 1.0.0 Prometheus exporter for hardware and OS metrics.   OAM Kubernetes Runtime 0.3.0 Plug-in for implementing Open Application Model (OAM) control plane with Kubernetes.   Prometheus 2.13.1 Provides event monitoring and alerting.   Rancher 2.5.7 Manages multiple Kubernetes clusters.   WebLogic Kubernetes Operator 3.2.5 Assists with deploying and managing WebLogic domains.    ","excerpt":"Verrazzano installs a curated set of open source components. The following table lists each open source component with its version and a brief description.\n   Component Version Description …","ref":"/docs/reference/versions/","title":"Software Versions"},{"body":"Verrazzano provides two proxies that enable authentication and authorization for Keycloak users accessing Verrazzano resources. The proxies are automatically configured and deployed.\nVerrazzano API proxy The API proxy is used to authenticate and authorize Keycloak users, then impersonate them to the Kubernetes API, so that Keycloak users can access Kubernetes resources.\nThe Verrazzano API proxy is used primarily by the Verrazzano console. The console authenticates users against Keycloak, using the PKCE flow, obtains a bearer token, and then sends the token to the API along with the Kubernetes API request. The API proxy validates the token and, if valid, impersonates the user to the Kubernetes API server. This allows the console to run Kubernetes API calls on behalf of Keycloak users, with Kubernetes enforcing role-based access control (RBAC) based on the impersonated identity.\nIn multicluster scenarios, the console directs all Kubernetes requests to the admin cluster’s API proxy. If a request refers to a resource in a different cluster, the API proxy forwards the request, along with the user’s authentication token, to the API proxy running in the remote cluster.\nVerrazzano OpenID Connect (OIDC) proxy The OIDC proxy provides Single Sign-On (SSO) across the Verrazzano console and the Verrazzano Monitoring Instance (VMI) logging and metrics consoles. The OIDC proxy is deployed as a sidecar in Kubernetes pods that host VMI consoles. When an unauthenticated request is received by the proxy, it runs the OIDC PKCE authentication flow to obtain tokens for the user. If the user is already authenticated to Keycloak (because they have already accessed either the Verrazzano console or another VMI component), Keycloak returns tokens based on the existing user session, and the process is transparent to the user. If not, Keycloak will authenticate the user, establishing a session, before returning tokens.\n","excerpt":"Verrazzano provides two proxies that enable authentication and authorization for Keycloak users accessing Verrazzano resources. The proxies are automatically configured and deployed.\nVerrazzano API …","ref":"/docs/security/proxies/proxies/","title":"Verrazzano Proxies"},{"body":"During installation, Verrazzano generates several default accounts.\n   System Account Secret Secret Namespace Description     Keycloak keycloakadmin keycloak-http keycloak Keycloak root user: full administrative privileges for Keycloak.   Keycloak verrazzano verrazzano verrazzano-system Verrazzano root user: can manage the verrazzano-system realm in Keycloak, including managing users in that realm. This user is a member of the verrazzano-admins group, and, if default role bindings are used, has the verrazzano-admin role.   Ranger admin rancher-admin-secret cattle-system Rancher root user: full administrative privileges for Rancher.    ","excerpt":"During installation, Verrazzano generates several default accounts.\n   System Account Secret Secret Namespace Description     Keycloak keycloakadmin keycloak-http keycloak Keycloak root user: full …","ref":"/docs/security/accounts/accounts/","title":"Default User Accounts"},{"body":"KIND is a tool for running local Kubernetes clusters using Docker container “nodes”. Follow these instructions to prepare a KIND cluster for running Verrazzano.\nNOTE KIND is not recommended for use on macOS and Windows because the Docker network is not directly exposed to the host.  Prerequisites  Install Docker Install KIND  Prepare the KIND cluster To prepare the KIND cluster for use with Verrazzano, you must create the cluster and then install and configure MetalLB in that cluster.\nYou can create the KIND cluster in two ways: with or without image caching; image caching can speed up your installation time.\nCreate a KIND cluster KIND images are prebuilt for each release. To find images suitable for a given release, check the release notes for your KIND version (check with kind version). There you’ll find a complete listing of images created for a KIND release.\nThe following example references a Kubernetes v1.18.8-based image built for KIND v0.9.0. Replace that image with one suitable for the KIND release you are using.\n$ kind create cluster --config - \u003c\u003cEOF kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane image: kindest/node:v1.18.8@sha256:f4bcc97a0ad6e7abaf3f643d890add7efe6ee4ab90baeb374b4f41a4c95567eb kubeadmConfigPatches: - | kind: ClusterConfiguration apiServer: extraArgs: \"service-account-issuer\": \"kubernetes.default.svc\" \"service-account-signing-key-file\": \"/etc/kubernetes/pki/sa.key\" EOF Create a KIND cluster with image caching While developing or experimenting with Verrazzano, you might destroy and re-create your KIND cluster multiple times. To speed up Verrazzano installation, follow these steps to ensure that the image cache used by containerd inside a KIND cluster, is preserved across clusters. Subsequent installations will be faster because they will not need to pull the images again.\n1. Create a named Docker volume that will be used for the image cache, and note its Mountpoint path. In this example, the volume is named containerd.\n$ docker volume create containerd $ docker volume inspect containerd #Sample output is shown { \"CreatedAt\": \"2021-01-11T16:27:47Z\", \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/containerd/_data\", \"Name\": \"containerd\", \"Options\": {}, \"Scope\": \"local\" } 2. Specify the Mountpoint path obtained, as the hostPath under extraMounts in your KIND configuration file, with a containerPath of /var/lib/containerd, which is the default containerd image caching location inside the KIND container. An example of the modified KIND configuration is shown in the following create cluster command:\n$ kind create cluster --config - \u003c\u003cEOF kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane image: kindest/node:v1.18.8@sha256:f4bcc97a0ad6e7abaf3f643d890add7efe6ee4ab90baeb374b4f41a4c95567eb kubeadmConfigPatches: - | kind: ClusterConfiguration apiServer: extraArgs: \"service-account-issuer\": \"kubernetes.default.svc\" \"service-account-signing-key-file\": \"/etc/kubernetes/pki/sa.key\" extraMounts: - hostPath: /var/lib/docker/volumes/containerd/_data containerPath: /var/lib/containerd #This is the location of the image cache inside the KIND container EOF Install and configure MetalLB By default, KIND does not provide an implementation of network load balancers (Services of type LoadBalancer). MetalLB offers a network load balancer implementation.\nTo install MetalLB:\n$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/namespace.yaml $ kubectl create secret generic \\  -n metallb-system memberlist \\  --from-literal=secretkey=\"$(openssl rand -base64 128)\" $ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/metallb.yaml For further details, see the MetalLB installation guide.\nMetalLB is idle until configured. Configure MetalLB in Layer 2 mode and give it control over a range of IP addresses in the kind Docker network. In versions v0.7.0 and earlier, KIND uses Docker’s default bridge network; in versions v0.8.0 and later, it creates its own bridge network in KIND.\nTo determine the subnet of the kind Docker network in KIND v0.8.0 and later:\n$ docker inspect kind | jq '.[0].IPAM.Config[0].Subnet' -r 172.18.0.0/16 To determine the subnet of the kind Docker network in KIND v0.7.0 and earlier:\n$ docker inspect bridge | jq '.[0].IPAM.Config[0].Subnet' -r 172.17.0.0/16 For use by MetalLB, assign a range of IP addresses at the end of the kind network’s subnet CIDR range.\n$ kubectl apply -f - \u003c\u003c-EOF apiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: my-ip-space protocol: layer2 addresses: - 172.18.0.230-172.18.0.250 EOF Next steps To continue, see the Installation Guide.\n","excerpt":"KIND is a tool for running local Kubernetes clusters using Docker container “nodes”. Follow these instructions to prepare a KIND cluster for running Verrazzano.\nNOTE KIND is not recommended for use on …","ref":"/docs/setup/platforms/kind/kind/","title":"KIND"},{"body":"   Example Description     hello-helidon Hello World Helidon example deployed to a multicluster environment. The example also demonstrates how to change the placement of the application to a different cluster.   sock-shop Implementation of the Sock Shop Microservices Demo Application deployed to a multicluster environment.   todo-list ToDo List is an example application containing a WebLogic component deployed to a multicluster environment.    Prerequisites Prior to running the multicluster examples, complete the multicluster installation and managed cluster registration documented here.\n","excerpt":"   Example Description     hello-helidon Hello World Helidon example deployed to a multicluster environment. The example also demonstrates how to change the placement of the application to a different …","ref":"/docs/samples/multicluster/","title":"Multicluster"},{"body":"The Verrazzano metrics stack automates metrics aggregation and consists of Prometheus and Grafana components. Metrics sources expose system and application metrics. The Prometheus components retrieve and store the metrics and Grafana provides dashboards to visualize them.\nMetrics sources Metrics sources produce metrics and expose them to the Kubernetes Prometheus system using annotations in the pods. The metrics annotations may differ slightly depending on the resource type. The following is an example of the WebLogic Prometheus-related configuration specified in the todo-list application pod:\n$ kubectl describe pod tododomain-adminserver -n todo-list\nAnnotations: prometheus.io/path: /wls-exporter/metrics prometheus.io/port: 7001 prometheus.io/scrape: true For other resource types, such as Coherence or Helidon, the annotations would look similar to this:\nAnnotations: verrazzano.io/metricsEnabled: true verrazzano.io/metricsPath: /metrics verrazzano.io/metricsPort: 8080 To look directly at the metrics that are being made available by the metric source, map the port and then access the path.\nFor example, for the previous metric source:\n  Map the port being used to expose the metrics.\n$ kubectl port-forward tododomain-adminserver 7001:7001 -n todo-list   Get the user name and password used to access the metrics source from the corresponding secret.\n$ kubectl get secret \\ --namespace todo-list tododomain-weblogic-credentials \\ -o jsonpath={.data.username} | base64 \\ --decode; echo $ kubectl get secret \\ --namespace todo-list tododomain-weblogic-credentials \\ -o jsonpath={.data.password} | base64 \\ --decode; echo   Access the metrics at the exported path, using the user name and password retrieved in the previous step.\n$ curl -u USERNAME:PASSWORD localhost:7001/wls-exporter/metrics   Metrics server  Single pod per cluster. Named vmi-system-prometheus-* in verrazzano-system namespace. Discovers exposed metrics source endpoints. Scrapes metrics from metrics sources. Responsible for exposing all metrics.  Grafana Grafana provides visualization for your Prometheus metric data.\n Single pod per cluster. Named vmi-system-grafana-* in verrazzano-system namespace. Provides dashboards for metrics visualization.  To access Grafana:\n  Get the hostname from the Grafana ingress.\n$ kubectl get ingress vmi-system-grafana -n verrazzano-system NAME CLASS HOSTS ADDRESS PORTS AGE vmi-system-grafana \u003cnone\u003e grafana.vmi.system.default.123.456.789.10.nip.io 123.456.789.10 80, 443 26h   Get the password for the user verrazzano.\n$ kubectl get secret \\ --namespace verrazzano-system verrazzano \\ -o jsonpath={.data.password} | base64 \\ --decode; echo   Access Grafana in a browser using the previous hostname.\n  Log in using the verrazzano user and the previous password.\n  From here, you can select an existing dashboard or create a new dashboard. To select an existing dashboard, use the drop-down list in the top left corner. The initial value of this list is Home.\nTo view host level metrics, select Host Metrics. This will provide system metrics for all of the nodes in your cluster.\nTo view the application metrics for the todo-list example application, select WebLogic Server Dashboard because the todo-list application is a WebLogic application.\n","excerpt":"The Verrazzano metrics stack automates metrics aggregation and consists of Prometheus and Grafana components. Metrics sources expose system and application metrics. The Prometheus components retrieve …","ref":"/docs/monitoring/metrics/metrics/","title":"Metrics"},{"body":"","excerpt":"","ref":"/docs/monitoring/","title":"Monitoring \u0026 Logging"},{"body":"Verrazzano provides the following support.\nKeycloak Applications can use the Verrazzano Keycloak server as an Identity Provider. Keycloak supports SAML 2.0 and OpenID Connect (OIDC) authentication and authorization flows. Verrazzano does not provide any explicit integrations for applications.\nNOTE If using Keycloak for application authentication and authorization, create a new realm to contain application users and clients. Do not use the verrazzano-system realm, or the default (Keycloak system) realm. The Keycloak root user account (keycloakadmin) has privileges to create realms.  Network security Verrazzano uses Istio to authenticate and authorize incoming network connections for applications. Verrazzano also provides support for configuring Kubernetes NetworkPolicy on Verrazzano Projects. NetworkPolicy rules control where network connections can be made.\nNOTE Enforcement of NetworkPolicy requires that a Kubernetes CNI provider, such as Calico, be configured for the cluster.  For more information on how Verrazzano secures network traffic, see Network Security.\n","excerpt":"Verrazzano provides the following support.\nKeycloak Applications can use the Verrazzano Keycloak server as an Identity Provider. Keycloak supports SAML 2.0 and OpenID Connect (OIDC) authentication and …","ref":"/docs/security/appsec/appsec/","title":"Application Security"},{"body":"Prepare for the generic install If your generic Kubernetes implementation provides a load balancer implementation, then you can use a default configuration of the Verrazzano custom resource with no customizations and follow the Installation Guide.\nOtherwise, you can install a load balancer, such as MetalLB. For details, see Install and configure MetalLB.\nCustomizations Verrazzano is highly customizable. If your Kubernetes implementation requires a custom configuration, see Customize Installations.\nNext steps To continue, see the Installation Guide.\n","excerpt":"Prepare for the generic install If your generic Kubernetes implementation provides a load balancer implementation, then you can use a default configuration of the Verrazzano custom resource with no …","ref":"/docs/setup/platforms/generic/generic/","title":"Generic Kubernetes"},{"body":"The following components allow persistent storage:\n Elasticsearch Prometheus Grafana Keycloak/MySQL  By default, each Verrazzano installation profile has different storage characteristics. The dev profile uses ephemeral storage only, but in all other profiles, each of the listed components use persistent storage. For more information, see Profile Configurations.\nNOTE Ephemeral storage is not recommended for use in production; Kubernetes pods can be restarted at any time, leading to a loss of data and system instability if non-persistent storage is used. Persistent storage is recommended for all use cases beyond evaluation or development.  While each profile has its own default persistence settings, in each case you have the option to override the profile defaults to customize your persistence settings.\nCustomize persistent storage The following components can use persistent storage:\n Elasticsearch Kibana Prometheus Grafana Keycloak  You can customize the persistence settings for these components through the VerrazzanoSpec, as follows:\n Overriding the persistence settings for all components (Keycloak, Grafana, Prometheus, Elasticsearch, and Kibana) by using the defaultVolumeSource field. Overriding the persistence settings for Keycloak by using the volumeSource field on that component’s configuration.  You can set the global defaultVolumeSource and component-level volumeSource fields to one of the following values:\n   Value Storage     emptyDir Ephemeral storage; should not be used for production scenarios.   persistentVolumeClaim A PersistentVolumeClaimVolumeSource where the claimSource field references a named volumeClaimSpecTemplate.    When you want to use a persistentVolumeClaim to override the storage settings for components, you must do the following:\n Create a volumeClaimSpecTemplate which identifies the desired persistence settings. Configure a persistentVolumeClaim for the component where the claimName field references the template you created previously.  This lets you create named persistence settings that can be shared across multiple components within a Verrazzano configuration. Note that the existence of a persistence template in the volumeClaimSpecTemplates list does not directly result in the creation of a persistent volume, or affect any component storage settings until it is referenced by either defaultVolumeSource or a specific component’s volumeSource.\nExamples Review the following customizing persistent storage examples:\n Customize persistence globally using defaultVolumeSource Customize PersistentVolumeClaim settings for Keycloak using volumeSource Use global and local persistence settings together  Customize persistence globally using defaultVolumeSource If defaultVolumeSource is configured, then that setting will be used for all components that require storage.\nFor example, the following Verrazzano configuration uses the prod profile, but disables persistent storage for all components:\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: no-storage-prod spec: profile: prod defaultVolumeSource: emptyDir: {} The following example uses persistentVolumeClaim to override persistence settings globally for a prod profile, to use 100Gi volumes for all components, instead of the default of 50Gi:\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: prod-global-override spec: profile: prod defaultVolumeSource: persistentVolumeClaim: claimName: globalOverride volumeClaimSpecTemplates: - metadata: name: globalOverride spec: resources: requests: storage: 100Gi The following example uses a managed-cluster profile but overrides the persistence settings to use ephemeral storage:\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: mgdcluster-empty-storage-example spec: profile: managed-cluster defaultVolumeSource: emptyDir: {} # Use emphemeral storage for all Components unless overridden Customize PersistentVolumeClaim settings for Keycloak using volumeSource The following example Verrazzano configuration enables a 100Gi PersistentVolumeClaim for the MySQL component in Keycloak in a dev profile configuration. This overrides the default of ephemeral storage for Keycloak in that profile, while retaining the default storage settings for other components:\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: dev-mysql-storage-example spec: profile: dev components: keycloak: mysql: volumeSource: persistentVolumeClaim: claimName: mysql # Use the \"mysql\" PVC template for the MySQL volume configuration volumeClaimSpecTemplates: - metadata: name: mysql spec: resources: requests: storage: 100Gi Use global and local persistence settings together The following example uses a dev installation profile, but overrides the profile persistence settings to:\n Use 200Gi volumes for all components by default. Use a 100Gi volume for the MySQL instance associated with Keycloak.  apiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: dev-storage-example spec: profile: dev defaultVolumeSource: persistentVolumeClaim: claimName: vmi # set storage globally for the metrics stack components: keycloak: mysql: volumeSource: persistentVolumeClaim: claimName: mysql # set storage separately for keycloak's MySql instance volumeClaimSpecTemplates: - metadata: name: mysql spec: resources: requests: storage: 100Gi - metadata: name: vmi spec: resources: requests: storage: 200Gi ","excerpt":"The following components allow persistent storage:\n Elasticsearch Prometheus Grafana Keycloak/MySQL  By default, each Verrazzano installation profile has different storage characteristics. The dev …","ref":"/docs/setup/install/customizing/storage/","title":"Customize Persistent Storage"},{"body":"","excerpt":"","ref":"/docs/troubleshooting/","title":"Troubleshooting"},{"body":"","excerpt":"","ref":"/docs/samples/","title":"Examples"},{"body":"Using the Verrazzano Console, you can:\n Find general information about your Verrazzano environment, such as Status and Version. Access the installed Rancher and Keycloak consoles. Access Kibana, Grafana, Prometheus, and Elasticsearch telemetry data. View applications, projects, and Components resources. In a multicluster environment, view clusters. Use the breadcrumbs for quick and easy navigation. Use the Resources navigation to browse child resources associated with the Resource being displayed. Sort (order) and filter data according to certain attributes.  Selecting an application, project, or Component will open a detailed view of the associated child resources. In the central metadata section, you’ll find detailed information about the Verrazzano resource being displayed.\n","excerpt":"Using the Verrazzano Console, you can:\n Find general information about your Verrazzano environment, such as Status and Version. Access the installed Rancher and Keycloak consoles. Access Kibana, …","ref":"/docs/operations/console/","title":"Verrazzano Console"},{"body":"","excerpt":"","ref":"/docs/reference/","title":"Reference"},{"body":"v0.17.0 Features:\n Allow Verrazzano Monitoring Instance (VMI) replicas and memory sizes to be changed during installation for both dev and prod profiles. When installing Verrazzano on OKE, the OKE-specific Fluentd extraVolumeMounts configuration is no longer required. Updated to the v3.2.5 WebLogic Kubernetes Operator.  Fixes:\n During uninstall, delete application resources only from namespaces which are managed by Verrazzano. During upgrade, honor the APP_OPERATOR_IMAGE override. Fixed Keycloak installation failure when Prometheus is disabled. Allow empty values for Helm overrides in config.json.  v0.16.0 Features:\n Provided options to configure log volume/mount of the log collector, Fluentd, and pre-configured profiles. Automatically enabled metrics and log capture for WebLogic domains deployed in Verrazzano. Added security-related data/project YAML files to the Verrazzano console, under project details. Updated to the v3.2.4 WebLogic Kubernetes Operator.  Fixes:\n Added a fix for default metrics traits not always being injected into the appconfig. Updated the timestamp in WebLogic application logs so that the time filter can be used in Kibana. Corrected the incorrect podSelector in the node exporter network policy. Fixed the DNS resolution issue due to the missing cluster section of the coredns configmap. Stability improvements for the platform, tests, and examples. Renamed the Elasticsearch fields in a multicluster registration secret to be consistent.  v0.15.1 Features:\n Allow customization of Elasticsearch node sizes and topology during installation. If runtimeEncryptionSecret, specified in the WebLogic domain spec, does not already exist, then create it. Support overrides of persistent storage configuration for Elasticsearch, Kibana, Prometheus, Grafana, and Keycloak.  Known Issues:\n After upgrade to 0.15.1, for Verrazzano Custom Resource installed on OCI Container Engine for Kubernetes (OKE), the Fluentd DaemonSet in the verrazzano-system namespace cannot access logs. Run following command to patch the Fluentd DaemonSet and correct the issue: kubectl patch -n verrazzano-system ds fluentd --patch '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\": \"fluentd\",\"volumeMounts\":[{\"mountPath\":\"/u01/data/\",\"name\":\"extravol0\",\"readOnly\":true}]}],\"volumes\":[{\"hostPath\":{\"path\":\"/u01/data/\",\"type\":\"\"},\"name\":\"extravol0\"}]}}}}'   v0.15.0 Features:\n Support for private container registries. Secured communication between Verrazzano resources using Istio. Updated to the following versions:  cert-manager to 1.2.0. Coherence Operator to 3.1.5. WebLogic Kubernetes Operator to 3.2.3. Node Exporter to 1.0.0. NGINX Ingress Controller to 0.46. Fluentd to 1.12.3.   Added network policies for Istio.  Fixes:\n Stability improvements for the platform, tests, and examples. Several fixes for scraping Prometheus metrics. Several fixes for logging and Elasticsearch. Replaced keycloak.json with dynamic realm creation. Removed the LoggingScope CRD from the Verrazzano API. Fixed issues related to multicluster resources being orphaned.  v0.14.0 Features:\n Multicluster support for Verrazzano. Now you can:  Register participating clusters as VerrazzanoManagedClusters. Deploy MutiClusterComponents and MultiClusterApplicationConfigurations. Organize multicluster namespaces as VerrazzanoProjects. Access MultiCluster Components and ApplicationConfigurations in the Verrazzano Console UI.   Changed default wildcard DNS from xip.io to nip.io. Support for OKE clusters with private endpoints. Support for network policies. Now you can:  Add ingress-NGINX network policies. Add Rancher network policies. Add NetworkPolicy support to Verrazzano projects. Add network policies for Keycloak. Add platform operator network policies. Add network policies for Elasticsearch and Kibana. Set network policies for Verrazzano operators, console, and API proxy. Add network policies for WebLogic Kubernetes Operator.   Changes to allow magic DNS provider to be specified (xip.io, nip.io, sslip.io). Support service setup for multiple containers. Enabled use of self-signed certs with OCI DNS. Support for setting DeploymentStrategy for VerrazzanoHelidonWorkload.  Fixes:\n Several stability improvements for the platform, tests, and examples. Added retries around lookup of Rancher admin user. Granted specific privileges instead of ALL for Keycloak user in MySQL. Disabled the installation of the Verrazzano Console UI on managed clusters.  v0.13.0 Features:\n IngressTrait support for explicit destination host and port. Experimental cluster diagnostic tooling. Grafana dashboards for VerrazzanoHelidonWorkload. Now you can update application Fluentd sidecar images following a Verrazzano update. Documented Verrazzano specific OAM workload resources. Documented Verrazzano hardware requirements and installed software versions.  Fixes:\n VerrazzanoWebLogicWorkload and VerrazzanoCoherenceWorkload resources now handle updates. Now VerrazzanoHelidonWorkload supports the use of the ManualScalarTrait. Now you can delete a Namespace containing an ApplicationConfiguration resource. Fixed frequent restarts of Prometheus during application deployment. Made verrazzano-application-operator logging more useful and use structured logging. Fixed Verrazzano uninstall issues.  v0.12.0 Features:\n Observability stack now uses Keycloak SSO for authentication. Istio sidecars now automatically injected when namespaces labeled istio-injection=enabled. Support for Helidon applications now defined using VerrazzanoHelidonWorkload type.  Fixes:\n Fixed issues where logs were not captured from all containers in workloads with multiple containers. Fixed issue where some resources were not cleaned up during uninstall.  v0.11.0 Features:\n OAM applications are optionally deployed into an Istio service mesh. Incremental improvements to user-facing roles.  Fixes:\n Fixed issue with logging when an application has multiple workload types. Fixed metrics configuration in Spring Boot example application.  v0.10.0 Breaking Changes:\n Model/binding files removed; now application deployment done exclusively by using Open Application Model (OAM). Syntax changes for WebLogic and Coherence OAM workloads, now defined using VerrazzanoCoherenceWorkload and VerrazzanoWebLogicWorkload types.  Features:\n By default, application endpoints now use HTTPs - when using magic DNS, certificates are issued by cluster issuer, when using OCI DNS certificates are issued using Let’s Encrypt, or the end user can provide certificates. Updated Coherence operator to 3.1.3. Updates for running Verrazzano on Kubernetes 1.19 and 1.20. RBAC roles and role bindings created at install time. Added instance information to status of Verrazzano custom resource; can be used to obtain instance URLs. Upgraded Istio to v1.7.3.  Fixes:\n Reduced log level of Elasticsearch; excessive logging could have resulted in filling up disks.  v0.9.0  Features:  Added platform support for installing Verrazzano on Kind clusters. Log records are indexed from the OAM appconfig and component definitions using the following pattern: namespace-appconfig-component. All system and curated components are now patchable. More updates to Open Application Model (OAM) support.    To enable OAM, when you install Verrazzano, specify the following in the Kubernetes manifest file for the Verrazzano custom resource:\nspec: oam: enabled: true v0.8.0  Features:  Support for two installation profiles, development (dev) and production (prod). The production profile, which is the default, provides a 3-node Elasticsearch and persistent storage for the Verrazzano Monitoring Instance (VMI). The development profile provides a single node Elasticsearch and no persistent storage for the VMI. The default behavior has been changed to use the system VMI for all monitoring (applications and Verrazzano components). It is still possible to customize one of the profiles to enable the original, non-shared VMI mode. Initial support for the Open Application Model (OAM).   Fixes:  Updated Axios NPM package to v0.21.1 to resolve a security vulnerability in the examples code.    v.0.7.0   Features:\n Ability to upgrade an existing Verrazzano installation. Added the Verrazzano Console. Enhanced the structure of the Verrazzano custom resource to allow more configurability. Streamlined the secret usage for OCI DNS installations.    Fixes:\n Fixed bug where the Verrazzano CR Certificate.CA fields were being ignored. Removed secret used for hello-world; hello-world-application image is now public in ghcr so ImagePullSecrets is no longer needed. Fixed issue #339 (PRs #208 \u0026 #210.)    v0.6.0  Features:  In-cluster installer which replaces client-side install scripts. Added installation profiles; in this release, there are two: production and development. Verrazzano system components now emit JSON structured logs.   Fixes:  Updated Elasticsearch and Kibana versions (elasticsearch:7.6.1-20201130145440-5c76ab1) and (kibana:7.6.1-20201130145840-7717e73).    ","excerpt":"v0.17.0 Features:\n Allow Verrazzano Monitoring Instance (VMI) replicas and memory sizes to be changed during installation for both dev and prod profiles. When installing Verrazzano on OKE, the …","ref":"/docs/releasenotes/","title":"Release Notes"},{"body":"Verrazzano is an end-to-end enterprise container platform for deploying cloud-native and traditional applications in multicloud and hybrid environments. It is made up of a curated set of open source components – many that you may already use and trust, and some that were written specifically to pull together all of the pieces that make Verrazzano a cohesive and easy to use platform.\nVerrazzano includes the following capabilities:\n Hybrid and multicluster workload management Special handling for WebLogic, Coherence, and Helidon applications Multicluster infrastructure management Integrated and pre-wired application monitoring Integrated security DevOps and GitOps enablement  NOTE This is a developer preview release of Verrazzano. You should install Verrazzano only in a Kubernetes cluster that can be safely deleted when your evaluation is complete.  Select Quick Start to get started.\nVerrazzano release versions and source code are available at https://github.com/verrazzano/verrazzano. This repository contains a Kubernetes operator for installing Verrazzano and example applications for use with Verrazzano.\n","excerpt":"Verrazzano is an end-to-end enterprise container platform for deploying cloud-native and traditional applications in multicloud and hybrid environments. It is made up of a curated set of open source …","ref":"/docs/","title":"Welcome to Verrazzano"},{"body":"Hello Config World Helidon This example is a Helidon-based service that returns a “HelloConfig World” response when invoked. The application configuration uses a Kubernetes ConfigMap, instead of the default, microprofile properties file.\nBefore you begin Install Verrazzano by following the installation instructions.\nNOTE: The Hello World Helidon configuration example application deployment files are contained in the Verrazzano project located at \u003cVERRAZZANO_HOME\u003e/examples/helidon-config, where \u003cVERRAZZANO_HOME\u003e is the root of the Verrazzano project.\nDeploy the Hello Config World Helidon application   Create a namespace for the application and add a label identifying the namespace as managed by Verrazzano.\n$ kubectl create namespace helidon-config $ kubectl label namespace helidon-config verrazzano-managed=true istio-injection=enabled   To deploy the application, apply the helidon-config OAM resources.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/helidon-config/helidon-config-comp.yaml $ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/helidon-config/helidon-config-app.yaml   Wait for the application to be ready.\n$ kubectl wait \\ --for=condition=Ready pods \\ --all -n helidon-config \\ --timeout=300s   Explore the application The Hello World Helidon configuration example implements a REST API endpoint, /config, which returns a message {\"message\":\"HelloConfig World!\"} when invoked.\nNOTE: The following instructions assume that you are using a Kubernetes environment such as OKE. Other environments or deployments may require alternative mechanisms for retrieving addresses, ports, and such.\nFollow these steps to test the endpoints:\n  Get the generated host name for the application.\n$ HOST=$(kubectl get gateway helidon-config-helidon-config-appconf-gw \\ -n helidon-config \\ -o jsonpath={.spec.servers[0].hosts[0]}) $ echo $HOST helidon-config-appconf.helidon-config.11.22.33.44.nip.io   Get the EXTERNAL_IP address of the istio-ingressgateway service.\n$ ADDRESS=$(kubectl get service \\ -n istio-system istio-ingressgateway \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}') $ echo $ADDRESS 11.22.33.44   Access the application:\n  Using the command line\n$ curl -sk \\ -X GET \\ https://${HOST}/config \\ --resolve ${HOST}:443:${ADDRESS} {\"message\":\"HelloConfig World!\"} If you are using nip.io, then you do not need to include --resolve.\n  Local testing with a browser\nTemporarily, modify the /etc/hosts file (on Mac or Linux) or c:\\Windows\\System32\\Drivers\\etc\\hosts file (on Windows 10), to add an entry mapping the host name to the ingress gateway’s EXTERNAL-IP address. For example:\n11.22.33.44 helidon-config.example.com Then you can access the application in a browser at https://\u003chost\u003e/config.\n  Using your own DNS name\n Point your own DNS name to the ingress gateway’s EXTERNAL-IP address. In this case, you would need to edit the helidon-config-app.yaml file to use the appropriate value under the hosts section (such as yourhost.your.domain), before deploying the helidon-config application. Then, you can use a browser to access the application at https://\u003cyourhost.your.domain\u003e/config.      A variety of endpoints associated with the deployed application, are available to further explore the logs, metrics, and such.\nAccessing them may require the following:\n  Run this command to get the password that was generated for the telemetry components:\n$ kubectl get secret \\ --namespace verrazzano-system verrazzano \\ -o jsonpath={.data.password} | base64 \\ --decode; echo The associated user name is verrazzano.\n  You will have to accept the certificates associated with the endpoints.\nYou can retrieve the list of available ingresses with following command:\n$ kubectl get ing -n verrazzano-system NAME CLASS HOSTS ADDRESS PORTS AGE verrazzano-console-ingress \u003cnone\u003e verrazzano.default.140.238.94.217.nip.io 140.238.94.217 80, 443 7d2h vmi-system-api \u003cnone\u003e api.vmi.system.default.140.238.94.217.nip.io 140.238.94.217 80, 443 7d2h vmi-system-es-ingest \u003cnone\u003e elasticsearch.vmi.system.default.140.238.94.217.nip.io 140.238.94.217 80, 443 7d2h vmi-system-grafana \u003cnone\u003e grafana.vmi.system.default.140.238.94.217.nip.io 140.238.94.217 80, 443 7d2h vmi-system-kibana \u003cnone\u003e kibana.vmi.system.default.140.238.94.217.nip.io 140.238.94.217 80, 443 7d2h vmi-system-prometheus \u003cnone\u003e prometheus.vmi.system.default.140.238.94.217.nip.io 140.238.94.217 80, 443 7d2h Using the ingress host information, some of the endpoints available are:\n   Description Address Credentials     Kibana https://[vmi-system-kibana ingress host] verrazzano/telemetry-password   Grafana https://[vmi-system-grafana ingress host] verrazzano/telemetry-password   Prometheus https://[vmi-system-prometheus ingress host] verrazzano/telemetry-password        Troubleshooting   Verify that the application configuration, domain, and ingress trait all exist.\n$ kubectl get ApplicationConfiguration -n helidon-config $ kubectl get IngressTrait -n helidon-config   Verify that the helidon-config service pods are successfully created and transition to the READY state. Note that this may take a few minutes and that you may see some of the services terminate and restart.\n $ kubectl get pods -n helidon-config NAME READY STATUS RESTARTS AGE helidon-config-deployment-676d97c7d4-wkrj2 3/3 Running 0 5m39s   ","excerpt":"Hello Config World Helidon This example is a Helidon-based service that returns a “HelloConfig World” response when invoked. The application configuration uses a Kubernetes ConfigMap, instead of the …","ref":"/docs/samples/helidon-config/","title":""},{"body":"Hello World Helidon This example is a Helidon-based service that returns a “Hello World” response when invoked. The application configuration uses the default, microprofile properties file.\nBefore you begin Install Verrazzano by following the installation instructions.\nNOTE: The Hello World Helidon example application deployment files are contained in the Verrazzano project located at \u003cVERRAZZANO_HOME\u003e/examples/hello-helidon, where \u003cVERRAZZANO_HOME\u003e is the root of the Verrazzano project.\nDeploy the Hello World Helidon application   Create a namespace for the application and add a label identifying the namespace as managed by Verrazzano.\n$ kubectl create namespace hello-helidon $ kubectl label namespace hello-helidon verrazzano-managed=true istio-injection=enabled   To deploy the application, apply the hello-helidon OAM resources.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-comp.yaml $ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-app.yaml   Wait for the application to be ready.\n$ kubectl wait \\ --for=condition=Ready pods \\ --all \\ -n hello-helidon \\ --timeout=300s   Explore the application The Hello World Helidon microservices application implements a REST API endpoint, /greet, which returns a message {\"message\":\"Hello World!\"} when invoked.\nNOTE: The following instructions assume that you are using a Kubernetes environment such as OKE. Other environments or deployments may require alternative mechanisms for retrieving addresses, ports, and such.\nFollow these steps to test the endpoints:\n  Get the generated host name for the application.\n$ HOST=$(kubectl get gateway hello-helidon-hello-helidon-appconf-gw \\ -n hello-helidon \\ -o jsonpath={.spec.servers[0].hosts[0]}) $ echo $HOST hello-helidon-appconf.hello-helidon.11.22.33.44.nip.io   Get the EXTERNAL_IP address of the istio-ingressgateway service.\n$ ADDRESS=$(kubectl get service \\ -n istio-system istio-ingressgateway \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}') $ echo $ADDRESS 11.22.33.44   Access the application:\n  Using the command line\n$ curl -sk \\ -X GET \\ https://${HOST}/greet \\ --resolve ${HOST}:443:${ADDRESS} {\"message\":\"Hello World!\"} If you are using nip.io, then you do not need to include --resolve.\n  Local testing with a browser\nTemporarily, modify the /etc/hosts file (on Mac or Linux) or c:\\Windows\\System32\\Drivers\\etc\\hosts file (on Windows 10), to add an entry mapping the host name to the ingress gateway’s EXTERNAL-IP address. For example:\n11.22.33.44 hello-helidon.example.com Then you can access the application in a browser at https://\u003chost\u003e/greet.\n  Using your own DNS name\n Point your own DNS name to the ingress gateway’s EXTERNAL-IP address. In this case, you would need to edit the hello-helidon-app.yaml file to use the appropriate value under the hosts section (such as yourhost.your.domain), before deploying the hello-helidon application. Then, you can use a browser to access the application at https://\u003cyourhost.your.domain\u003e/greet.      A variety of endpoints associated with the deployed application, are available to further explore the logs, metrics, and such.\nAccessing them may require the following:\n  Run this command to get the password that was generated for the telemetry components:\n$ kubectl get secret \\ --namespace verrazzano-system verrazzano \\ -o jsonpath={.data.password} | base64 \\ --decode; echo The associated user name is verrazzano.\n  You will have to accept the certificates associated with the endpoints.\nYou can retrieve the list of available ingresses with following command:\n$ kubectl get ing -n verrazzano-system NAME CLASS HOSTS ADDRESS PORTS AGE verrazzano-ingress \u003cnone\u003e verrazzano.default.140.238.94.217.nip.io 140.238.94.217 80, 443 7d2h vmi-system-es-ingest \u003cnone\u003e elasticsearch.vmi.system.default.140.238.94.217.nip.io 140.238.94.217 80, 443 7d2h vmi-system-grafana \u003cnone\u003e grafana.vmi.system.default.140.238.94.217.nip.io 140.238.94.217 80, 443 7d2h vmi-system-kibana \u003cnone\u003e kibana.vmi.system.default.140.238.94.217.nip.io 140.238.94.217 80, 443 7d2h vmi-system-prometheus \u003cnone\u003e prometheus.vmi.system.default.140.238.94.217.nip.io 140.238.94.217 80, 443 7d2h Using the ingress host information, some of the endpoints available are:\n   Description Address Credentials     Kibana https://[vmi-system-kibana ingress host] verrazzano/telemetry-password   Grafana https://[vmi-system-grafana ingress host] verrazzano/telemetry-password   Prometheus https://[vmi-system-prometheus ingress host] verrazzano/telemetry-password        Troubleshooting   Verify that the application configuration, domain, and ingress trait all exist.\n$ kubectl get ApplicationConfiguration -n hello-helidon $ kubectl get IngressTrait -n hello-helidon   Verify that the hello-helidon service pods are successfully created and transition to the READY state. Note that this may take a few minutes and that you may see some of the services terminate and restart.\n $ kubectl get pods -n hello-helidon NAME READY STATUS RESTARTS AGE hello-helidon-workload-676d97c7d4-wkrj2 2/2 Running 0 5m39s   ","excerpt":"Hello World Helidon This example is a Helidon-based service that returns a “Hello World” response when invoked. The application configuration uses the default, microprofile properties file.\nBefore you …","ref":"/docs/samples/hello-helidon/","title":""},{"body":"Multicluster Hello World Helidon The Hello World Helidon example is a Helidon-based service that returns a “Hello World” response when invoked. The example application is specified using Open Application Model (OAM) component and application configuration YAML files, and then deployed by applying those files. This example shows how to deploy the Hello World Helidon application in a multicluster environment.\nBefore you begin Create a multicluster Verrazzano installation with one admin and one managed cluster, and register the managed cluster, by following the instructions here.\nNOTE: The Hello World Helidon application deployment files are contained in the Verrazzano project located at \u003cVERRAZZANO_HOME\u003e/examples/multicluster/hello-helidon, where \u003cVERRAZZANO_HOME\u003e is the root of the Verrazzano project.\nCreate the application namespace Apply the VerrazzanoProject resource on the admin cluster that defines the namespace for the application. The namespaces defined in the VerrazzanoProject resource will be created on the admin cluster and all the managed clusters.\n$ KUBECONFIG=$KUBECONFIG_ADMIN kubectl apply \\  -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/multicluster/hello-helidon/verrazzano-project.yaml Deploy the Hello World Helidon application   Apply the hello-helidon multicluster resources to deploy the application. Each multicluster resource is an envelope that contains an OAM resource and a list of clusters to which to deploy.\n$ KUBECONFIG=$KUBECONFIG_ADMIN kubectl apply \\  -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/multicluster/hello-helidon/mc-hello-helidon-comp.yaml $ KUBECONFIG=$KUBECONFIG_ADMIN kubectl apply \\  -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/multicluster/hello-helidon/mc-hello-helidon-app.yaml   Wait for the application to be ready on the managed cluster.\n$ KUBECONFIG=$KUBECONFIG_MANAGED1 kubectl wait \\  --for=condition=Ready pods \\  --all -n hello-helidon \\  --timeout=300s   Explore the example application Follow the instructions for exploring the Hello World Helidon application in a single cluster use case. Use the managed cluster kubeconfig for testing the example application.\nTroubleshooting Follow the instructions for troubleshooting the Hello World Helidon application in a single cluster use case. Use the managed cluster kubeconfig for troubleshooting the example application.\n  Verify that the application namespace exists on the managed cluster.\n$ KUBECONFIG=$KUBECONFIG_MANAGED1 kubectl get namespace hello-helidon   Verify that the multicluster resources for the application all exist.\n$ KUBECONFIG=$KUBECONFIG_MANAGED1 kubectl get MultiClusterComponent -n hello-helidon $ KUBECONFIG=$KUBECONFIG_MANAGED1 kubectl get MultiClusterApplicationConfiguration -n hello-helidon   Locating the application on a different cluster By default, the application is located on the managed cluster called managed1. You can change the application’s location to be on a different cluster, which can be the admin cluster or a different managed cluster. In this example, you change the placement of the application to the admin cluster by patching the multicluster resources.\n  To change the application’s location to the admin cluster, specify the change placement patch file.\n# To change the placement to the admin cluster $ export CHANGE_PLACEMENT_PATCH_FILE=\"https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/multicluster/hello-helidon/patch-change-placement-to-admin.yaml\" This environment variable is used in subsequent steps.\n  To change their placement, patch the hello-helidon multicluster resources.\n$ KUBECONFIG=$KUBECONFIG_ADMIN kubectl patch mcappconf hello-helidon-appconf \\  -n hello-helidon \\  --type merge \\  --patch \"$(cat $CHANGE_PLACEMENT_PATCH_FILE)\" $ KUBECONFIG=$KUBECONFIG_ADMIN kubectl patch mccomp hello-helidon-component \\  -n hello-helidon \\  --type merge \\  --patch \"$(cat $CHANGE_PLACEMENT_PATCH_FILE)\"   To verify that their placement has changed, view the multicluster resources.\n$ KUBECONFIG=$KUBECONFIG_ADMIN kubectl get mccomp hello-helidon-component \\  -n hello-helidon \\  -o jsonpath='{.spec.placement}';echo $ KUBECONFIG=$KUBECONFIG_ADMIN kubectl get mcappconf hello-helidon-appconf \\  -n hello-helidon \\  -o jsonpath='{.spec.placement}';echo The cluster name, local, indicates placement in the admin cluster.\n  To change its placement, patch the VerrazzanoProject.\n$ KUBECONFIG=$KUBECONFIG_ADMIN kubectl patch vp hello-helidon \\  -n verrazzano-mc \\  --type merge \\  --patch \"$(cat $CHANGE_PLACEMENT_PATCH_FILE)\"   Wait for the application to be ready on the admin cluster.\n$ KUBECONFIG=$KUBECONFIG_ADMIN kubectl wait \\  --for=condition=Ready pods \\  --all -n hello-helidon \\  --timeout=300s Note: If you are returning the application to the managed cluster, then instead, wait for the application to be ready on the managed cluster.\n$ KUBECONFIG=$KUBECONFIG_MANAGED1 kubectl wait \\  --for=condition=Ready pods \\  --all -n hello-helidon \\  --timeout=300s   Now, you can test the example application running in its new location.\n  To return the application to the managed cluster named managed1, set the value of the CHANGE_PLACEMENT_PATCH_FILE environment variable to the patch file provided for that purpose, then repeat the previous numbered steps.\n# To change the placement back to the managed cluster named managed1 $ export CHANGE_PLACEMENT_PATCH_FILE=\"https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/multicluster/hello-helidon/patch-return-placement-to-managed1.yaml\" Undeploy the Hello World Helidon application Regardless of its location, to undeploy the application, delete the multicluster resources and the project from the admin cluster:\n# Delete the multicluster application configuration $ KUBECONFIG=$KUBECONFIG_ADMIN kubectl delete \\  -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/multicluster/hello-helidon/mc-hello-helidon-app.yaml # Delete the multicluster components for the application $ KUBECONFIG=$KUBECONFIG_ADMIN kubectl delete \\  -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/multicluster/hello-helidon/mc-hello-helidon-comp.yaml # Delete the project $ KUBECONFIG=$KUBECONFIG_ADMIN kubectl delete \\  -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/multicluster/hello-helidon/verrazzano-project.yaml ","excerpt":"Multicluster Hello World Helidon The Hello World Helidon example is a Helidon-based service that returns a “Hello World” response when invoked. The example application is specified using Open …","ref":"/docs/samples/multicluster/hello-helidon/","title":""},{"body":"Multicluster Helidon Sock Shop This example application provides a Helidon implementation of the Sock Shop Microservices Demo Application. It uses OAM resources to define the application deployment in a multicluster environment.\nBefore you begin  Set up a multicluster Verrazzano environment following the installation instructions. The example assumes that there is a managed cluster named managed1 associated with the multicluster environment. If your environment does not have a cluster of that name, then you should edit the deployment files and change the cluster name listed in the placement section.  NOTE: The Sock Shop application deployment files are contained in the Verrazzano project located at \u003cVERRAZZANO_HOME\u003e/examples/sockshop, where \u003cVERRAZZANO_HOME\u003e is the root of the Verrazzano project.\nDeploy the Sock Shop application   Create a namespace for the Sock Shop application by deploying the Verrazzano project.\n$ KUBECONFIG=$KUBECONFIG_ADMIN kubectl apply \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/multicluster/sock-shop/verrazzano-project.yaml   Apply the Sock Shop OAM resources to deploy the application.\n$ KUBECONFIG=$KUBECONFIG_ADMIN kubectl apply \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/multicluster/sock-shop/sock-shop-comp.yaml $ KUBECONFIG=$KUBECONFIG_ADMIN kubectl apply \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/multicluster/sock-shop/sock-shop-app.yaml   Wait for the Sock Shop application to be ready.\n$ KUBECONFIG=$KUBECONFIG_MANAGED1 kubectl wait \\ --for=condition=Ready pods \\ --all -n mc-sockshop \\ --timeout=300s   Explore the Sock Shop application The Sock Shop microservices application implements REST API endpoints including:\n /catalogue - Returns the Sock Shop catalog. This endpoint accepts the GET HTTP request method. /register - POST { \"username\":\"xxx\", \"password\":\"***\", \"email\":\"foo@example.com\", \"firstName\":\"foo\", \"lastName\":\"coo\" } to create a user. This endpoint accepts the POST HTTP request method.  NOTE: The following instructions assume that you are using a Kubernetes environment, such as OKE. Other environments or deployments may require alternative mechanisms for retrieving addresses, ports, and such.\nFollow these steps to test the endpoints:\n  Get the generated host name for the application.\n$ HOST=$(KUBECONFIG=$KUBECONFIG_MANAGED1 kubectl get gateway \\ -n mc-sockshop \\ -o jsonpath={.items[0].spec.servers[0].hosts[0]}) $ echo $HOST sockshop-appconf.mc-sockshop.11.22.33.44.nip.io   Get the EXTERNAL_IP address of the istio-ingressgateway service.\n$ ADDRESS=$(KUBECONFIG=$KUBECONFIG_MANAGED1 kubectl get service \\ -n istio-system istio-ingressgateway \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}') $ echo $ADDRESS 11.22.33.44   Access the Sock Shop example application:\n  Using the command line\n# Get catalogue $ curl -sk \\ -X GET \\ https://${HOST}/catalogue \\ --resolve ${HOST}:443:${ADDRESS} [{\"count\":115,\"description\":\"For all those leg lovers out there....\", ...}] # Add a new user (replace values of username and password) $ curl -i \\ --header \"Content-Type: application/json\" --request POST \\ --data '{\"username\":\"foo\",\"password\":\"****\",\"email\":\"foo@example.com\",\"firstName\":\"foo\",\"lastName\":\"foo\"}' \\ -k https://${HOST}/register \\ --resolve ${HOST}:443:${ADDRESS} # Add an item to the user's cart $ curl -i \\ --header \"Content-Type: application/json\" --request POST \\ --data '{\"itemId\": \"a0a4f044-b040-410d-8ead-4de0446aec7e\",\"unitPrice\": \"7.99\"}' \\ -k https://${HOST}/carts/{username}/items \\ --resolve ${HOST}:443:${ADDRESS} # Get cart items $ curl -i \\ -k https://${HOST}/carts/{username}/items \\ --resolve ${HOST}:443:${ADDRESS} If you are using nip.io, then you do not need to include --resolve.\n  Local testing with a browser\nTemporarily, modify the /etc/hosts file (on Mac or Linux) or c:\\Windows\\System32\\Drivers\\etc\\hosts file (on Windows 10), to add an entry mapping the host name to the ingress gateway’s EXTERNAL-IP address. For example:\n11.22.33.44 sockshop.example.com Then, you can access the application in a browser at https://sockshop.example.com/catalogue.\nIf you are using nip.io, then you can access the application in a browser using the HOST variable (for example, https://${HOST}/catalogue). If you are going through a proxy, you may need to add *.nip.io to the NO_PROXY list.\n  Using your own DNS name\n Point your own DNS name to the ingress gateway’s EXTERNAL-IP address. In this case, you would need to edit the sock-shop-app.yaml file to use the appropriate value under the hosts section (such as yourhost.your.domain), before deploying the Sock Shop application. Then, you can use a browser to access the application at https://\u003cyourhost.your.domain\u003e/catalogue.      Troubleshooting   Verify that the application configuration, domain, and ingress trait all exist.\n$ KUBECONFIG=$KUBECONFIG_MANAGED1 kubectl get ApplicationConfiguration -n mc-sockshop $ KUBECONFIG=$KUBECONFIG_MANAGED1 kubectl get Domain -n mc-sockshop $ KUBECONFIG=$KUBECONFIG_MANAGED1 kubectl get IngressTrait -n mc-sockshop   Verify that the Sock Shop service pods are successfully created and transition to the READY state. Note that this may take a few minutes and that you may see some of the services terminate and restart.\n $ KUBECONFIG=$KUBECONFIG_MANAGED1 kubectl get pods -n mc-sockshop NAME READY STATUS RESTARTS AGE carts-coh-0 2/2 Running 0 38m catalog-coh-0 2/2 Running 0 38m orders-coh-0 2/2 Running 0 38m payment-coh-0 2/2 Running 0 38m shipping-coh-0 2/2 Running 0 38m users-coh-0 2/2 Running 0 38m   A variety of endpoints are available to further explore the logs, metrics, and such, associated with the deployed Sock Shop application. Accessing them may require the following:\n  Run this command to get the password that was generated for the telemetry components:\n$ KUBECONFIG=$KUBECONFIG_ADMIN kubectl get secret \\ --namespace verrazzano-system verrazzano \\ -o jsonpath={.data.password} | base64 \\ --decode; echo The associated user name is verrazzano.\n  You will have to accept the certificates associated with the endpoints.\n  You can retrieve the list of available ingresses with following command:\n$ KUBECONFIG=$KUBECONFIG_MANAGED1 kubectl get ing -n verrazzano-system NAME CLASS HOSTS ADDRESS PORTS AGE verrazzano-ingress \u003cnone\u003e verrazzano.default.140.238.94.217.nip.io 140.238.94.217 80, 443 7d2h vmi-system-es-ingest \u003cnone\u003e elasticsearch.vmi.system.default.140.238.94.217.nip.io 140.238.94.217 80, 443 7d2h vmi-system-grafana \u003cnone\u003e grafana.vmi.system.default.140.238.94.217.nip.io 140.238.94.217 80, 443 7d2h vmi-system-kibana \u003cnone\u003e kibana.vmi.system.default.140.238.94.217.nip.io 140.238.94.217 80, 443 7d2h vmi-system-prometheus \u003cnone\u003e prometheus.vmi.system.default.140.238.94.217.nip.io 140.238.94.217 80, 443 7d2h Using the ingress host information, some of the endpoints available are:\n   Description Address Credentials     Kibana https://[vmi-system-kibana ingress host] verrazzano/telemetry-password   Grafana https://[vmi-system-grafana ingress host] verrazzano/telemetry-password   Prometheus https://[vmi-system-prometheus ingress host] verrazzano/telemetry-password      ","excerpt":"Multicluster Helidon Sock Shop This example application provides a Helidon implementation of the Sock Shop Microservices Demo Application. It uses OAM resources to define the application deployment in …","ref":"/docs/samples/multicluster/sock-shop/","title":""},{"body":"ToDo List ToDo List is an example application containing a WebLogic component. For more information and the source code of this application, see the Verrazzano Examples.\nBefore you begin  Set up a multicluster Verrazzano environment following the installation instructions. The example assumes that there is a managed cluster named managed1 associated with the multicluster environment. If your environment does not have a cluster of that name, then you should edit the deployment files and change the cluster name listed in the placement section. To download the example application image, you must first accept the license agreement.  In a browser, navigate to https://container-registry.oracle.com/ and sign in. Search for example-todo and select the image name in the results. Click Continue, then read and accept the license agreement.    NOTE: The ToDo List application deployment files are contained in the Verrazzano project located at \u003cVERRAZZANO_HOME\u003e/examples/todo-list, where \u003cVERRAZZANO_HOME\u003e is the root of the Verrazzano project.\nDeploy the example application   Create a namespace for the multicluster ToDo List example by applying the Verrazzano project file.\n$ KUBECONFIG=$KUBECONFIG_ADMIN kubectl apply \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/multicluster/todo-list/verrazzano-project.yaml   Log in to the container-registry.oracle.com Docker registry in which the Todo List application image is deployed. You will need the updated Docker config.json, containing your authentication token, for the next step.\n$ docker login container-registry.oracle.com   Update the mc-docker-registry-secret.yaml file with the your registry authentication info. Edit the file and replace the \u003cBASE 64 ENCODED DOCKER CONFIG JSON\u003e with the value generated from the following command.\n$ cat ~/.docker/config.json | base64   Create a docker-registry secret to enable pulling the ToDo List example image from the registry by applying the mc-docker-registry-secret.yaml file. The multicluster secret resource will generate the required secret in the mc-todo-list namespace.\n$ KUBECONFIG=$KUBECONFIG_ADMIN kubectl apply \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/multicluster/todo-list/mc-docker-registry-secret.yaml   Create the WebLogic domain secret by applying the mc-weblogic-domain-secret.yaml file:\n$ KUBECONFIG=$KUBECONFIG_ADMIN kubectl apply \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/multicluster/todo-list/mc-weblogic-domain-secret.yaml Note that the ToDo List example application is preconfigured to use these credentials. If you want to use different credentials, you will need to rebuild the Docker images for the example application. For the source code of this application, see the Verrazzano examples.\n  Apply the ToDo List example multicluster application resources to deploy the application.\n$ KUBECONFIG=$KUBECONFIG_ADMIN kubectl apply \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/multicluster/todo-list/todo-list-components.yaml $ KUBECONFIG=$KUBECONFIG_ADMIN kubectl apply \\ -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/multicluster/todo-list/todo-list-application.yaml   Wait for the ToDo List example application to be ready. You may need to repeat this command several times before it is successful. The tododomain-adminserver pod may take a while to be created and Ready.\n$ KUBECONFIG=$KUBECONFIG_MANAGED1 kubectl wait pod \\ --for=condition=Ready tododomain-adminserver \\ -n mc-todo-list   Get the generated host name for the application.\n$ HOST=$(KUBECONFIG=$KUBECONFIG_MANAGED1 kubectl get gateway \\ -n mc-todo-list \\ -o jsonpath={.items[0].spec.servers[0].hosts[0]}) $ echo $HOST todo-appconf.mc-todo-list.11.22.33.44.nip.io   Get the EXTERNAL_IP address of the istio-ingressgateway service.\n$ ADDRESS=$(KUBECONFIG=$KUBECONFIG_MANAGED1 kubectl get service \\ -n istio-system istio-ingressgateway \\ -o jsonpath='{.status.loadBalancer.ingress[0].ip}') $ echo $ADDRESS 11.22.33.44   Access the ToDo List example application:\n  Using the command line\n$ curl -sk https://${HOST}/todo/ \\ --resolve ${HOST}:443:${ADDRESS} If you are using nip.io, then you do not need to include --resolve.\n  Local testing with a browser\nTemporarily, modify the /etc/hosts file (on Mac or Linux) or c:\\Windows\\System32\\Drivers\\etc\\hosts file (on Windows 10), to add an entry mapping the host name to the ingress gateway’s EXTERNAL-IP address. For example:\n11.22.33.44 todo.example.com Then, you can access the application in a browser at https://todo.example.com/todo.\n  Using your own DNS name\n Point your own DNS name to the ingress gateway’s EXTERNAL-IP address. In this case, you would need to have edited the todo-list-application.yaml file to use the appropriate value under the hosts section (such as yourhost.your.domain), before deploying the ToDo List application. Then, you can use a browser to access the application at https://\u003cyourhost.your.domain\u003e/todo/.  Accessing the application in a browser will open a page, “Derek’s ToDo List”, with an edit field and an Add button that lets add tasks.\n    A variety of endpoints associated with the deployed ToDo List application, are available to further explore the logs, metrics, and such. Accessing them may require the following:\n  Run this command to get the password that was generated for the telemetry components:\n$ KUBECONFIG=$KUBECONFIG_ADMIN kubectl get secret \\ --namespace verrazzano-system verrazzano -o jsonpath={.data.password} | base64 \\ --decode; echo The associated user name is verrazzano.\n  You will have to accept the certificates associated with the endpoints.\n  You can retrieve the list of available ingresses with following command:\n$ KUBECONFIG=$KUBECONFIG_MANAGED1 kubectl get ingress -n verrazzano-system NAME CLASS HOSTS ADDRESS PORTS AGE verrazzano-ingress \u003cnone\u003e verrazzano.default.140.141.142.143.nip.io 140.141.142.143 80, 443 7d2h vmi-system-es-ingest \u003cnone\u003e elasticsearch.vmi.system.default.140.141.142.143.nip.io 140.141.142.143 80, 443 7d2h vmi-system-grafana \u003cnone\u003e grafana.vmi.system.default.140.141.142.143.nip.io 140.141.142.143 80, 443 7d2h vmi-system-kibana \u003cnone\u003e kibana.vmi.system.default.140.141.142.143.nip.io 140.141.142.143 80, 443 7d2h vmi-system-prometheus \u003cnone\u003e prometheus.vmi.system.default.140.141.142.143.nip.io 140.141.142.143 80, 443 7d2h Using the ingress host information, some of the endpoints available are:\n   Description Address Credentials     Kibana https://[vmi-system-kibana ingress host] verrazzano/telemetry-password   Grafana https://[vmi-system-grafana ingress host] verrazzano/telemetry-password   Prometheus https://[vmi-system-prometheus ingress host] verrazzano/telemetry-password      Troubleshooting   Verify that the application configuration, domain, and ingress trait all exist.\n$ KUBECONFIG=$KUBECONFIG_MANAGED1 kubectl get ApplicationConfiguration -n mc-todo-list NAME AGE todo-appconf 19h $ KUBECONFIG=$KUBECONFIG_MANAGED1 kubectl get Domain -n mc-todo-list NAME AGE todo-domain 19h $ KUBECONFIG=$KUBECONFIG_MANAGED1 kubectl get IngressTrait -n mc-todo-list NAME AGE todo-domain-trait-7cbd798c96 19h   Verify that the WebLogic Administration Server and MySQL pods have been created and are running. Note that this will take several minutes.\n$ KUBECONFIG=$KUBECONFIG_MANAGED1 kubectl get pods -n mc-todo-list NAME READY STATUS RESTARTS AGE mysql-5c75c8b7f-vlhck 1/1 Running 0 19h tododomain-adminserver 2/2 Running 0 19h   ","excerpt":"ToDo List ToDo List is an example application containing a WebLogic component. For more information and the source code of this application, see the Verrazzano Examples.\nBefore you begin  Set up a …","ref":"/docs/samples/multicluster/todo-list/","title":""},{"body":"","excerpt":"","ref":"/search/","title":"Search Results"},{"body":"  #td-cover-block-0 { background-image: url(/featured-background_hu3fcc2fb45a2768e8c72dcbce1f48d58b_164050_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/featured-background_hu3fcc2fb45a2768e8c72dcbce1f48d58b_164050_1920x1080_fill_q75_catmullrom_top.jpg); } }  Verrazzano Enterprise Container Platform Learn More  View Repository   A hybrid multicloud Kubernetes-based Enterprise Container Platform for running both cloud-native and traditional applications.\n\n          Application Lifecycle Management Use Open Application Model constructs to describe, deploy, and update multicomponent application systems across clusters in multiple clouds, including clusters on premises.\n   Integrated Monitoring Verrazzano can provision a full monitoring stack for your application, including Elasticsearch, Kibana, Prometheus, and Grafana. Your applications are automatically wired up to send logs and metrics into the monitoring tools.\n   Security Built in security with encryption, certificate management, authentication and authorization services, and network policies.\n       Contributions welcome! We do a Pull Request contributions workflow on GitHub. New users are always welcome!\nRead more …\n   Connect with us on Slack! For project discussions.\nRead more …\n   Follow us on Twitter! For announcement of latest features etc.\nRead more …\n    ","excerpt":"  #td-cover-block-0 { background-image: url(/featured-background_hu3fcc2fb45a2768e8c72dcbce1f48d58b_164050_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { …","ref":"/","title":"Verrazzano Enterprise Container Platform"}]
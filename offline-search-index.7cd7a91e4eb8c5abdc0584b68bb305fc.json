[{"body":"","excerpt":"","ref":"/docs/reference/api/","title":"API"},{"body":"","excerpt":"","ref":"/docs/concepts/","title":"Concepts"},{"body":"Open Application Model (OAM) is a runtime-agnostic specification for defining cloud native applications; it allows developers to focus on the application instead of the complexities of a particular runtime infrastructure. OAM provides the specification for several file formats and rules for a runtime to interpret. Verrazzano uses OAM to enable the definition of a composite application abstraction and makes OAM constructs available within a VerrazzanoApplication YAML file. Verrazzano provides the flexibility to combine what you want into a multi-cloud enablement. It uses the VerrazzanoApplication as a means to encapsulate a set of components, scopes, and traits, and deploy them on a selected cluster.\nOAM’s workload concept makes it easy to use many different workload types. Verrazzano includes specific workload types with special handling to deploy and manage those types, such as WebLogic, Coherence, and Helidon. OAM’s flexibility lets you create a grouping that is managed as a unit, although each component can be scaled or updated independently.\nHow does OAM work? OAM has five core concepts:\n Workloads - Declarations of the kinds of resources supported by the platform and the OpenAPI schema for that resource. Most Kubernetes CRDs can be exposed as workloads. Standard Kubernetes resource types can also be used (for example, Deployment, Service, Pod, ConfigMap). Components - Wrap a workload resource’s spec data within OAM specific metadata. Application Configurations - Describe a collection of components that comprise an application. This is also where customization (such as environmental) of each component is done. Customization is achieved using scopes and traits. Scopes - Apply customization to several components. Traits - Apply customization to a single component.  ","excerpt":"Open Application Model (OAM) is a runtime-agnostic specification for defining cloud native applications; it allows developers to focus on the application instead of the complexities of a particular …","ref":"/docs/concepts/verrazzanooam/","title":"Verrazzano and the Open Application Model"},{"body":"The IngressTrait custom resource contains the configuration of host and path rules for traffic routing to an application. Here is a sample ApplicationConfiguration that specifies an IngressTrait. To deploy an example application that demonstrates this IngressTrait, see Hello World Helidon.\napiVersion: core.oam.dev/v1alpha2 kind: ApplicationConfiguration metadata: name: hello-helidon-appconf namespace: hello-helidon annotations: version: v1.0.0 description: \"Hello Helidon application\" spec: components: - componentName: hello-helidon-component traits: - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: MetricsTrait spec: scraper: verrazzano-system/vmi-system-prometheus-0 - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: IngressTrait metadata: name: hello-helidon-ingress spec: rules: - paths: - path: \"/greet\" pathType: Prefix In the sample configuration, the IngressTrait hello-helidon-ingress is set on the hello-helidon-component application component and defines an ingress rule that configures a path and path type. This exposes a route for external access to the application. Note that because no hosts list is given for the IngressRule, a DNS host name is automatically generated.\nFor example, with the sample application configuration successfully deployed, the application will be accessible with the path specified in the IngressTrait and the generated host name.\n$ HOST=$(kubectl get gateway hello-helidon-hello-helidon-appconf-gw -n hello-helidon -o jsonpath={.spec.servers[0].hosts[0]}) $ echo $HOST hello-helidon-appconf.hello-helidon.11.22.33.44.xip.io $ curl -sk -X GET https://${HOST}/greet Alternatively, specific host names can be given in an IngressRule. Doing this implies that a secret and certificate have been created for the specific hosts and the secret name has been specified in the associated IngressSecurity secretName field.\nIngressTrait    Field Type Description Required     apiVersion string oam.verrazzano.io/v1alpha1 Yes   kind string IngressTrait Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec IngressTraitSpec The desired state of an ingress trait. Yes    IngressTraitSpec IngressTraitSpec specifies the desired state of an ingress trait.\n   Field Type Description Required     rules IngressRule array A list of ingress rules to for an ingress trait. Yes   tls IngressSecurity The security parameters for an ingress trait. This is required only if specific hosts are given in an IngressRule. No    IngressRule IngressRule specifies a rule for an ingress trait.\n   Field Type Description Required     hosts string array One or more hosts exposed by the ingress trait. Wildcard hosts or hosts that are empty are filtered out. If there are no valid hosts provided, then a DNS host name is automatically generated and used. No   paths IngressPath array The paths to be exposed for an ingress trait. Yes    IngressPath IngressPath specifies a specific path to be exposed for an ingress trait.\n   Field Type Description Required     path string If no path is provided, it defaults to /. No   pathType string Path type values are case-sensitive and formatted as follows: exact: exact string matchprefix: prefix-based matchregex: regex-based matchIf the provided ingress path doesn’t contain a pathType, it defaults to prefix if the path is / and exact otherwise. No    IngressSecurity IngressSecurity specifies the secret containing the certificate securing the transport for an ingress trait.\n   Field Type Description Required     secretName string The name of a secret containing the certificate securing the transport. The specification of a secret here implies that a certificate was created for specific hosts, as specified in an IngressRule. Yes    ","excerpt":"The IngressTrait custom resource contains the configuration of host and path rules for traffic routing to an application. Here is a sample ApplicationConfiguration that specifies an IngressTrait. To …","ref":"/docs/reference/api/oam/ingresstrait/","title":"IngressTrait Custom Resource Definition"},{"body":"The LoggingScope custom resource contains the configuration information needed to enable logging for an application component. Here is a sample LoggingScope.\napiVersion: oam.verrazzano.io/v1alpha1 kind: LoggingScope metadata: name: logging-scope namespace: todo-list spec: elasticSearchURL: http://vmi-system-es-ingest.verrazzano-system.svc.cluster.local:9200 secretName: verrazzano workloadRefs: [] Here is a sample ApplicationConfiguration that specifies a LoggingScope. To deploy an example application that demonstrates a LoggingScope, see the ToDo List Lift-and-Shift application.\nNote that if an ApplicationConfiguration does not specify a LoggingScope, then a default LoggingScope will be generated.\napiVersion: core.oam.dev/v1alpha2 kind: ApplicationConfiguration metadata: name: todo-appconf namespace: todo-list annotations: version: v1.0.0 description: \"ToDo List example application\" spec: components: - componentName: todo-domain traits: - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: MetricsTrait spec: scraper: verrazzano-system/vmi-system-prometheus-0 - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: IngressTrait spec: rules: - paths: - path: \"/todo\" pathType: Prefix scopes: - scopeRef: apiVersion: oam.verrazzano.io/v1alpha1 kind: LoggingScope name: logging-scope - componentName: todo-jdbc-configmap - componentName: todo-mysql-configmap - componentName: todo-mysql-service - componentName: todo-mysql-deployment In the above example, the logs for the todo-domain component will be written to the Elasticsearch instance specified in the LoggingScope.\nWith the above application configuration successfully deployed, you can get the log messages for the index todo-list-todo-appconf-todo-domain.\n$ HOST=$(kubectl get ingress -n verrazzano-system vmi-system-es-ingest -o jsonpath={.spec.rules[0].host}) $ VZPASS=$(kubectl get secret --namespace verrazzano-system verrazzano -o jsonpath={.data.password} | base64 --decode; echo) $ curl -ik --user verrazzano:$VZPASS https://$HOST/todo-list-todo-appconf-todo-domain/_doc/_search?q=message:* {\"took\":883,\"timed_out\":false,\"_shards\":{\"total\":1,\"successful\":1,\"skipped\":0,\"failed\":0},\"hits\":{\"total\":{\"value\":235,\"relation\":\"eq\"},\"max_score\":1.0,\"hits\":[{\"_index\":\"todo-list-todo-appconf-todo-domain\",\"_type\":\"_doc\",\"_id\":\"AWV8YXgB5tCoQIDeiWXB\",\"_score\":1.0,\"_source\":{\"timestamp\":\"Mar 23, 2021 11:46:22,784 PM GMT\",\"level\":\"Info\",\"subSystem\":\"Security\",\"serverName\":\"tododomain-adminserver\",\"serverName2\":\"\",\"threadName\":\"main\",\"info1\":\"\",\"info2\":\"\",\"info3\":\"\",\"sequenceNumber\":\"1616543182784\",\"severity\":\"[severity-value: 64] [partition-id: 0] [partition-name: DOMAIN] \",\"messageID\":\"BEA-090905\"... LoggingScope    Field Type Description Required     apiVersion string oam.verrazzano.io/v1alpha1 Yes   kind string LoggingScope Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec LoggingScopeSpec The desired state of a logging scope. Yes    LoggingScopeSpec LoggingScopeSpec specifies the desired state of a logging scope.\n   Field Type Description Required     fluentdImage string The fluentd image. No   elasticSearchURL string The URL for Elasticsearch. No   secretName string The name of secret with Elasticsearch credentials. No   workloadRefs TypedReference array workloadRefs: [] The references to the workloads to which this trait applies. This value is populated by the OAM runtime when a ApplicationConfiguration resource is processed. Yes    ","excerpt":"The LoggingScope custom resource contains the configuration information needed to enable logging for an application component. Here is a sample LoggingScope.\napiVersion: oam.verrazzano.io/v1alpha1 …","ref":"/docs/reference/api/oam/loggingscope/","title":"LoggingScope Custom Resource Definition"},{"body":"The MetricsTrait custom resource contains the configuration information needed to enable metrics for an application component. Component workloads configured with a MetricsTrait are setup to emit metrics through an endpoint that are scraped by a given Prometheus deployment. Here is a sample ApplicationConfiguration that specifies a MetricsTrait. To deploy an example application that demonstrates a MetricsTrait, see Hello World Helidon.\nNote that if an ApplicationConfiguration does not specify a MetricsTrait, then a default MetricsTrait will be generated with values appropriate for the workload type.\napiVersion: core.oam.dev/v1alpha2 kind: ApplicationConfiguration metadata: name: hello-helidon-appconf namespace: hello-helidon annotations: version: v1.0.0 description: \"Hello Helidon application\" spec: components: - componentName: hello-helidon-component traits: - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: MetricsTrait spec: scraper: verrazzano-system/vmi-system-prometheus-0 - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: IngressTrait metadata: name: hello-helidon-ingress spec: rules: - paths: - path: \"/greet\" pathType: Prefix In the sample configuration, a MetricsTrait is specified for the hello-helidon-component application component.\nWith the sample application configuration successfully deployed, you can query for metrics from the application component.\n$ HOST=$(kubectl get ingress -n verrazzano-system vmi-system-prometheus -o jsonpath={.spec.rules[0].host}) $ echo $HOST prometheus.vmi.system.default.\u003cip\u003e.xip.io $ VZPASS=$(kubectl get secret --namespace verrazzano-system verrazzano -o jsonpath={.data.password} | base64 --decode; echo) $ curl -sk --user verrazzano:${VZPASS} -X GET https://${HOST}/api/v1/query?query=vendor_requests_count_total {\"status\":\"success\",\"data\":{\"resultType\":\"vector\",\"result\":[{\"metric\":{\"__name__\":\"vendor_requests_count_total\",\"app\":\"hello-helidon\",\"app_oam_dev_component\":\"hello-helidon-component\",\"app_oam_dev_name\":\"hello-helidon-appconf\",\"app_oam_dev_resourceType\":\"WORKLOAD\",\"app_oam_dev_revision\":\"hello-helidon-component-v1\",\"containerizedworkload_oam_crossplane_io\":\"496df78f-ef8b-4753-97fd-d9218d2f38f1\",\"job\":\"hello-helidon-appconf_default_helidon-logging_hello-helidon-component\",\"namespace\":\"helidon-logging\",\"pod_name\":\"hello-helidon-workload-b7d9d95d8-ht7gb\",\"pod_template_hash\":\"b7d9d95d8\"},\"value\":[1616535232.487,\"4800\"]}]}} MetricsTrait    Field Type Description Required     apiVersion string oam.verrazzano.io/v1alpha1 Yes   kind string MetricsTrait Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec MetricsTraitSpec The desired state of a metrics trait. Yes    MetricsTraitSpec MetricsTraitSpec specifies the desired state of a metrics trait.\n   Field Type Description Required     port integer The HTTP port for the related metrics endpoint. Defaults to 8080. No   path string The HTTP path for the related metrics endpoint. Defaults to /metrics. No   secret string The name of an opaque secret (for example, user name and password) within the workload’s namespace for metrics endpoint access. No   scraper string The Prometheus deployment used to scrape the related metrics endpoints. Defaults to verrazzano-system/vmi-system-prometheus-0. No    ","excerpt":"The MetricsTrait custom resource contains the configuration information needed to enable metrics for an application component. Component workloads configured with a MetricsTrait are setup to emit …","ref":"/docs/reference/api/oam/metricstrait/","title":"MetricsTrait Custom Resource Definition"},{"body":"The MultiClusterApplicationConfiguration custom resource is used to distribute core.oam.dev/v1alpha2/ApplicationConfiguration resources in a multicluster environment. Here is a sample MultiClusterApplicationConfiguration that specifies an ApplicationConfiguration resource to create on the cluster named managed1. To deploy an example application that demonstrates a MultiClusterApplicationConfiguration, see Multicluster Hello World Helidon.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: MultiClusterApplicationConfiguration metadata: name: hello-helidon-appconf namespace: hello-helidon spec: template: metadata: annotations: version: v1.0.0 description: \"Hello Helidon application\" spec: components: - componentName: hello-helidon-component traits: - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: MetricsTrait spec: scraper: verrazzano-system/vmi-system-prometheus-0 - trait: apiVersion: oam.verrazzano.io/v1alpha1 kind: IngressTrait metadata: name: hello-helidon-ingress spec: rules: - paths: - path: \"/greet\" pathType: Prefix placement: clusters: - name: managed1 MultiClusterApplicationConfiguration A MultiClusterApplicationConfiguration is an envelope to create core.oam.dev/v1alpha2/ApplicationConfiguration resources on the clusters specified in the placement section.\n   Field Type Description Required     apiVersion string clusters.verrazzano.io/v1alpha1 Yes   kind string MultiClusterApplicationConfiguration Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec MultiClusterApplicationConfigurationSpec The desired state of a core.oam.dev/v1alpha2/ApplicationConfiguration resource. Yes   status MultiClusterResourceStatus The runtime status of a multicluster resource. No    MultiClusterApplicationConfigurationSpec MultiClusterApplicationConfigurationSpec specifies the desired state of a core.oam.dev/v1alpha2/ApplicationConfiguration resource.\n   Field Type Description Required     template ApplicationConfigurationTemplate The embedded core.oam.dev/v1alpha2/ApplicationConfiguration resource. Yes   placement Placement Clusters in which the resource is to be placed. Yes    ApplicationConfigurationTemplate ApplicationConfigurationTemplate has the metadata and spec of the core.oam.dev/v1alpha2/ApplicationConfiguration resource.\n   Field Type Description Required     metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec ApplicationConfigurationSpec An instance of the struct ApplicationConfigurationSpec defined in core_types.go. No    ","excerpt":"The MultiClusterApplicationConfiguration custom resource is used to distribute core.oam.dev/v1alpha2/ApplicationConfiguration resources in a multicluster environment. Here is a sample …","ref":"/docs/reference/api/multicluster/multiclusterapplicationconfiguration/","title":"MultiClusterApplicationConfiguration Custom Resource Definition"},{"body":"The MultiClusterComponent custom resource is used to distribute core.oam.dev/v1alpha2/Component resources in a multicluster environment. Here is a sample MultiClusterComponent that specifies a OAM component resource to create on the cluster named managed1. To deploy an example application that demonstrates a MultiClusterComponent, see Multicluster Hello World Helidon.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: MultiClusterComponent metadata: name: hello-helidon-component namespace: hello-helidon spec: template: spec: workload: apiVersion: oam.verrazzano.io/v1alpha1 kind: VerrazzanoHelidonWorkload metadata: name: hello-helidon-workload namespace: hello-helidon labels: app: hello-helidon spec: deploymentTemplate: metadata: name: hello-helidon-deployment podSpec: containers: - name: hello-helidon-container image: \"ghcr.io/verrazzano/example-helidon-greet-app-v1:0.1.12-1-20210409130027-707ecc4\" ports: - containerPort: 8080 name: http placement: clusters: - name: managed1 MultiClusterComponent A MultiClusterComponent is an envelope to create core.oam.dev/v1alpha2/Component resources on the clusters specified in the placement section.\n   Field Type Description Required     apiVersion string clusters.verrazzano.io/v1alpha1 Yes   kind string MultiClusterComponent Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec MultiClusterComponentSpec The desired state of a core.oam.dev/v1alpha2/Component resource. Yes   status MultiClusterResourceStatus The runtime status of a multicluster resource. No    MultiClusterComponentSpec MultiClusterComponentSpec specifies the desired state of a core.oam.dev/v1alpha2/Component resource.\n   Field Type Description Required     template ComponentTemplate The embedded core.oam.dev/v1alpha2/Component resource. Yes   placement Placement Clusters in which the resource is to be placed. Yes    ComponentTemplate ComponentTemplate has the metadata and spec of the core.oam.dev/v1alpha2/Component resource.\n   Field Type Description Required     metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec ComponentSpec An instance of the struct ComponentSpec defined in core_types.go. No    ","excerpt":"The MultiClusterComponent custom resource is used to distribute core.oam.dev/v1alpha2/Component resources in a multicluster environment. Here is a sample MultiClusterComponent that specifies a OAM …","ref":"/docs/reference/api/multicluster/multiclustercomponent/","title":"MultiClusterComponent Custom Resource Definition"},{"body":"The MultiClusterConfigMap custom resource is used to distribute Kubernetes ConfigMap resources in a multicluster environment. Here is a sample MultiClusterConfigMap that specifies a Kubernetes ConfigMap to create on the cluster named managed1.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: MultiClusterConfigMap metadata: name: mymcconfigmap namespace: multiclustertest spec: template: metadata: name: myconfigmap namespace: myns data: simple.key: \"simplevalue\" placement: clusters: - name: managed1 MultiClusterConfigMap A MultiClusterConfigMap is an envelope to create Kubernetes ConfigMap resources on the clusters specified in the placement section.\n   Field Type Description Required     apiVersion string clusters.verrazzano.io/v1alpha1 Yes   kind string MultiClusterConfigMap Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec MultiClusterConfigMapSpec The desired state of a Kubernetes ConfigMap. Yes   status MultiClusterResourceStatus The runtime status of a multicluster resource. No    MultiClusterConfigMapSpec MultiClusterConfigMapSpec specifies the desired state of a Kubernetes ConfigMap.\n   Field Type Description Required     template ConfigMapTemplate The embedded Kubernetes ConfigMap. Yes   placement Placement Clusters in which the ConfigMap is to be placed. Yes    ConfigMapTemplate ConfigMapTemplate has the metadata and spec of the Kubernetes ConfigMap.\n   Field Type Description Required     metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   immutable *bool Corresponds to the immutable field of the struct ConfigMap defined in types.go. No   data map[string]string Corresponds to the data field of the struct ConfigMap defined in types.go. No   binaryData map[string][]byte Corresponds to the binaryData field of the struct ConfigMap defined in types.go. No    ","excerpt":"The MultiClusterConfigMap custom resource is used to distribute Kubernetes ConfigMap resources in a multicluster environment. Here is a sample MultiClusterConfigMap that specifies a Kubernetes …","ref":"/docs/reference/api/multicluster/multiclusterconfigmap/","title":"MultiClusterConfigMap Custom Resource Definition"},{"body":"The MultiClusterLoggingScope custom resource is used to distribute core.oam.dev/v1alpha2/LoggingScope resources in a multicluster environment. Here is a sample MultiClusterLoggingScope that specifies a LoggingScope resource to create on the cluster named managed1.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: MultiClusterLoggingScope metadata: name: unit-mclogscope namespace: unit-mclogscope-namespace labels: label1: test1 spec: template: spec: fluentdImage: myFluentdImage:v123 elasticSearchURL: http://myLocalEsHost:9200 secretName: logScopeSecret workloadRefs: [] placement: clusters: - name: managed1 MultiClusterLoggingScope A MultiClusterLoggingScope is an envelope to create core.oam.dev/v1alpha2/LoggingScope resources on the clusters specified in the placement section.\n   Field Type Description Required     apiVersion string clusters.verrazzano.io/v1alpha1 Yes   kind string MultiClusterLoggingScope Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec MultiClusterLoggingScopeSpec The desired state of a core.oam.dev/v1alpha2/LoggingScope resource. Yes   status MultiClusterResourceStatus The runtime status of a multicluster resource. No    MultiClusterLoggingScopeSpec MultiClusterLoggingScopeSpec specifies the desired state of a core.oam.dev/v1alpha2/LoggingScope resource.\n   Field Type Description Required     template LoggingScopeTemplate The embedded core.oam.dev/v1alpha2/LoggingScope resource. Yes   placement Placement Clusters in which the resource is to be placed. Yes    LoggingScopeTemplate LoggingScopeTemplate has the metadata and spec of the core.oam.dev/v1alpha2/LoggingScope resource.\n   Field Type Description Required     metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec ComponentSpec An instance of the struct LoggingScopeSpec defined in core_types.go. No    ","excerpt":"The MultiClusterLoggingScope custom resource is used to distribute core.oam.dev/v1alpha2/LoggingScope resources in a multicluster environment. Here is a sample MultiClusterLoggingScope that specifies …","ref":"/docs/reference/api/multicluster/multiclusterloggingscope/","title":"MultiClusterLoggingScope Custom Resource Definition"},{"body":"The MultiClusterResourceStatus subresource is shared by multicluster custom resources.\nMultiClusterResourceStatus MultiClusterResourceStatus specifies the status portion of a multicluster resource.\n   Field Type Description Required     conditions Condition array The current state of a multicluster resource. No   state string The state of the multicluster resource. State values are case-sensitive and formatted as follows: Pending: deployment to cluster is in progressSucceeded: deployment to cluster successfully completedFailed: deployment to cluster failed No   clusters ClusterLevelStatus array Array of status information for each cluster. No    Condition Condition describes current state of a multicluster resource across all clusters.\n   Field Type Description Required     type string The condition of the multicluster resource which can be checked with a kubectl wait command. Condition values are case-sensitive and formatted as follows: DeployComplete: deployment to all clusters completed successfullyDeployFailed: deployment to all clusters failed Yes   status ConditionStatus An instance of the type ConditionStatus that is defined in types.go. Yes   lastTransitionTime string The last time the condition transitioned from one status to another. No   message string A message with details about the last transition. No    ClusterLevelStatus ClusterLevelStatus describes the status of the multicluster resource on an individual cluster.\n   Field Type Description Required     name string Name of the cluster. Yes   state string The state of the multicluster resource. State values are case-sensitive and formatted as follows: Pending: deployment is in progressSucceeded: deployment successfully completedFailed: deployment failed No   message string Message with details about the status in this cluster. No   lastUpdateTime string The last time the resource state was updated. Yes    ","excerpt":"The MultiClusterResourceStatus subresource is shared by multicluster custom resources.\nMultiClusterResourceStatus MultiClusterResourceStatus specifies the status portion of a multicluster resource. …","ref":"/docs/reference/api/multicluster/multiclusterresourcestatus/","title":"MultiClusterResourceStatus Subresource"},{"body":"The MultiClusterSecret custom resource is used to distribute Kubernetes Secret resources in a multicluster environment. Here is a sample MultiClusterSecret that specifies a Kubernetes secret to create on the cluster named managed1.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: MultiClusterSecret metadata: name: mymcsecret namespace: multiclustertest spec: template: data: username: dmVycmF6emFubw== password: dmVycmF6emFubw== spec: placement: clusters: - name: managed1 MultiClusterSecret A MultiClusterSecret is an envelope to create Kubernetes Secret resources on the clusters specified in the placement section.\n   Field Type Description Required     apiVersion string clusters.verrazzano.io/v1alpha1 Yes   kind string MultiClusterSecret Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec MultiClusterSecretSpec The desired state of a Kubernetes Secret. Yes   status MultiClusterResourceStatus The runtime status of a multicluster resource. No    MultiClusterSecretSpec MultiClusterSecretSpec specifies the desired state of a Kubernetes Secret.\n   Field Type Description Required     template SecretTemplate The embedded Kubernetes Secret. Yes   placement Placement Clusters in which the Secret is to be placed. Yes    SecretTemplate SecretTemplate has the metadata and spec of the Kubernetes Secret.\n   Field Type Description Required     metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   data map[string][]byte Corresponds to the data field of the struct Secret defined in types.go. No   stringData map[string]string Corresponds to the stringData field of the struct Secret defined in types.go. No   type string Corresponds to the type field of the struct Secret defined in types.go. No    ","excerpt":"The MultiClusterSecret custom resource is used to distribute Kubernetes Secret resources in a multicluster environment. Here is a sample MultiClusterSecret that specifies a Kubernetes secret to create …","ref":"/docs/reference/api/multicluster/multiclustersecret/","title":"MultiClusterSecret Custom Resource Definition"},{"body":"The Placement subresource is shared by multicluster custom resources.\nPlacement Placement contains the name of each cluster where this resource will be located.\n   Field Type Description Required     clusters Cluster array An array of cluster locations. Yes    Cluster Cluster contains the name of a single cluster.\n   Field Type Description Required     cluster string The name of a cluster. Yes    ","excerpt":"The Placement subresource is shared by multicluster custom resources.\nPlacement Placement contains the name of each cluster where this resource will be located.\n   Field Type Description Required …","ref":"/docs/reference/api/multicluster/placement/","title":"Placement Subresource"},{"body":"Prerequisites The Quick Start assumes that you have already installed a Kubernetes cluster. For instructions on preparing Kubernetes platforms for installing Verrazzano, see Platform Setup. For detailed installation instructions, see the Installation Guide.\nVerrazzano requires the following:\n A Kubernetes cluster and a compatible kubectl. At least 2 CPUs, 100GB disk storage, and 16GB RAM available on the Kubernetes worker nodes. This is sufficient to install the development profile of Verrazzano. Depending on the resource requirements of the applications you deploy, this may or may not be sufficient for deploying your applications.  For a list of the open source components and versions installed with Verrazzano, see Software Versions.\nNOTE Verrazzano has been tested only on the following versions of Kubernetes: 1.17.x and 1.18.x. Other versions have not been tested and are not guaranteed to work.  Install the Verrazzano platform operator Verrazzano provides a Kubernetes operator to manage the life cycle of Verrazzano installations. The operator works with a custom resource defined in the cluster. You can install, uninstall, and update Verrazzano installations by updating the Verrazzano custom resource. The Verrazzano platform operator controller will apply the configuration from the custom resource to the cluster for you.\nTo install the Verrazzano platform operator:\n  Deploy the Verrazzano platform operator.\n$ kubectl apply -f https://github.com/verrazzano/verrazzano/releases/latest/download/operator.yaml   Wait for the deployment to complete.\n$ kubectl -n verrazzano-install rollout status deployment/verrazzano-platform-operator deployment \"verrazzano-platform-operator\" successfully rolled out   Confirm that the operator pod is correctly defined and running.\n$ kubectl -n verrazzano-install get pods NAME READY STATUS RESTARTS AGE verrazzano-platform-operator-59d5c585fd-lwhsx 1/1 Running 0 114s   Install Verrazzano You install Verrazzano by creating a Verrazzano custom resource in your Kubernetes cluster. Verrazzano currently supports a default production (prod) profile and a development (dev) profile suitable for evaluation.\nThe development profile has the following characteristics:\n Magic (xip.io) DNS Self-signed certificates Shared observability stack used by the system components and all applications Ephemeral storage for the observability stack (if the pods are restarted, you lose all of your logs and metrics) Single-node, reduced memory Elasticsearch cluster  To install Verrazzano:\n  Install Verrazzano with its dev profile.\n$ kubectl apply -f - \u003c\u003cEOF apiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: example-verrazzano spec: profile: dev EOF   Wait for the installation to complete.\n$ kubectl wait \\  --timeout=20m \\  --for=condition=InstallComplete \\  verrazzano/example-verrazzano   (Optional) View the installation logs.\nThe Verrazzano operator launches a Kubernetes job to install Verrazzano. You can view the installation logs from that job with the following command:\n$ kubectl logs -f \\  $( \\  kubectl get pod \\  -l job-name=verrazzano-install-example-verrazzano \\  -o jsonpath=\"{.items[0].metadata.name}\" \\  )   Deploy an example application The Hello World Helidon example application provides a simple Hello World REST service written with Helidon. For more information and the code of this application, see the Verrazzano Examples.\nTo deploy the Hello World Helidon example application:\n  Create a namespace for the example application and add labels identifying the namespace as managed by Verrazzano and enabled for Istio.\n$ kubectl create namespace hello-helidon $ kubectl label namespace hello-helidon verrazzano-managed=true istio-injection=enabled   Apply the hello-helidon resources to deploy the application.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-comp.yaml $ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-app.yaml   Wait for the application to be ready.\n$ kubectl wait --for=condition=Ready pods --all -n hello-helidon --timeout=300s pod/hello-helidon-deployment-78468f5f9c-czmp4 condition met This creates the Verrazzano OAM component application resources for the example, waits for the pods in the hello-helidon namespace to be ready.\n  Save the host name of the load balancer exposing the application’s REST service endpoints.\n$ HOST=$(kubectl get gateway hello-helidon-hello-helidon-appconf-gw -n hello-helidon -o jsonpath='{.spec.servers[0].hosts[0]}')   Get the default message.\n$ curl -sk -X GET \"https://${HOST}/greet\" {\"message\":\"Hello World!\"}   Uninstall the example application To uninstall the Hello World Helidon example application:\n  Delete the Verrazzano application resources.\n$ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-comp.yaml $ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-app.yaml   Delete the example namespace.\n$ kubectl delete namespace hello-helidon namespace \"hello-helidon\" deleted   Verify that the hello-helidon namespace has been deleted.\n$ kubectl get ns hello-helidon Error from server (NotFound): namespaces \"hello-helidon\" not found   Uninstall Verrazzano To uninstall Verrazzano:\n  Delete the Verrazzano custom resource.\n$ kubectl delete verrazzano example-verrazzano  NOTE This command blocks until the uninstall has completed. To follow the progress, you can view the uninstall logs.    (Optional) View the uninstall logs.\nThe Verrazzano operator launches a Kubernetes job to delete the Verrazzano installation. You can view the uninstall logs from that job with the following command:\n$ kubectl logs -f \\  $( \\  kubectl get pod \\  -l job-name=verrazzano-uninstall-example-verrazzano \\  -o jsonpath=\"{.items[0].metadata.name}\" \\  )   Next steps See the Verrazzano Example Applications.\n","excerpt":"Prerequisites The Quick Start assumes that you have already installed a Kubernetes cluster. For instructions on preparing Kubernetes platforms for installing Verrazzano, see Platform Setup. For …","ref":"/docs/setup/quickstart/","title":"Quick Start"},{"body":"","excerpt":"","ref":"/docs/setup/","title":"Setup"},{"body":"The Verrazzano custom resource contains the configuration information for an installation. Here is a sample Verrazzano custom resource file that uses OCI DNS. See other examples here.\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: my-verrazzano spec: environmentName: env profile: prod components: certManager: certificate: acme: provider: letsEncrypt emailAddress: emailAddress@domain.com dns: oci: ociConfigSecret: oci dnsZoneCompartmentOCID: dnsZoneCompartmentOcid dnsZoneOCID: dnsZoneOcid dnsZoneName: my.dns.zone.name ingress: type: LoadBalancer The following table describes the spec portion of the Verrazzano custom resource:\n   Field Type Description Required     environmentName string Name of the installation. This name is part of the endpoint access URLs that are generated. The default value is default. No   profile string The installation profile to select. Valid values are prod (production) and dev (development). The default is prod. No   version string The version to install. Valid versions can be found here. Defaults to the current version supported by the Verrazzano platform operator. No   components Components The Verrazzano components. No    Components    Field Type Description Required     certManager CertManagerComponent The cert-manager component configuration. No   dns DNSComponent The DNS component configuration. No   ingress IngressComponent The ingress component configuration. No   istio IstioComponent The Istio component configuration. No    CertManager Component    Field Type Description Required     certificate Certificate The certificate configuration. No    Certificate    Field Type Description Required     acme Acme The ACME configuration. Either acme or ca must be specified. No   ca CertificateAuthority The certificate authority configuration. Either acme or ca must be specified. No    Acme    Field Type Description Required     provider string Name of the Acme provider. Yes   emailAddress string Email address of the user. Yes    CertificateAuthority    Field Type Description Required     secretName string The secret name. Yes   clusterResourceNamespace string The secrete namespace. Yes    DNS Component    Field Type Description Required     oci DNS-OCI OCI DNS configuration. Either oci or external must be specified. No   external DNS-External External DNS configuration. Either oci or external must be specified. No    DNS OCI    Field Type Description Required     ociConfigSecret string Name of the OCI configuration secret. Generate a secret based on the OCI configuration profile you want to use. You can specify a profile other than DEFAULT and specify the secret name. See instructions by running ./install/create_oci_config_secret.sh. Yes   dnsZoneCompartmentOCID string The OCI DNS compartment OCID. Yes   dnsZoneOCID string The OCI DNS zone OCID. Yes   dnsZoneName string Name of OCI DNS zone. Yes    DNS External    Field Type Description Required     external.suffix string The suffix for DNS names. Yes    Ingress Component    Field Type Description Required     type string The ingress type. Valid values are LoadBalancer and NodePort. The default value is LoadBalancer. Yes   ingressNginxArgs NameValue list The list of argument names and values. No   ports PortConfig list The list port configurations used by the ingress. No    Port Config    Field Type Description Required     name string The port name. No   port string The port value. Yes   targetPort string The target port value. The default is same as the port value. Yes   protocol string The protocol used by the port. TCP is the default. No   nodePort string The nodePort value. No    Name Value    Field Type Description Required     name string The argument name. Yes   value string The argument value. Either value or valueList must be specifed. No   valueList string list The list of argument values. Either value or valueList must be specified. No   setString Boolean Specifies if the value is a string No    Istio Component    Field Type Description Required     istioInstallArgs NameValue list A list of Istio Helm chart arguments and values to apply during the installation of Istio. Each argument is specified as either a name/value or name/valueList pair. No    ","excerpt":"The Verrazzano custom resource contains the configuration information for an installation. Here is a sample Verrazzano custom resource file that uses OCI DNS. See other examples here.\napiVersion: …","ref":"/docs/reference/api/verrazzano/verrazzano/","title":"Verrazzano Custom Resource Definition"},{"body":"VerrazzanoCoherenceWorkload The VerrazzanoCoherenceWorkload custom resource contains the configuration information for a Coherence workload within Verrazzano. Here is a sample component that specifies a VerrazzanoCoherenceWorkload. To deploy an example application that demonstrates this workload type, see Sock Shop.\napiVersion: core.oam.dev/v1alpha2 kind: Component metadata: name: carts namespace: sockshop spec: workload: apiVersion: oam.verrazzano.io/v1alpha1 kind: VerrazzanoCoherenceWorkload spec: template: metadata: name: carts-coh spec: cluster: SockShop role: Carts replicas: 1 image: ghcr.io/helidon-sockshop/carts-coherence:2.2.0 imagePullPolicy: Always application: type: helidon jvm: args: - \"-Dcoherence.k8s.operator.health.wait.dcs=false\" - \"-Dcoherence.metrics.legacy.names=false\" memory: heapSize: 2g coherence: logLevel: 9 ports: - name: http port: 7001 service: name: carts port: 80 serviceMonitor: enabled: true - name: metrics port: 7001 serviceMonitor: enabled: true VerrazzanoCoherenceWorkload    Field Type Description Required     apiVersion string oam.verrazzano.io/v1alpha1 Yes   kind string VerrazzanoCoherenceWorkload Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec VerrazzanoCoherenceWorkloadSpec The desired state of a Verrazzano Coherence workload. Yes    VerrazzanoCoherenceWorkloadSpec VerrazzanoCoherenceWorkloadSpec specifies the desired state of a Verrazzano Coherence workload.\n   Field Type Description Required     template RawExtension The metadata and spec for the underlying Coherence resource. Yes    VerrazzanoHelidonWorkload The VerrazzanoHelidonWorkload custom resource contains the configuration information for a Helidon workload within Verrazzano. Here is a sample component that specifies a VerrazzanoHelidonWorkload. To deploy an example application that demonstrates this workload type, see Hello World Helidon.\napiVersion: core.oam.dev/v1alpha2 kind: Component metadata: name: hello-helidon-component namespace: hello-helidon spec: workload: apiVersion: oam.verrazzano.io/v1alpha1 kind: VerrazzanoHelidonWorkload metadata: name: hello-helidon-workload labels: app: hello-helidon spec: deploymentTemplate: metadata: name: hello-helidon-deployment podSpec: containers: - name: hello-helidon-container image: \"ghcr.io/verrazzano/example-helidon-greet-app-v1:0.1.10-3-20201016220428-56fb4d4\" ports: - containerPort: 8080 name: http VerrazzanoHelidonWorkload    Field Type Description Required     apiVersion string oam.verrazzano.io/v1alpha1 Yes   kind string VerrazzanoHelidonWorkload Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec VerrazzanoHelidonWorkloadSpec The desired state of a Verrazzano Helidon workload. Yes    VerrazzanoHelidonWorkloadSpec VerrazzanoHelidonWorkloadSpec specifies the desired state of a Verrazzano Helidon workload.\n   Field Type Description Required     deploymentTemplate DeploymentTemplate The embedded deployment. Yes    DeploymentTemplate DeploymentTemplate specifies the metadata and pod spec of the underlying deployment.\n   Field Type Description Required     metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   strategy DeploymentStrategy The replacement strategy of the underlying deployment. No   podSpec PodSpec The pod spec of the underlying deployment. Yes    VerrazzanoWebLogicWorkload The VerrazzanoWebLogicWorkload custom resource contains the configuration information for a WebLogic Domain workload within Verrazzano. Here is a sample component that specifies a VerrazzanoWebLogicWorkload. To deploy an example application that demonstrates this workload type, see the ToDo List Lift-and-Shift application.\napiVersion: core.oam.dev/v1alpha2 kind: Component metadata: name: todo-domain namespace: todo-list spec: workload: apiVersion: oam.verrazzano.io/v1alpha1 kind: VerrazzanoWebLogicWorkload spec: template: metadata: name: todo-domain namespace: todo-list spec: domainUID: tododomain domainHome: /u01/domains/tododomain image: container-registry.oracle.com/verrazzano/example-todo:0.8.0 imagePullSecrets: - name: tododomain-repo-credentials domainHomeSourceType: \"FromModel\" includeServerOutInPodLog: true replicas: 1 webLogicCredentialsSecret: name: tododomain-weblogic-credentials configuration: introspectorJobActiveDeadlineSeconds: 900 model: configMap: tododomain-jdbc-config domainType: WLS modelHome: /u01/wdt/models runtimeEncryptionSecret: tododomain-runtime-encrypt-secret secrets: - tododomain-jdbc-tododb serverPod: env: - name: JAVA_OPTIONS value: \"-Dweblogic.StdoutDebugEnabled=false\" - name: USER_MEM_ARGS value: \"-Djava.security.egd=file:/dev/./urandom -Xms64m -Xmx256m \" - name: WL_HOME value: /u01/oracle/wlserver - name: MW_HOME value: /u01/oracle VerrazzanoWebLogicWorkload    Field Type Description Required     apiVersion string oam.verrazzano.io/v1alpha1 Yes   kind string VerrazzanoWebLogicWorkload Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. No   spec VerrazzanoWebLogicWorkloadSpec The desired state of a Verrazzano WebLogic workload. Yes    VerrazzanoWebLogicWorkloadSpec VerrazzanoWebLogicWorkloadSpec specifies the desired state of a Verrazzano WebLogic workload.\n   Field Type Description Required     template RawExtension The metadata and spec for the underlying WebLogic Domain resource. Yes    ","excerpt":"VerrazzanoCoherenceWorkload The VerrazzanoCoherenceWorkload custom resource contains the configuration information for a Coherence workload within Verrazzano. Here is a sample component that specifies …","ref":"/docs/reference/api/oam/workloads/","title":"Verrazzano Workload Custom Resource Definitions"},{"body":"The VerrazzanoManagedCluster custom resource is used to register a managed cluster with an admin cluster. Here is a sample VerrazzanoManagedCluster that registers the cluster named managed1. To deploy an example application that demonstrates a VerrazzanoManagedCluster, see Multicluster Hello World Helidon.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: VerrazzanoManagedCluster metadata: name: managed1 namespace: verrazzano-mc spec: description: \"Managed Cluster 1\" prometheusSecret: prometheus-managed1 VerrazzanoManagedCluster    Field Type Description Required     apiVersion string clusters.verrazzano.io/v1alpha1 Yes   kind string VerrazzanoManagedCluster Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec VerrazzanoManagedClusterSpec The managed cluster specification. Yes   status VerrazzanoManagedClusterStatus The runtime status this resource. No    VerrazzanoManagedClusterSpec VerrazzanoManagedClusterSpec specifies a managed cluster to associate with an admin cluster.\n   Field Type Description Required     description string The description of the managed cluster. No   prometheusSecret string The name of a Secret that is used to configure the admin cluster to scrape metrics from the Prometheus endpoint on the managed cluster. See the instructions for how to create this Secret. Yes   serviceAccount string The name of the ServiceAccount that was generated for the managed cluster. This field is managed by a Verrazzano Kubernetes operator. No   managedClusterManifestSecret string The name of the Secret containing generated YAML manifest file to be applied by the user to the managed cluster. This field is managed by a Verrazzano Kubernetes operator. No    VerrazzanoManagedClusterStatus    Field Type Description Required     conditions Condition array The current state of this resource. No   lastAgentConnectTime string The last time the agent from this managed cluster connected to the admin cluster. No   apiUrl string The Verrazzano API server URL for the managed cluster. No    Condition Condition describes current state of this resource.\n   Field Type Description Required     type string The condition of the multicluster resource which can be checked with a kubectl wait command. Condition values are case-sensitive and formatted as follows: Ready: the VerrazzanoManagedCluster is ready to be used and all resources needed have been generated. Yes   status ConditionStatus An instance of the type ConditionStatus that is defined in types.go. Yes   lastTransitionTime string The last time the condition transitioned from one status to another. No   message string A message with details about the last transition. No    Instructions to create prometheusSecret Instructions to create the Secret that is referenced in the field prometheusSecret.\n$ CLUSTER_NAME=managed2 $ echo \"prometheus:\" \u003e ${CLUSTER_NAME}.yaml $ echo \" host: $(kubectl get ing vmi-system-prometheus -n verrazzano-system -o jsonpath='{.spec.tls[0].hosts[0]}')\" \u003e\u003e ${CLUSTER_NAME}.yaml $ CA_CERT=$(kubectl -n verrazzano-system get secret system-tls -o json | jq -r '.data.\"ca.crt\"' | base64 --decode) $ echo \" cacrt: |\" \u003e\u003e ${CLUSTER_NAME}.yaml $ echo -e \"$CA_CERT\" | sed 's/^/ /' \u003e\u003e ${CLUSTER_NAME}.yaml $ kubectl create secret generic prometheus-${CLUSTER_NAME} -n verrazzano-mc --from-file=${CLUSTER_NAME}.yaml ","excerpt":"The VerrazzanoManagedCluster custom resource is used to register a managed cluster with an admin cluster. Here is a sample VerrazzanoManagedCluster that registers the cluster named managed1. To deploy …","ref":"/docs/reference/api/multicluster/verrazzanomanagedcluster/","title":"VerrazzanoManagedCluster Custom Resource Definition"},{"body":"The VerrazzanoProject custom resource is used to create the application namespaces and their associated security settings on one or more clusters. The namespaces are always created on the admin cluster. Here is a sample VerrazzanoProject that specifies a namespace to create on the cluster named managed1.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: VerrazzanoProject metadata: name: hello-helidon namespace: verrazzano-mc spec: template: namespaces: - metadata: name: hello-helidon placement: clusters: - name: managed1 VerrazzanoProject    Field Type Description Required     apiVersion string clusters.verrazzano.io/v1alpha1 Yes   kind string VerrazzanoProject Yes   metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec VerrazzanoProjectSpec The project specification. Yes   status MultiClusterResourceStatus The runtime status of a multicluster resource. No    VerrazzanoProjectSpec VerrazzanoProjectSpec specifies the namespaces to create and on which clusters to create them.\n   Field Type Description Required     template ProjectTemplate The project template. Yes   placement Placement Clusters on which the namespaces are to be created. Yes    ProjectTemplate ProjectTemplate contains the list of namespaces to create and the optional security configuration for each namespace.\n   Field Type Description Required     namespaces NamespaceTemplate array The list of application namespaces to create for this project. Yes   security SecuritySpec The project security configuration. No   networkPolicies NetworkPolicyTemplate array The network policies applied to namespaces in the project. No    NamespaceTemplate NamespaceTemplate contains the metadata and specification of a Kubernetes namespace.\n   Field Type Description Required     metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec NamespaceSpec An instance of the struct NamespaceSpec defined in types.go. No    SecuritySpec SecuritySpec defines the security configuration for a project.\n   Field Type Description Required     projectAdminSubjects Subject The subject to bind to the verrazzano-project-admin role. Encoded as an instance of the struct Subject defined in types.go. No   projectMonitorSubjects Subject The subject to bind to the verrazzano-project-monitoring role. Encoded as an instance of the struct Subject defined in types.go. No    NetworkPolicyTemplate NetworkPolicyTemplate contains the metadata and specification of the underlying NetworkPolicy.\n   Field Type Description Required     metadata ObjectMeta Refer to Kubernetes API documentation for fields of metadata. Yes   spec NetworkPolicySpec An instance of the struct NetworkPolicySpec defined in types.go. No    ","excerpt":"The VerrazzanoProject custom resource is used to create the application namespaces and their associated security settings on one or more clusters. The namespaces are always created on the admin …","ref":"/docs/reference/api/multicluster/verrazzanoproject/","title":"VerrazzanoProject Custom Resource Definition"},{"body":"Get the consoles URLs Verrazzano installs several consoles. Get the ingress for the consoles with the following command:\n$ kubectl get ingress -A\nTo get the URL, prefix https:// to the host name returned. For example, https://rancher.myenv.mydomain.com.\nThe following is an example of the ingresses:\n NAMESPACE NAME HOSTS ADDRESS PORTS AGE cattle-system rancher rancher.myenv.mydomain.com 128.234.33.198 80, 443 93m keycloak keycloak keycloak.myenv.mydomain.com 128.234.33.198 80, 443 69m verrazzano-system verrazzano-operator-ingress api.myenv.mydomain.com 128.234.33.198 80, 443 81m verrazzano-system vmi-system-api api.vmi.system.myenv.mydomain.com 128.234.33.198 80, 443 80m verrazzano-system vmi-system-es-ingest elasticsearch.vmi.system.myenv.mydomain.com 128.234.33.198 80, 443 80m verrazzano-system vmi-system-grafana grafana.vmi.system.myenv.mydomain.com 128.234.33.198 80, 443 80m verrazzano-system vmi-system-kibana kibana.vmi.system.myenv.mydomain.com 128.234.33.198 80, 443 80m verrazzano-system vmi-system-prometheus prometheus.vmi.system.myenv.mydomain.com 128.234.33.198 80, 443 80m verrazzano-system vmi-system-prometheus-gw prometheus-gw.vmi.system.myenv.mydomain.com 128.234.33.198 80, 443 80m Get console credentials You will need the credentials to access the consoles installed by Verrazzano.\nConsoles accessed by the same user name/password  Grafana Prometheus Kibana Elasticsearch  User: verrazzano\nTo get the password:\n$ kubectl get secret --namespace verrazzano-system verrazzano -o jsonpath={.data.password} | base64 --decode; echo\nThe Keycloak admin console User: keycloakadmin\nTo get the password:\n$ kubectl get secret --namespace keycloak keycloak-http -o jsonpath={.data.password} | base64 --decode; echo\nThe Rancher console User: admin\nTo get the password:\n$ kubectl get secret --namespace cattle-system rancher-admin-secret -o jsonpath={.data.password} | base64 --decode; echo\nChange the Verrazzano password To change the Verrazzano password, first change the user password in Keycloak and then update the Verrazzano secret.\nChange the user in Keycloak\n Navigate to the Keycloak admin console. Obtaining the Keycloak admin console URL is described here. Obtaining the Keycloak admin console credentials is described here. In the left pane, under Manage, select Users. In the Users pane, search for verrazzano or click View all users. For the verrazzano user, click the Edit action. At the top, select the Credentials tab. Specify the new password and confirm. Specify whether the new password is a temporary password. A temporary password must be reset on next login. Click Reset Password. Confirm the password reset by clicking Reset password in the confirmation dialog.  Update the Verrazzano secret\nGet the base64 encoding for your new password:\n$ echo -n 'MyNewPwd' | base64\nUpdate the password in the secret:\n$ kubectl edit secret verrazzano -n verrazzano-system\nReplace the existing password value with the new base64 encoded value.\n","excerpt":"Get the consoles URLs Verrazzano installs several consoles. Get the ingress for the consoles with the following command:\n$ kubectl get ingress -A\nTo get the URL, prefix https:// to the host name …","ref":"/docs/operations/","title":"Operations"},{"body":"Overview Developing and deploying an application to Verrazzano consists of:\n Packaging the application as a Docker image. Publishing the application’s Docker image to a container registry. Applying the application’s Verrazzano components to the cluster. Applying the application’s Verrazzano applications to the cluster.  This guide does not provide the full details for the first two steps. An existing example application Docker image has been packaged and published for use.\nVerrazzano supports application definition using Open Application Model (OAM). Verrrazzano applications are composed of components and application configurations. This document demonstrates creating OAM resources that define an application as well as the steps required to deploy those resources.\nWhat you need   About 10 minutes.\n  Access to an existing Kubernetes cluster with Verrazzano installed.\n  Access to the application’s image in GitHub Container Registry.\nConfirm access using this command to pull the example’s Docker image:\n$ docker pull ghcr.io/verrazzano/example-helidon-greet-app-v1:0.1.12-1-20210218160249-d8db8f3   Application development This guide uses an example application which was written with Java and Helidon. For the implementation details, see the Helidon MP tutorial. See the application source code in the Verrazzano examples repository.\nThe example application is a JAX-RS service and implements the following REST endpoints:\n /greet - Returns a default greeting message that is stored in memory. This endpoint accepts the GET HTTP request method. /greet/{name} - Returns a greeting message including the name provided in the path parameter. This endpoint accepts the GET HTTP request method. /greet/greeting - Changes the greeting message to be used in future calls to the other endpoints. This endpoint accepts the PUT HTTP request method and a JSON payload.  The following code shows a portion of the application’s implementation. The Verrazzano examples repository contains the complete implementation. An important detail here is that the application contains a single resource exposed on path /greet.\npackage io.helidon.examples.quickstart.mp; ... @Path(\"/greet\") @RequestScoped public class GreetResource { @GET @Produces(MediaType.APPLICATION_JSON) public JsonObject getDefaultMessage() { ... } @Path(\"/{name}\") @GET @Produces(MediaType.APPLICATION_JSON) public JsonObject getMessage(@PathParam(\"name\") String name) { ... } @Path(\"/greeting\") @PUT @Consumes(MediaType.APPLICATION_JSON) ... public Response updateGreeting(JsonObject jsonObject) { ... } } A Dockerfile is used to package the completed application JAR file into a Docker image. The following code shows a portion of the Dockerfile. The Verrazzano examples repository contains the complete Dockerfile. Note that the Docker container exposes a single port 8080.\nFROMghcr.io/oracle/oraclelinux:7-slim...CMD java -cp /app/helidon-quickstart-mp.jar:/app/* io.helidon.examples.quickstart.mp.MainEXPOSE8080Application deployment When you deploy applications with Verrazzano, the platform sets up connections, network policies, and ingresses in the service mesh, and wires up a monitoring stack to capture the metrics, logs, and traces. Verrazzano employs OAM components to define the functional units of a system that are then assembled and configured by defining associated application configurations.\nVerrazzano components A Verrazzano OAM component is a Kubernetes Custom Resource describing an application’s general composition and environment requirements. The following code shows the component for the example application used in this guide. This resource describes a component which is implemented by a single Docker image containing a Helidon application exposing a single endpoint.\napiVersion:core.oam.dev/v1alpha2kind:Componentmetadata:name:hello-helidon-componentnamespace:hello-helidonspec:workload:apiVersion:oam.verrazzano.io/v1alpha1kind:VerrazzanoHelidonWorkloadmetadata:name:hello-helidon-workloadlabels:app:hello-helidonspec:deploymentTemplate:metadata:name:hello-helidon-deploymentpodSpec:containers:- name:hello-helidon-containerimage:\"ghcr.io/verrazzano/example-helidon-greet-app-v1:0.1.10-3-20201016220428-56fb4d4\"ports:- containerPort:8080name:httpA brief description of each field of the component:\n apiVersion - Version of the component custom resource definition kind - Standard name of the component custom resource definition metadata.name - The name used to create the component’s custom resource metadata.namespace - The namespace used to create this component’s custom resource spec.workload.kind - VerrazzanoHelidonWorkload defines a stateless workload of Kubernetes spec.workload.spec.deploymentTemplate.podSpec.metadata.name - The name used to create the stateless workload of Kubernetes spec.workload.spec.deploymentTemplate.podSpec.containers - The implementation containers spec.workload.spec.deploymentTemplate.podSpec.containers.ports - Ports exposed by the container  Verrazzano application configurations A Verrazzano application configuration is a Kubernetes Custom Resource which provides environment specific customizations. The following code shows the application configuration for the example used in this guide. This resource specifies the deployment of the application to the hello-helidon namespace. Additional runtime features are specified using traits, or runtime overlays that augment the workload. For example, the ingress trait specifies the ingress host and path, while the metrics trait provides the Prometheus scraper used to obtain the application related metrics.\napiVersion:core.oam.dev/v1alpha2kind:ApplicationConfigurationmetadata:name:hello-helidon-appconfnamespace:hello-helidonannotations:version:v1.0.0description:\"Hello Helidon application\"spec:components:- componentName:hello-helidon-componenttraits:- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:MetricsTraitspec:scraper:verrazzano-system/vmi-system-prometheus-0- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:IngressTraitmetadata:name:hello-helidon-ingressspec:rules:- paths:- path:\"/greet\"pathType:PrefixA brief description of each field in the application configuration:\n apiVersion - Version of the ApplicationConfiguration custom resource definition kind - Standard name of the application configuration custom resource definition metadata.name - The name used to create this application configuration resource metadata.namespace - The namespace used for this application configuration custom resource spec.components - Reference to the application’s components leveraged to specify runtime configuration spec.components[].traits - The traits specified for the application’s components  To explore traits, we can examine the fields of an ingress trait:\n apiVersion - Version of the OAM trait custom resource definition kind - IngressTrait is the name of the OAM application ingress trait custom resource definition spec.rules.paths - The context paths for accessing the application  Deploy the application The following steps are required to deploy the example application. Steps similar to the apply steps would be used to deploy any application to Verrazzano.\n  Create a namespace for the example application and add labels identifying the namespace as managed by Verrazzano and enabled for Istio.\n$ kubectl create namespace hello-helidon $ kubectl label namespace hello-helidon verrazzano-managed=true istio-injection=enabled   Apply the application’s component.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-comp.yaml This step causes the validation and creation of the component resource. No other resources or objects are created as a result. Application configurations applied in the future may reference this component resource.\n  Apply the application configuration.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-app.yaml This step causes the validation and creation of the application configuration resource. This operation triggers the activation of a number of Verrazzano operators. These operators create other Kubernetes objects (for example, Deployments, ReplicaSets, Pods, Services, Ingresses) that collectively provide and support the application.\n  Configure the application’s DNS resolution.\nAfter deploying the application, configure DNS to resolve the application’s ingress DNS name to the application’s load balancer IP address. The generated host name is obtained by querying Kubernetes for the gateway:\n$ kubectl get gateway hello-helidon-hello-helidon-appconf-gw -n hello-helidon -o jsonpath='{.spec.servers[0].hosts[0]}' The load balancer IP is obtained by querying Kubernetes for the Istio ingress gateway status:\n$ kubectl get service -n istio-system istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}' DNS configuration steps are outside the scope of this guide. For DNS infrastructure that can be configured and used, see the Oracle Cloud Infrastructure DNS documentation. In some small non-production scenarios, DNS configuration using /etc/hosts or an equivalent may be sufficient.\n  Verify the deployment Applying the application configuration initiates the creation of several Kubernetes objects. Actual creation and initialization of these objects occurs asynchronously. The following steps provide commands for determining when these objects are ready for use.\nNote: Many other Kubernetes objects unrelated to the example application may also exist. Those have been omitted from the lists.\n  Verify the Helidon application pod is running.\n$ kubectl get pods -n hello-helidon | grep '^NAME\\|hello-helidon-deployment' NAME READY STATUS RESTARTS AGE hello-helidon-deployment-78468f5f9c-czmp4 3/3 Running 0 22h The parameter hello-helidon-deployment is from the component’s spec.workload.spec.deploymentTemplate.podSpec.metadata.name value.\n  Verify the Verrazzano application operator pod is running.\n$ kubectl get pods -n verrazzano-system | grep '^NAME\\|verrazzano-application-operator' NAME READY STATUS RESTARTS AGE verrazzano-application-operator-5485967588-lp6cw 1/1 Running 0 8d The namespace verrazzano-system is used by Verrazzano for non-application objects managed by Verrazzano. A single verrazzano-application-operator manages the life cycle of all OAM based applications within the cluster.\n  Verify the Verrazzano monitoring infrastructure is running.\n$ kubectl get pods -n verrazzano-system | grep '^NAME\\|vmi-system' NAME READY STATUS RESTARTS AGE vmi-system-api-6fb4fd57cb-95ttz 1/1 Running 0 8d vmi-system-es-master-0 1/1 Running 0 11h vmi-system-grafana-674b4f5df7-f4f2p 1/1 Running 0 8d vmi-system-kibana-759b854fc6-4tsjv 1/1 Running 0 8d vmi-system-prometheus-0-f6f587664-pfm54 3/3 Running 0 101m vmi-system-prometheus-gw-68c45f84b8-jrxlt 1/1 Running 0 8d These pods in the verrazzano-system namespace constitute a monitoring stack created by Verrazzano for the deployed applications.\nThe monitoring infrastructure comprises several components:\n vmi-system-api - Internal API for configuring monitoring vmi-system-es - Elasticsearch for log collection vmi-system-kibana - Kibana for log visualization vmi-system-grafana - Grafana for metric visualization vmi-system-prometheus - Prometheus for metric collection     Diagnose failures.\nView the event logs of any pod not entering the Running state within a reasonable length of time, such as five minutes.\n$ kubectl describe pod -n hello-helidon hello-helidon-deployment-78468f5f9c-czmp4 Use the specific namespace and name for the pod being investigated.\n  Explore the application Follow these steps to explore the application’s functionality. If DNS was not configured, then use the alternative commands.\n  Save the host name and IP address of the load balancer exposing the application’s REST service endpoints for later.\n$ HOST=$(kubectl get gateway hello-helidon-hello-helidon-appconf-gw -n hello-helidon -o jsonpath='{.spec.servers[0].hosts[0]}') $ ADDRESS=$(kubectl get service -n istio-system istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}') NOTE:\n The value of ADDRESS is used only if DNS has not been configured. The following alternative commands may not work in conjunction with firewalls that validate HTTP Host headers.    Get the default message.\n$ curl -sk -X GET \"https://${HOST}/greet\" {\"message\":\"Hello World!\"} If DNS has not been configured, then use this command.\n$ curl -sk -X GET \"https://${HOST}/greet\" --resolve ${HOST}:443:${ADDRESS}   Get a message for Robert.\n$ curl -sk -X GET \"https://${HOST}/greet/Robert\" {\"message\":\"Hello Robert!\"} If DNS has not been configured, then use this command.\n$ curl -sk -X GET \"https://${HOST}/greet/Robert\" --resolve ${HOST}:443:${ADDRESS}   Update the default greeting.\n$ curl -sk -X PUT \"https://${HOST}/greet/greeting\" -H 'Content-Type: application/json' -d '{\"greeting\" : \"Greetings\"}' If DNS has not been configured, then use this command.\n$ curl -sk -X PUT \"https://${HOST}/greet/greeting\" -H 'Content-Type: application/json' -d '{\"greeting\" : \"Greetings\"}' --resolve ${HOST}:443:${ADDRESS}   Get the new message for Robert.\n$ curl -sk -X GET \"https://${HOST}/greet/Robert\" {\"message\":\"Greetings Robert!\"} If DNS has not been configured, then use this command.\n$ curl -sk -X GET \"https://${HOST}/greet/Robert\" --resolve ${HOST}:443:${ADDRESS}   Access the application’s logs Deployed applications have log collection enabled. These logs are collected using Elasticsearch and can be accessed using Kibana. Elasticsearch and Kibana are examples of infrastructure Verrazzano creates in support of an application as a result of applying an application configuration.\nDetermine the URL to access Kibana:\n$ KIBANA_HOST=$(kubectl get ingress -n verrazzano-system vmi-system-kibana -o jsonpath='{.spec.rules[0].host}') $ KIBANA_URL=\"https://${KIBANA_HOST}\" $ echo \"${KIBANA_URL}\" $ open \"${KIBANA_URL}\" The user name to access Kibana defaults to verrazzano during the Verrazzano installation.\nDetermine the password to access Kibana:\n$ echo $(kubectl get secret -n verrazzano-system verrazzano -o jsonpath={.data.password} | base64 --decode) Access the application’s metrics Deployed applications have metric collection enabled. Grafana can be used to access these metrics collected by Prometheus. Prometheus and Grafana are additional components Verrazzano creates as a result of applying an application configuration.\nDetermine the URL to access Grafana:\n$ GRAFANA_HOST=$(kubectl get ingress -n verrazzano-system vmi-system-grafana -o jsonpath='{.spec.rules[0].host}') $ GRAFANA_URL=\"https://${GRAFANA_HOST}\" $ echo \"${GRAFANA_URL}\" $ open \"${GRAFANA_URL}\" The user name to access Grafana is set to the default value verrazzano during the Verrazzano installation.\nDetermine the password to access Grafana:\n$ echo $(kubectl get secret -n verrazzano-system verrazzano -o jsonpath={.data.password} | base64 --decode) Alternatively, metrics can be accessed directly using Prometheus. Determine the URL for this access:\n$ PROMETHEUS_HOST=$(kubectl get ingress -n verrazzano-system vmi-system-prometheus -o jsonpath='{.spec.rules[0].host}') $ PROMETHEUS_URL=\"https://${PROMETHEUS_HOST}\" $ echo \"${PROMETHEUS_URL}\" $ open \"${PROMETHEUS_URL}\" The user name and password for both Prometheus and Grafana are the same.\nRemove the application Run the following commands to delete the application configuration, and optionally the component and namespace.\n  Delete the application configuration.\n$ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-app.yaml The deletion of the application configuration will result in the destruction of all application-specific Kubernetes objects.\n  (Optional) Delete the application’s component.\n$ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-comp.yaml Note: This step is not required if other application configurations for this component will be applied in the future.\n  (Optional) Delete the namespace.\n$ kubectl delete namespace hello-helidon   ","excerpt":"Overview Developing and deploying an application to Verrazzano consists of:\n Packaging the application as a Docker image. Publishing the application’s Docker image to a container registry. Applying …","ref":"/docs/guides/application-deployment-guide/","title":"Application Deployment Guide"},{"body":"","excerpt":"","ref":"/docs/guides/","title":"Guides"},{"body":"","excerpt":"","ref":"/docs/setup/platforms/","title":"Platform Setup"},{"body":" NOTE You should install this developer preview release of Verrazzano only in a cluster that can be safely deleted when your evaluation is complete.  The following instructions show you how to install Verrazzano in a single Kubernetes cluster.\nPrerequisites Verrazzano requires the following:\n A Kubernetes cluster and a compatible kubectl. At least 2 CPUs, 100GB disk storage, and 16GB RAM available on the Kubernetes worker nodes. This is sufficient to install the development profile of Verrazzano. Depending on the resource requirements of the applications you deploy, this may or may not be sufficient for deploying your applications.  For a list of the open source components and versions installed with Verrazzano, see Software Versions.\nNOTE Verrazzano has been tested only on the following versions of Kubernetes: 1.17.x and 1.18.x. Other versions have not been tested and are not guaranteed to work.  Prepare for the install Before installing Verrazzano, see instructions on preparing the following Kubernetes platforms:\n  OCI Container Engine for Kubernetes\n  OLCNE\n  KIND\n  minikube\n  Generic Kubernetes\n  Install the Verrazzano platform operator Verrazzano provides a platform operator to manage the life cycle of Verrazzano installations. You can install, uninstall, and update Verrazzano installations by updating the Verrazzano custom resource.\nTo install the Verrazzano platform operator:\n  Deploy the Verrazzano platform operator.\n$ kubectl apply -f https://github.com/verrazzano/verrazzano/releases/latest/download/operator.yaml   Wait for the deployment to complete.\n$ kubectl -n verrazzano-install rollout status deployment/verrazzano-platform-operator deployment \"verrazzano-platform-operator\" successfully rolled out   Confirm that the operator pod is correctly defined and running.\n$ kubectl -n verrazzano-install get pods NAME READY STATUS RESTARTS AGE verrazzano-platform-operator-59d5c585fd-lwhsx 1/1 Running 0 114s   Perform the install Verrazzano supports two installation profiles: development (dev) and production (prod). The production profile, which is the default, provides a 3-node Elasticsearch and persistent storage for the Verrazzano Monitoring Instance (VMI). The development profile provides a single node Elasticsearch and no persistent storage for the VMI. To change profiles in any of the following commands, set the VZ_PROFILE environment variable to the name of the profile you want to install.\nNOTE For Verrazzano installations on the minikube platform, use only the development profile.  For a complete description of Verrazzano configuration options, see the Verrazzano Custom Resource Definition.\nAccording to your DNS choice, xip.io or Oracle OCI DNS, install Verrazzano using one of the following methods:\n xip.io OCI DNS   Install using xip.io\nRun the following commands:\n$ kubectl apply -f - \u003c\u003cEOF apiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: my-verrazzano spec: profile: ${VZ_PROFILE:-dev} EOF $ kubectl wait --timeout=20m --for=condition=InstallComplete verrazzano/my-verrazzano   Install using OCI DNS\nPrerequisites\n  A DNS zone is a distinct portion of a domain namespace. Therefore, ensure that the zone is appropriately associated with a parent domain. For example, an appropriate zone name for parent domain v8o.example.com domain is us.v8o.example.com.\n  Create a public OCI DNS zone using the OCI CLI or the OCI Console.\nTo create an OCI DNS zone using the OCI CLI:\n$ oci dns zone create -c \u003ccompartment ocid\u003e --name \u003czone-name-prefix\u003e.v8o.example.com --zone-type PRIMARY To create an OCI DNS zone using the OCI console, see Managing DNS Service Zones.\n  Create a secret in the default namespace. The secret is created using the script create_oci_config_secret.sh which reads an OCI configuration file to create the secret.\nDownload the create_oci_config_secret.sh script:\n$ curl -o ./create_oci_config_secret.sh https://raw.githubusercontent.com/verrazzano/verrazzano/master/platform-operator/scripts/install/create_oci_config_secret.sh Run the create_oci_config_secret.sh script:\n$ chmod +x create_oci_config_secret.sh $ export KUBECONFIG=\u003ckubeconfig-file\u003e $ ./create_oci_config_secret.sh -o \u003coci-config-file\u003e -s \u003cconfig-file-section\u003e -k \u003csecret-name\u003e -o defaults to the OCI configuration file in ~/.oci/config -s defaults to the DEFAULT properties section within the OCI configuration file -k defaults to a secret named oci   NOTE The key_file value within the OCI configuration file must reference a .pem file that contains a RSA private key. The contents of a RSA private key file starts with -----BEGIN RSA PRIVATE KEY-----. If your OCI configuration file references a .pem file that is not of this form, then you must generate a RSA private key file. See Generating a RSA Private Key. After generating the correct form of the .pem file, make sure to change the reference within the OCI configuration file.  Installation\nInstalling Verrazzano using OCI DNS requires some configuration settings to create DNS records.\nDownload the sample Verrazzano custom resource install-oci.yaml for OCI DNS:\n$ curl -o ./install-oci.yaml https://raw.githubusercontent.com/verrazzano/verrazzano/master/platform-operator/config/samples/install-oci.yaml Edit the downloaded install-oci.yaml file and provide values for the following configuration settings:\n spec.environmentName spec.certificate.acme.emailAddress spec.dns.oci.ociConfigSecret spec.dns.oci.dnsZoneCompartmentOCID spec.dns.oci.dnsZoneOCID spec.dns.oci.dnsZoneName  For the full configuration information for an installation, see the Verrazzano Custom Resource Definition.\nWhen you use the OCI DNS installation, you need to provide a Verrazzano name in the Verrazzano custom resource (spec.environmentName) that will be used as part of the domain name used to access Verrazzano ingresses. For example, you could use sales as an environmentName, yielding sales.us.v8o.example.com as the sales-related domain (assuming the domain and zone names listed previously).\nRun the following commands:\n$ kubectl apply -f ./install-oci.yaml $ kubectl wait --timeout=20m --for=condition=InstallComplete verrazzano/my-verrazzano    To monitor the console log output of the installation:\n$ kubectl logs -f $(kubectl get pod -l job-name=verrazzano-install-my-verrazzano -o jsonpath=\"{.items[0].metadata.name}\") Verify the install Verrazzano installs multiple objects in multiple namespaces. In the verrazzano-system namespaces, all the pods in the Running state, does not guarantee, but likely indicates that Verrazzano is up and running.\n$ kubectl get pods -n verrazzano-system coherence-operator-controller-manager-684d7bddf6-8l9s2 1/1 Running 0 52m oam-kubernetes-runtime-76cbbb969f-97lhm 1/1 Running 0 52m verrazzano-api-f89cdd678-lfr2t 1/1 Running 0 52m verrazzano-application-operator-7b554ff955-ms9pp 1/1 Running 0 51m verrazzano-console-6488fbfd45-8csm2 1/1 Running 0 52m verrazzano-monitoring-operator-74c6c956fb-r4zw5 1/1 Running 0 52m verrazzano-operator-84b8c677ff-2pz2k 1/1 Running 0 52m vmi-system-api-f7577d8-c7zmq 1/1 Running 0 52m vmi-system-es-master-0 2/2 Running 0 52m vmi-system-grafana-6f4bd5d964-74q2z 2/2 Running 0 52m vmi-system-kibana-8687b8f754-hr7kt 2/2 Running 0 52m vmi-system-prometheus-0-649b67bd8c-dm97k 5/5 Running 0 52m vmi-system-prometheus-gw-6bb6b68b98-xpk65 1/1 Running 0 52m weblogic-operator-5d7579db46-qlxds 1/1 Running 0 52m (Optional) Install the example applications Example applications are located here.\nTo get the consoles URLs and credentials, see Operations. Uninstall Verrazzano To delete a Verrazzano installations:\n# Get the name of the Verrazzano custom resource $ kubectl get verrazzano # Delete the Verrazzano custom resource $ kubectl delete verrazzano \u003cname of custom resource\u003e To monitor the console log of the uninstall:\n$ kubectl logs -f $(kubectl get pod -l job-name=verrazzano-uninstall-my-verrazzano -o jsonpath=\"{.items[0].metadata.name}\") ","excerpt":" NOTE You should install this developer preview release of Verrazzano only in a cluster that can be safely deleted when your evaluation is complete.  The following instructions show you how to install …","ref":"/docs/setup/install/installation/","title":"Installation Guide"},{"body":"This guide describes how to move (“Lift-and-Shift”) an on-premises WebLogic Server domain to a cloud environment running Kubernetes using Verrazzano.\nOverview The Initial steps create a very simple on-premises domain that you will move to Kubernetes. The sample domain is the starting point for the lift and shift process; it contains one application (ToDo List) and one data source. First, you’ll configure the database and the WebLogic Server domain. Then, in Lift and Shift, you will move the domain to Kubernetes with Verrazzano. This guide does not include the setup of the networking that would be needed to access an on-premises database, nor does it document how to migrate a database to the cloud.\nWhat you need   The Git command-line tool and access to GitHub\n  MySQL Database 8.x - a database server\n  WebLogic Server 12.2.1.4.0 - an application server; Note that all WebLogic Server installers are supported except the Quick Installer.\n  Maven - to build the application\n  WebLogic Deploy Tooling (WDT) - v1.9.9 or later, to convert the WebLogic Server domain to and from metadata\n  WebLogic Image Tool (WIT) - v1.9.8 or later, to build the Docker image\n  Initial steps In the initial steps, you create a sample domain that represents your on-premises WebLogic Server domain.\nCreate a database using MySQL called tododb   Download the MySQL image from Docker Hub.\n$ docker pull mysql:latest   Start the container database (and optionally mount a volume for data).\n$ docker run --name tododb \\ -p 3306:3306 \\ -e MYSQL_USER=derek \\ -e MYSQL_PASSWORD=welcome1 \\ -e MYSQL_DATABASE=tododb \\ -e MYSQL_ROOT_PASSWORD=welcome1 \\ -d mysql:latest  NOTE You should use a more secure password.    Start a MySQL client to change the password algorithm to mysql_native_password.\n Assuming the database server is running, start a database CLI client. $ docker exec -it tododb mysql -uroot -p  When prompted for the password, enter the password for the root user, welcome1 or whatever password you set when starting the container in the previous step. After being connected, run the ALTER command at the MySQL prompt. $ ALTER USER 'derek'@'%' IDENTIFIED WITH mysql_native_password BY 'welcome1';   NOTE You should use a more secure password.    Create a WebLogic Server domain   If you do not have WebLogic Server 12.2.1.4.0 installed, install it now.\n  Choose the GENERIC installer from WebLogic Server Downloads and follow the documented installation instructions.\n  Be aware of these domain limitations:\n There are two supported domain types, single server and single cluster. Domains must use:  The default value AdminServer for AdminServerName. WebLogic Server listen port for the Administration Server: 7001. WebLogic Server listen port for the Managed Server: 8001. Note that these are all standard WebLogic Server default values.      Save the installer after you have finished; you will need it to build the Docker image.\n  To make copying commands easier, define an environment variable for ORACLE_HOME that points to the directory where you installed WebLogic Server 12.2.1.4.0. For example:\n$ export ORACLE_HOME=$HOME/Oracle/Middleware/Oracle_Home     Use the Oracle WebLogic Server Configuration Wizard to create a domain called tododomain.\n Launch $ORACLE_HOME/oracle_common/common/bin/config.sh. Select Create a new domain. Specify a Domain Location of \u003coracle home\u003e/user_projects/domains/tododomain and click Next. Select the Basic WebLogic Server Domain [wlserver] template and click Next. Enter the password for the administrative user (the examples here assume a password of “welcome1”) and click Next. Accept the defaults for Domain Mode and JDK, and click Next. Select Administration Server and click Next. Ensure that the server name is AdminServer and click Next. Click Create. After it has completed, click Next, then Finish.    To start the newly created domain, run the domain’s start script.\n$ $ORACLE_HOME/user_projects/domains/tododomain/bin/startWebLogic.sh   Access the Console of the newly started domain with your browser, for example, http://localhost:7001/console, and log in using the administrator credentials you specified.\n  Add a data source configuration to access the database Using the WebLogic Server Administration Console, log in and add a data source configuration to access the MySQL database. During the data source configuration, you can accept the default values for most fields, but the following fields are required to match the application and database settings you used when you created the MySQL database.\n  In the left pane in the Console, expand Services and select Data Sources.\n  On the Summary of JDBC Data Sources page, click New and select Generic Data Source.\n  On the JDBC Data Sources page, enter or select the following information:\n Name: tododb JNDI Name: jdbc/ToDoDB Database Type: MySQL    Click Next and then click Next two more times.\n  On the Create a New JDBC Data Source page, enter the following information:\n Database Name: tododb Host name: localhost Database Port: 3306 Database User Name: derek Password: welcome1 (or whatever password you used) Confirm Password: welcome1    Click Next.\n  Select Test Configuration, and make sure you see “Connection Test Succeeded” in the Messages field of the Console.\n  Click Next.\n  On the Select Targets page, select AdminServer.\n  Click Finish to complete the configuration.\n  Build and deploy the application   Using Maven, build this project to produce todo.war.\nNOTE: You should clone this repo outside of $ORACLE_HOME or copy the WAR file to another location, as WDT may ignore it during the model creation phase.\n$ git clone https://github.com/verrazzano/examples.git $ cd examples/todo-list/ $ mvn clean package   Using the WebLogic Server Administration Console, deploy the ToDo List application.\n In the left pane in the Console, select Deployments and click Install. Use the navigation links or provide the file path to todo.war typically \u003crepo\u003e/todo-list/target. For example, if you cloned the examples repository in your $HOME directory, the location should be $HOME/examples/examples/todo-list/target/todo.war. Click Next twice, then Finish.  NOTE: The remaining steps assume that the application context is todo.\n  Initialize the database After the application is deployed and running in WebLogic Server, access the http://localhost:7001/todo/rest/items/init REST service to create the database table used by the application. In addition to creating the application table, the init service also will load four sample items into the table.\nIf you get an error here, go back to the Select Targets page in the WebLogic Server Administration Console and make sure that you selected AdminServer as the data source target.\nAccess the application  Access the application at http://localhost:7001/todo/index.html.   Add a few entries or delete some. After verifying the application and database, you may shut down the local WebLogic Server domain.  Lift and Shift steps The following steps will move the sample domain to Kubernetes with Verrazzano.\nCreate a WDT Model  If you have not already done so, download v1.9.9 or later of WebLogic Deploy Tooling (WDT) from GitHub. Unzip the installer weblogic-deploy.zip file so that you can access bin/discoverDomain.sh. To make copying commands easier, define an environment variable for WDT_HOME that points to the directory where you installed WebLogic Deploy Tooling. $ export WDT_HOME=/install/directory   For example, to get the latest version:\n$ curl -OL https://github.com/oracle/weblogic-deploy-tooling/releases/latest/download/weblogic-deploy.zip $ unzip weblogic-deploy.zip $ cd weblogic-deploy $ export WDT_HOME=$(pwd) To create a reusable model of the application and domain, use WDT to create a metadata model of the domain.\n First, create an output directory to hold the generated scripts and models. Then, run WDT discoverDomain. $ mkdir v8o $ $WDT_HOME/bin/discoverDomain.sh \\  -oracle_home $ORACLE_HOME \\  -domain_home /path/to/domain/dir \\  -model_file ./v8o/wdt-model.yaml \\  -archive_file ./v8o/wdt-archive.zip \\  -target vz \\  -output_dir v8o   You will find the following files in ./v8o:\n application.yaml - Verrazzano application configuration and component file; you can view a sample generated file here wdt-archive.zip - The WDT archive file containing the ToDo List application WAR file wdt-model.yaml - The WDT model of the WebLogic Server domain vz_variable.properties - A set of properties extracted from the WDT domain model create_k8s_secrets.sh - A helper script with kubectl commands to apply the Kubernetes secrets needed for this domain  NOTE: Due to a bug in WDT v1.9.9, you need to make the following edits (preferably using an editor like vi) to the generated application.yaml file:\n  Delete the line between the copyright headers and the first apiVersion.\n  Delete the empty clusters field from the tododomain Domain component.\n  If you chose to skip the Access the application step and did not verify that the ToDo List application was deployed, then you should verify that you see the todo.war file inside the wdt-archive.zip file. If you do not see the WAR file, there was something wrong in your deployment of the application on WebLogic Server that will require additional troubleshooting in your domain.\nCreate a Docker image At this point, the Verrazzano model is just a template for the real model. The WebLogic Image Tool will fill in the placeholders for you, or you can edit the model manually to set the image name and domain home directory.\n If you have not already done so, download WebLogic Image Tool (WIT) from GitHub. Unzip the installer imagetool.zip file so that you can access bin/imagetool.sh. To make copying commands easier, define an environment variable for WIT_HOME that points to the directory where you installed WebLogic Image Tool. $ export WIT_HOME=/install/directory   For example, to get the latest WIT tool:\n$ curl -OL https://github.com/oracle/weblogic-image-tool/releases/latest/download/imagetool.zip $ unzip imagetool.zip $ cd imagetool $ export WIT_HOME=$(pwd) You will need a Docker image to run your WebLogic Server domain in Kubernetes. To use WIT to create the Docker image, run imagetool create. Although WIT will download patches and PSUs for you, it does not yet download installers. Until then, you must download the WebLogic Server and Java Development Kit installer manually and provide their location to the imagetool cache addInstaller command.\n# The directory created previously to hold the generated scripts and models. $ cd v8o $ $WIT_HOME/bin/imagetool.sh cache addInstaller \\  --path /path/to/installer/jdk-8u231-linux-x64.tar.gz \\  --type jdk \\  --version 8u231 # The installer file name may be slightly different depending on # which version of the 12.2.1.4.0 installer that you downloaded, slim or generic. $ $WIT_HOME/bin/imagetool.sh cache addInstaller \\  --path /path/to/installer/fmw_12.2.1.4.0_wls_Disk1_1of1.zip \\  --type wls \\  --version 12.2.1.4.0 $ $WIT_HOME/bin/imagetool.sh cache addInstaller \\  --path /path/to/installer/weblogic-deploy.zip \\  --type wdt \\  --version latest # Paths for the files in this command assume that you are running it from the # v8o directory created during the `discoverDomain` step. $ $WIT_HOME/bin/imagetool.sh create \\  --tag your/repo/todo:1 \\  --version 12.2.1.4.0 \\  --jdkVersion 8u231 \\  --wdtModel ./wdt-model.yaml \\  --wdtArchive ./wdt-archive.zip \\  --wdtVariables ./vz_variable.properties \\  --resourceTemplates=./application.yaml \\  --wdtModelOnly The imagetool create command will have created a local Docker image and updated the Verrazzano model with the domain home and image name. Check your Docker images for the tag that you used in the create command using docker images from the Docker CLI.\nIf everything worked correctly, it is time to push that image to the container registry that Verrazzano will use to access the image from Kubernetes. You can use the Oracle Cloud Infrastructure Registry (OCIR) as your repository for this example, but most Docker compliant registries should work.\nThe variables in the application.yaml resource template should be resolved with information from the image tool build.\nVerify this by looking in the v8o/application.yaml file to make sure that the image: {{{imageName}}} value has been set with the given --tag value.\nPush the image to your repo.\nNOTE: The image name must be the same as what is in the application.yaml file under spec \u003e workload \u003e spec \u003e image for the tododomain-domain component.\n$ docker push your/repo/todo:1 Deploy to Verrazzano After the application image has been created, there are several steps required to deploy a the application into a Verrazzano environment.\nThese include:\n Creating and labeling the tododomain namespace. Creating the necessary secrets required by the ToDo List application. Deploying MySQL to the tododomain namespace. Updating the application.yaml file to use the Verrazzano MySQL deployment and (optionally) expose the WLS Console. Applying the application.yaml file.  The following steps assume that you have a Kubernetes cluster and that Verrazzano is already installed in that cluster.\nLabel the namespace Create the tododomain namespace, and add labels to allow the WebLogic Server Kubernetes Operator to manage it and enabled for Istio.\n$ kubectl create namespace tododomain $ kubectl label namespace tododomain verrazzano-managed=true istio-injection=enabled Create the required secrets If you haven’t already done so, edit and run the create_k8s_secrets.sh script to generate the Kubernetes secrets. WDT does not discover passwords from your existing domain. Before running the create secrets script, you will need to edit create_k8s_secrets.sh to set the passwords for the WebLogic Server domain and the data source. In this domain, there are a few passwords that you need to enter:\n Administrator credentials (for example, weblogic/welcome1) ToDo database credentials (for example, derek/welcome1) Runtime encryption secret (for example, welcome1)  For example:\n# Update \u003cadmin-user\u003e and \u003cadmin-password\u003e for weblogic-credentials $ create_paired_k8s_secret weblogic-credentials weblogic welcome1 # Update \u003cuser\u003e and \u003cpassword\u003e for tododomain-jdbc-tododb $ create_paired_k8s_secret jdbc-tododb derek welcome1 # Update \u003cpassword\u003e used to encrypt hashes $ create_k8s_secret runtime-encryption-secret welcome1 Then run the script:\n$ sh ./create_k8s_secrets.sh Verrazzano will need a credential to pull the image that you just created, so you need to create one more secret. The name for this credential can be changed in the component.yaml file to anything you like, but it defaults to tododomain-registry-credentials.\nAssuming that you leave the name tododomain-registry-credentials, you will need to run a kubectl create secret command similar to the following:\n$ kubectl create secret docker-registry tododomain-registry-credentials \\  --docker-server=phx.ocir.io \\  --docker-email=your.name@company.com \\  --docker-username=tenancy/username \\  --docker-password='passwordForUsername' \\  --namespace=tododomain Update the application configuration Update the generated application.yaml file for the todo application to:\n Update the tododomain-configmap component to use the in-cluster MySQL service URL jdbc:mysql://mysql.tododomain.svc.cluster.local:3306/tododb to access the database.  wdt_jdbc.yaml:| resources:JDBCSystemResource:'todo-ds':JdbcResource:JDBCDriverParams:# This is the URL of the database used by the WebLogic Server applicationURL:\"jdbc:mysql://mysql.tododomain.svc.cluster.local:3306/tododb\" (Optional) Add a path in the tododomain-domain IngressTrait to allow access to the WebLogic Server Administration Console.  # WLS console- path:\"/console\"pathType:PrefixThe file application-modified.yaml is an example of a modified application.yaml file. A diff of these two sample files is shown:\n$ diff application.yaml application-modified.yaml 27a28,30 \u003e # WLS console \u003e - path: \"/console\" \u003e pathType: Prefix 105c108 \u003c URL: \"jdbc:mysql://localhost:3306/tododb\" --- \u003e URL: \"jdbc:mysql://mysql.tododomain.svc.cluster.local:3306/tododb\" Deploy MySQL As noted previously, moving a production environment to Verrazzano would require migrating the data as well. While data migration is beyond the scope of this guide, we will still need to include a MySQL instance to be deployed with the application in the Verrazzano environment.\nTo do so, download the mysql-oam.yaml file.\nThen, apply the YAML file:\n$ kubectl apply -f mysql-oam.yaml Wait for the MySQL pod to reach the Ready state.\n$ kubectl get pod -n tododomain -w NAME READY STATUS RESTARTS AGE mysql-5cfd58477b-mg5c7 0/1 Pending 0 0s mysql-5cfd58477b-mg5c7 0/1 Pending 0 0s mysql-5cfd58477b-mg5c7 0/1 ContainerCreating 0 0s mysql-5cfd58477b-mg5c7 1/1 Running 0 2s Deploy the ToDo List application Finally, run kubectl apply to apply the Verrazzano component and Verrazzano application configuration files to start your domain.\n$ kubectl apply -f application.yaml This will:\n Create the application component resources for the ToDo List application. Create the application configuration resources that create the instance of the ToDo List application in the Verrazzano cluster.  Wait for the ToDo List example application to be ready.\n$ kubectl wait pod --for=condition=Ready tododomain-adminserver -n tododomain pod/tododomain-adminserver condition met Verify the pods are in the Running state:\n$ kubectl get pod -n tododomain NAME READY STATUS RESTARTS AGE mysql-55bb4c4565-c8zf5 1/1 Running 0 8m tododomain-adminserver 2/2 Running 0 5m Access the application from your browser   Get the EXTERNAL_IP address of the istio-ingressgateway service.\n$ kubectl get service istio-ingressgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.96.97.98 11.22.33.44 80:31380/TCP,443:31390/TCP 13d The IP address is listed in the EXTERNAL-IP column.\n  Add an entry to /etc/hosts for the application hostname for the ingress gateway external IP.\nTemporarily modify the /etc/hosts file (on Mac or Linux) or c:\\Windows\\System32\\Drivers\\etc\\hosts file (on Windows 10), to add an entry mapping todo.example.com to the ingress gateway’s EXTERNAL-IP address.\nFor example:\n11.22.33.44 tododomain-appconf.tododomain.example.com   Initialize the database by accessing the init URL.\n$ curl http://tododomain-appconf.tododomain.example.com/todo/rest/items/init ToDos table initialized.   Access the application in a browser at http://tododomain-appconf.tododomain.example.com/todo.\n  (Optional) Access the WebLogic Server Administration Console at http://tododomain-appconf.tododomain.example.com/console.\n  ","excerpt":"This guide describes how to move (“Lift-and-Shift”) an on-premises WebLogic Server domain to a cloud environment running Kubernetes using Verrazzano.\nOverview The Initial steps create a very simple …","ref":"/docs/guides/lift-and-shift/","title":"Lift-and-Shift Guide"},{"body":"Prepare for the OCI install   Create the OKE cluster using the OCI Console or some other means.\n  For SHAPE, an OKE cluster with 3 nodes of VM.Standard2.4 OCI compute instance shape has proven sufficient to install Verrazzano and deploy the Bob’s Books example application.\n  Set the following ENV variable:\n   $ export KUBECONFIG=\u003cpath to valid Kubernetes config\u003e  Create the optional imagePullSecret named verrazzano-container-registry. This step is required when one or more of the Docker images installed by Verrazzano are private. For example, while testing a change to the verrazzano-operator, you may be using a Docker image that requires credentials to access it.   $ kubectl create secret docker-registry verrazzano-container-registry \\ --docker-username=\u003cusername\u003e \\ --docker-password=\u003cpassword\u003e \\ --docker-server=\u003cdocker server\u003e Next steps To continue, see the Installation Guide.\n","excerpt":"Prepare for the OCI install   Create the OKE cluster using the OCI Console or some other means.\n  For SHAPE, an OKE cluster with 3 nodes of VM.Standard2.4 OCI compute instance shape has proven …","ref":"/docs/setup/platforms/oci/oci/","title":"Oracle Cloud Infrastructure (OCI)"},{"body":"","excerpt":"","ref":"/docs/reference/","title":"Reference"},{"body":"v0.13.0 Features:\n IngressTrait support for explicit destination host and port. Experimental cluster diagnostic tooling. Grafana dashboards for VerrazzanoHelidonWorkload. Now you can update application Fluentd sidecar images following a Verrazzano update. Documented Verrazzano specific OAM workload resources. Documented Verrazzano hardware requirements and installed software versions.  Fixes:\n VerrazzanoWebLogicWorkload and VerrazzanoCoherenceWorkload resources now handle updates. Now VerrazzanoHelidonWorkload supports the use of the ManualScalarTrait. Now you can delete a Namespace containing an ApplicationConfiguration resource. Fixed frequent restarts of Prometheus during application deployment. Made verrazzano-application-operator logging more useful and use structured logging. Fixed Verrazzano uninstall issues.  v0.12.0 Features:\n Observability stack now uses Keycloak SSO for authentication. Istio sidecars now automatically injected when namespaces labeled istio-injection=enabled. Support for Helidon applications now defined using VerrazzanoHelidonWorkload type.  Fixes:\n Fixed issues where logs were not captured from all containers in workloads with multiple containers. Fixed issue where some resources were not cleaned up during uninstall.  v0.11.0 Features:\n OAM applications are optionally deployed into an Istio service mesh. Incremental improvements to user-facing roles.  Fixes:\n Fixed issue with logging when an application has multiple workload types. Fixed metrics configuration in Spring Boot example application.  v0.10.0 Breaking Changes:\n Model/binding files removed; now application deployment done exclusively by using Open Application Model (OAM). Syntax changes for WebLogic and Coherence OAM workloads, now defined using VerrazzanoCoherenceWorkload and VerrazzanoWebLogicWorkload types.  Features:\n By default, application endpoints now use HTTPs - when using magic DNS, certificates are issued by cluster issuer, when using OCI DNS certificates are issued using Let’s Encrypt, or the end user can provide certificates. Updated Coherence operator to 3.1.3. Updates for running Verrazzano on Kubernetes 1.19 and 1.20. RBAC roles and role bindings created at install time. Added instance information to status of Verrazzano custom resource; can be used to obtain instance URLs. Upgraded Istio to v1.7.3.  Fixes:\n Reduced log level of Elasticsearch; excessive logging could have resulted in filling up disks.  v0.9.0  Features:  Added platform support for installing Verrazzano on Kind clusters. Log records are indexed from the OAM appconfig and component definitions using the following pattern: namespace-appconfig-component. All system and curated components are now patchable. More updates to Open Application Model (OAM) support.    To enable OAM, when you install Verrazzano, specify the following in the Kubernetes manifest file for the Verrazzano custom resource:\nspec: oam: enabled: true v0.8.0  Features:  Support for two installation profiles, development (dev) and production (prod). The production profile, which is the default, provides a 3-node Elasticsearch and persistent storage for the Verrazzano Monitoring Instance (VMI). The development profile provides a single node Elasticsearch and no persistent storage for the VMI. The default behavior has been changed to use the system VMI for all monitoring (applications and Verrazzano components). It is still possible to customize one of the profiles to enable the original, non-shared VMI mode. Initial support for the Open Application Model (OAM).   Fixes:  Updated Axios NPM package to v0.21.1 to resolve a security vulnerability in the examples code.    v.0.7.0   Features:\n Ability to upgrade an existing Verrazzano installation. Added the Verrazzano Console. Enhanced the structure of the Verrazzano custom resource to allow more configurability. Streamlined the secret usage for OCI DNS installations.    Fixes:\n Fixed bug where the Verrazzano CR Certificate.CA fields were being ignored. Removed secret used for hello-world; hello-world-application image is now public in ghcr so ImagePullSecrets is no longer needed. Fixed issue #339 (PRs #208 \u0026 #210.)    v0.6.0  Features:  In-cluster installer which replaces client-side install scripts. Added installation profiles; in this release, there are two: production and development. Verrazzano system components now emit JSON structured logs.   Fixes:  Updated Elasticsearch and Kibana versions (elasticsearch:7.6.1-20201130145440-5c76ab1) and (kibana:7.6.1-20201130145840-7717e73).    ","excerpt":"v0.13.0 Features:\n IngressTrait support for explicit destination host and port. Experimental cluster diagnostic tooling. Grafana dashboards for VerrazzanoHelidonWorkload. Now you can update …","ref":"/docs/releasenotes/","title":"Release Notes"},{"body":"Verrazzano installs a curated set of open source components. This section lists each open source component with its version and a brief description.\n   Component Version Description     cert-manager 0.13.1 Automates the management and issuance of TLS certificates.   Coherence Operator 3.1.3 Assists with deploying and managing Coherence clusters.   Elasticsearch 7.6.1 Provides a distributed, multitenant-capable full-text search engine.   ExternalDNS 0.7.1 Synchronizes exposed Kubernetes Services and Ingresses with DNS providers.   Filebeat 6.8.3 Collects container log events and forwards them to Elasticsearch.   Grafana 6.4.4 Tool to help you study, analyze, and monitor metrics.   Istio 1.7.3 Service mesh that layers transparently onto existing distributed applications.   Journalbeat 6.8.3 Collects systemd log events and forwards them to Elasticsearch.   Keycloak 10.0.1 Provides single sign-on with Identity and Access Management.   Kibana 7.6.1 Provides search and data visualization capabilities for data indexed in Elasticsearch.   MySQL 8.0.20 Open source relational database management system used by Keycloak.   NGINX Ingress Controller 0.32.0 Traffic management solution for cloud‑native applications in Kubernetes.   Node Exporter 0.18.1 Prometheus exporter for hardware and OS metrics.   OAM Kubernetes Runtime 0.3.0 Plug-in for implementing Open Application Model (OAM) control plane with Kubernetes.   Prometheus 2.13.1 Provides event monitoring and alerting.   Rancher 2.5.7 Manages multiple Kubernetes clusters.   WebLogic Server Kubernetes Operator 3.1.0 Assists with deploying and managing WebLogic domains.    ","excerpt":"Verrazzano installs a curated set of open source components. This section lists each open source component with its version and a brief description.\n   Component Version Description     cert-manager …","ref":"/docs/setup/versions/versions/","title":"Software Versions"},{"body":"Prepare for the OCLNE install Oracle Linux Cloud Native Environment can be installed in several different types of environments. These range from physical, on-premises hardware to virtualized cloud infrastructure. The Oracle Linux Cloud Native Environment installation instructions assume that networking and compute resources already exist. The basic infrastructure requirements are a network with a public and private subnet and a set of hosts connected to those networks.\nOCI example The following is an example of OCI infrastructure that can be used to evaluate Verrazzano installed on Oracle Linux Cloud Native Environment. If other environments are used, the capacity and configuration should be similar.\nYou can use the VCN Wizard of the OCI Console to automatically create most of the described network infrastructure. Additional security lists/rules, as detailed below, need to be added manually. All CIDR values provided are examples and can be customized as required.\nVirtual Cloud Network (for example, CIDR 10.0.0.0/16) Public Subnet (for example, CIDR 10.0.0.0/24)\nSecurity List / Ingress Rules\n   Stateless Destination Protocol Source Ports Destination Ports Type \u0026 Code Description     No 0.0.0.0/0 ICMP   3, 4 ICMP errors   No 10.0.0.0/16 ICMP   3 ICMP errors   No 0.0.0.0/0 TCP All 22  SSH   No 0.0.0.0/0 TCP All 80  HTTP load balancer   No 0.0.0.0/0 TCP All 443  HTTPS load balancer    Security List / Egress Rules\n   Stateless Destination Protocol Source Ports Destination Ports Type \u0026 Code Description     No 10.0.1.0/24 TCP All 22  SSH   No 10.0.1.0/24 TCP All 30080  HTTP load balancer   No 10.0.1.0/24 TCP All 30443  HTTPS load balancer   No 10.0.1.0/24 TCP All 31380  HTTP load balancer   No 10.0.1.0/24 TCP All 31390  HTTPS load balancer    Private Subnet (for example, CIDR 10.0.1.0/24)\nSecurity List / Ingress Rules\n   Stateless Destination Protocol Source Ports Destination Ports Type \u0026 Code Description     No 0.0.0.0/0 ICMP   3, 4 ICMP errors   No 10.0.0.0/16 ICMP   3 ICMP errors   No 10.0.0.0/16 TCP All 22  SSH   No 10.0.0.0/24 TCP All 30080  HTTP load balancer   No 10.0.0.0/24 TCP All 30443  HTTPS load balancer   No 10.0.0.0/24 TCP All 31380  HTTP load balancer   No 10.0.0.0/24 TCP All 31390  HTTPS load balancer   No 10.0.1.0/24UDP All 111  NFS    No 10.0.1.0/24 TCP All 111  NFS   No 10.0.1.0/24 UDP All 2048  NFS   No 10.0.1.0/24 TCP All 2048-2050  NFS   No 10.0.1.0/24 TCP All 2379-2380  Kubernetes etcd   No 10.0.1.0/24 TCP All 6443  Kubernetes API Server   No 10.0.1.0/24 TCP All 6446  MySQL   No 10.0.1.0/24 TCP All 8090-8091  OLCNE Platform Agent   No 10.0.1.0/24 UDP All 8472  Flannel   No 10.0.1.0/24 TCP All 10250-10255  Kubernetes Kublet    Security List / Egress Rules\n   Stateless Destination Protocol Source Ports Destination Ports Type and Code Description     No 10.0.0.0/0 TCP    All egress traffic    DHCP Options\n   DNS Type     Internet and VCN Resolver    Route Tables\nPublic Subnet Route Table Rules\n   Destination Target     0.0.0.0/0 Internet Gateway    Private Subnet Route Table Rules\n   Destination Target     0.0.0.0/0 NAT Gateway   All OCI Services Service Gateway    Internet Gateway\nNAT Gateway\nService Gateway\nThe following compute resources adhere to the guidelines provided in the Oracle Linux Cloud Native Environment Getting Started guide. The attributes indicated (for example, Subnet, RAM, Shape, and Image) are recommendations that have been tested. Other values can be used if required.\nCompute Instances\n   Role Subnet Suggested RAM Compatible VM Shape Compatible VM Image     SSH Jump Host Public 8GB VM.Standard.E2.1 Oracle Linux 7.8   OLCNE Operator Host Private 16GB VM.Standard.E2.2 Oracle Linux 7.8   Kubernetes Control Plane Node Private 32GB VM.Standard.E2.4 Oracle Linux 7.8   Kubernetes Worker Node 1 Private 32GB VM.Standard.E2.4 Oracle Linux 7.8   Kubernetes Worker Node 2 Private 32GB VM.Standard.E2.4 Oracle Linux 7.8   Kubernetes Worker Node 3 Private 32GB VM.Standard.E2.4 Oracle Linux 7.8    Do the OLCNE install Deploy Oracle Linux Cloud Native Environment with the Kubernetes module, following instructions from the Getting Started guide.\n Use a single Kubernetes control plane node. Skip the Kubernetes API load balancer (3.4.3). Use private CA certificates (3.5.3).  Prepare for the Verrazzano install A Verrazzano Oracle Linux Cloud Native Environment deployment requires:\n A default storage provider that supports “Multiple Read/Write” mounts. For example, an NFS service like:  Oracle Cloud Infrastructure File Storage Service. A hardware-based storage system that provides NFS capabilities.   Load balancers in front of the worker nodes in the cluster. DNS records that reference the load balancers.  Examples for meeting these requirements follow.\nPrerequisites Details  Storage Load Balancers DNS   Storage Verrazzano requires persistent storage for several components. This persistent storage is provided by a default storage class. A number of persistent storage providers exist for Kubernetes. This guide will focus on pre-allocated persistent volumes. In particular, the provided samples will illustrate the use of OCI’s NFS File System.\nOCI example Before storage can be exposed to Kubernetes, it must be created. In OCI, this is done using File System resources. Using the OCI Console, create a new File System. Within the new File System, create an Export. Remember the value used for Export Path as it will be used later. Also note the Mount Target’s IP Address for use later.\nAfter the exports have been created, referenced persistent volume folders (for example, /example/pv0001) will need to be created. In OCI, this can be done by mounting the export on one of the Kubernetes worker nodes and creating the folders. In the following example, the value /example is the Export Path and 10.0.1.8 is the Mount Target’s IP Address. The following command should be run on one of the Kubernetes worker nodes. This will result in the creation of nine persistent volume folders. The reason for nine persistent volume folders is covered in the next section.\n$ sudo mount 10.0.1.8:/example /mnt $ for x in {0001..0009}; do sudo mkdir -p /mnt/pv${x} \u0026\u0026 sudo chmod 777 /mnt/pv${x}; done Persistent Volumes A default Kubernetes storage class is required by Verrazzano. When using pre-allocated PersistentVolumes, for example NFS, persistent volumes should be declared as following. The value for name may be customized but will need to match the PersistentVolume storageClassName value later.\n Create a default StorageClass $ cat \u003c\u003c EOF | kubectl apply -f - apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: example-nfs annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer EOF  Create the required number of PersistentVolume resources. The Verrazzano system requires five persistent volumes for itself. The following command creates nine persistent volumes. The value for storageClassName must match the above StorageClass name. The values for name may be customized. The value for path must match the Export Path of the Export from above, combined with the persistent volume folder from above. The value for server must be changed to match the location of your file system server. $ for n in {0001..0009}; do cat \u003c\u003c EOF | kubectl apply -f - apiVersion: v1 kind: PersistentVolume metadata: name: pv${n} spec: storageClassName: example-nfs accessModes: - ReadWriteOnce - ReadWriteMany capacity: storage: 50Gi nfs: path: /example/pv${n} server: 10.0.1.8 volumeMode: Filesystem persistentVolumeReclaimPolicy: Recycle EOF    Load Balancers Verrazzano on Oracle Linux Cloud Native Environment uses external load balancer services. These will not automatically be provided by Verrazzano or Kubernetes. Two load balancers must be deployed outside of the subnet used for the Kubernetes cluster. One load balancer is for management traffic and the other for application traffic.\nSpecific steps will differ for each load balancer provider, but a generic configuration and an OCI example follow.\nGeneric configuration:  Target Host: Host names of Kubernetes worker nodes Target Ports: See table External Ports: See table Distribution: Round-robin Health Check: TCP     Traffic Type Service Name Type Suggested External Port Target Port     Application istio-ingressgateway TCP 80 31380   Application istio-ingressgateway TCP 443 31390   Management ingress-controller-nginx-ingress-controller TCP 80 30080   Management ingress-controller-nginx-ingress-controller TCP 443 30443    OCI example The following details can be used to create OCI load balancers for accessing application and management user interfaces, respectively. These load balancers will route HTTP/HTTPS traffic from the Internet to the private subnet. If load balancers are desired, then they should be created now even though the application and management endpoints will be installed later.\n Application Load Balancer: Public Subnet  Listeners  HTTP Listener: Protocol TCP, Port 80 HTTPS Listener: Protocol TCP, Port 443   Backend Sets  HTTP Backend Sets:  Health Check: Protocol TCP, Port 31380 Backends: Kubernetes Worker Nodes, Port 31380, Distribution Policy Weighted Round Robin   HTTPS Backend Sets  Health Check: Protocol TCP, Port 31390 Backends: Kubernetes Worker Nodes, Port 31390, Distribution Policy Weighted Round Robin       Management Load Balancer: Public Subnet  Listeners  HTTP Listener: Protocol TCP, Port 80 HTTPS Listener: Protocol TCP, Port 443   Backend Sets  HTTP Backend Sets:  Health Check: Protocol TCP, Port 30080 Backends: Kubernetes Worker Nodes, Port 30080, Distribution Policy Weighted Round Robin   HTTPS Backend Sets  Health Check: Protocol TCP, Port 30443 Backends: Kubernetes Worker Nodes, Port 30443, Distribution Policy Weighted Round Robin         DNS When using the spec.dns.external DNS type, the installer searches the DNS zone you provide for two specific A records. These are used to configure the cluster and should refer to external addresses of the load balancers in the previous step. The A records will need to be created manually.\nNOTE: At this time, the only supported deployment for Oracle Linux Cloud Native Environment is the external DNS type.\n   Record Use     ingress-mgmt Set as the .spec.externalIPs value of the ingress-controller-nginx-ingress-controller service.   ingress-verrazzano Set as the .spec.externalIPs value of the istio-ingressgateway service.    For example:\n198.51.100.10 A ingress-mgmt.myenv.mydomain.com. 203.0.113.10 A ingress-verrazzano.myenv.mydomain.com. Verrazzano installation will result in a number of management services that need to point to the ingress-mgmt address.\nkeycloak.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. rancher.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. grafana.vmi.system.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. prometheus.vmi.system.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. kibana.vmi.system.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. elasticsearch.vmi.system.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. For simplicity, an administrator may want to create wildcard DNS records for the management addresses:\n*.system.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. OR\n*.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. OCI example DNS is configured in OCI by creating DNS zones in the OCI Console. When creating a DNS zone, use these values:\n Method: Manual Zone Name: \u003cdns-suffix\u003e Zone Type: Primary  The value for \u003cdns-suffix\u003e excludes the environment (for example, use the mydomain.com portion of myenv.mydomain.com).\nDNS A records must be manually added to the zone and published using values described above. DNS CNAME records, in the same way.\n  During the Verrazzano install, these steps should be performed on the Oracle Linux Cloud Native Environment operator node.\nEdit the sample Verrazzano custom resource install-olcne.yaml file and provide these configuration settings for your OLCNE environment:\n The value for spec.environmentName is a unique DNS subdomain for the cluster (for example, myenv in myenv.mydomain.com). The value for spec.dns.external.suffix is the remainder of the DNS domain (for example, mydomain.com in myenv.mydomain.com). Under spec.ingress.verrazzano.nginxInstallArgs, the value for controller.service.externalIPs is the IP address of ingress-mgmt.\u003cmyenv\u003e.\u003cmydomain.com\u003e configured during DNS set up. Under spec.ingress.application.istioInstallArgs, the value for gateways.istio-ingressgateway.externalIPs is the IP address of ingress-verrazzano.\u003cmyenv\u003e.\u003cmydomain.com\u003e configured during DNS set up.  You will install Verrazzano using the external DNS type (the example custom resource for OLCNE is already configured to use spec.dns.external).\nSet the following environment variable:\nThe value for \u003cpath to valid Kubernetes config\u003e is typically ${HOME}/.kube/config.\n$ export KUBECONFIG=$VERRAZZANO_KUBECONFIG Next steps To continue, see the Installation Guide.\n","excerpt":"Prepare for the OCLNE install Oracle Linux Cloud Native Environment can be installed in several different types of environments. These range from physical, on-premises hardware to virtualized cloud …","ref":"/docs/setup/platforms/olcne/olcne/","title":"Oracle Linux Cloud Native Environment (OLCNE)"},{"body":"Known Issues OKE Missing Security List Ingress Rules The install scripts perform a check, which attempts access through the ingress ports. If the check fails, then the install will exit and you will see error messages like this:\nERROR: Port 443 is NOT accessible on ingress(132.145.66.80)! Check that security lists include an ingress rule for the node port 31739.\nOn an OKE install, this may indicate that there is a missing ingress rule or rules. To verify and fix the issue, do the following:\n Get the ports for the LoadBalancer services.  Run kubectl get services -A. Note the ports for the LoadBalancer type services. For example 80:31541/TCP,443:31739/TCP.   Check the security lists in the OCI Console.  Go to Networking/Virtual Cloud Networks. Select the related VCN. Go to the Security Lists for the VCN. Select the security list named oke-wkr-.... Check the ingress rules for the security list. There should be one rule for each of the destination ports named in the LoadBalancer services. In the above example, the destination ports are 31541 \u0026 31739. We would expect the ingress rule for 31739 to be missing because it was named in the ERROR output. If a rule is missing, then add it by clicking Add Ingress Rules and filling in the source CIDR and destination port range (missing port). Use the existing rules as a guide.    ","excerpt":"Known Issues OKE Missing Security List Ingress Rules The install scripts perform a check, which attempts access through the ingress ports. If the check fails, then the install will exit and you will …","ref":"/docs/releasenotes/troubleshooting/","title":"Troubleshooting"},{"body":"Upgrading an existing Verrazzano installation involves:\n Upgrading the Verrazzano platform operator to the Verrazzano release version to which you want to upgrade. Updating the version of your installed Verrazzano resource to the version supported by the upgraded operator.  Performing an upgrade will upgrade only the Verrazzano components related to the existing installation. Upgrading will not have any impact on running applications.\nNOTE: You may only change the version field during an upgrade; changes to other fields or component configurations are not supported at this time.\nUpgrade the Verrazzano platform operator In order to upgrade an existing Verrazzano installation, you must first upgrade the Verrazzano platform operator.\n  Upgrade the Verrazzano platform operator.\nTo upgrade to the latest version:\n$ kubectl apply -f https://github.com/verrazzano/verrazzano/releases/latest/download/operator.yaml To upgrade to a specific version, where \u003cversion\u003e is the desired version:\n$ kubectl apply -f https://github.com/verrazzano/verrazzano/releases/download/\u003cversion\u003e/operator.yaml For example:\n$ kubectl apply -f https://github.com/verrazzano/verrazzano/releases/download/v0.7.0/operator.yaml   Wait for the deployment to complete.\n$ kubectl -n verrazzano-install rollout status deployment/verrazzano-platform-operator deployment \"verrazzano-platform-operator\" successfully rolled out   Confirm that the operator pod is correctly defined and running.\n$ kubectl -n verrazzano-install get pods NAME READY STATUS RESTARTS AGE verrazzano-platform-operator-59d5c585fd-lwhsx 1/1 Running 0 114s   Upgrade Verrazzano To upgrade Verrazzano:\n  Update the Verrazzano resource to the desired version.\nTo upgrade the Verrazzano components, you must update the version field in your Verrazzano resource spec to match the version supported by the platform operator to which you upgraded and apply it to the cluster.\nThe value of the version field in the resource spec must be a Semantic Versioning value corresponding to a valid Verrazzano release version.\nYou can update the resource by doing one of the following:\na. Editing the YAML file you used to install Verrazzano and setting the version field to the latest version.\nFor example, to upgrade to v0.7.0, your YAML file should be edited to add or update the version field:\napiVersion:install.verrazzano.io/v1alpha1kind:Verrazzanometadata:name:my-verrazzanospec:profile:devversion:v0.7.0Then apply the resource to the cluster (if you have not edited the resource in-place using kubectl edit):\n$ kubectl apply -f my-verrazzano.yaml b. Editing the Verrazzano resource directly using kubectl and setting the version field directly, for example:\n$ kubectl edit verrazzano my-verrazzano # In the resource editor, add or update the version field to \"version: v0.7.0\", then save.   Wait for the upgrade to complete:\n$ kubectl wait --timeout=20m --for=condition=UpgradeComplete verrazzano/my-verrazzano   Verify the upgrade Check that all the pods in the verrazzano-system namespace are in the Running state. While the upgrade is in progress, you may see some pods terminating and restarting as newer versions of components are applied.\nFor example:\n$ kubectl get pods -n verrazzano-system verrazzano-admission-controller-84d6bc647c-7b8tl 1/1 Running 0 5m13s verrazzano-cluster-operator-57fb95fc99-kqjll 1/1 Running 0 5m13s verrazzano-monitoring-operator-7cb5947f4c-x9kfc 1/1 Running 0 5m13s verrazzano-operator-b6d95b4c4-sxprv 1/1 Running 0 5m13s vmi-system-api-7c8654dc76-2bdll 1/1 Running 0 4m44s vmi-system-es-data-0-6679cf99f4-9p25f 2/2 Running 0 4m44s vmi-system-es-data-1-8588867569-zlwwx 2/2 Running 0 4m44s vmi-system-es-ingest-78f6dfddfc-2v5nc 1/1 Running 0 4m44s vmi-system-es-master-0 1/1 Running 0 4m44s vmi-system-es-master-1 1/1 Running 0 4m44s vmi-system-es-master-2 1/1 Running 0 4m44s vmi-system-grafana-5f7bc8b676-xx49f 1/1 Running 0 4m44s vmi-system-kibana-649466fcf8-4n8ct 1/1 Running 0 4m44s vmi-system-prometheus-0-7f97ff97dc-gfclv 3/3 Running 0 4m44s vmi-system-prometheus-gw-7cb9df774-48g4b 1/1 Running 0 4m44s ","excerpt":"Upgrading an existing Verrazzano installation involves:\n Upgrading the Verrazzano platform operator to the Verrazzano release version to which you want to upgrade. Updating the version of your …","ref":"/docs/setup/upgrade/upgrade/","title":"Upgrade Guide"},{"body":"KIND is a tool for running local Kubernetes clusters using Docker container “nodes”. Follow these instructions to prepare a KIND cluster for running Verrazzano.\nNOTE KIND is not recommended for use on macOS and Windows because the Docker network is not directly exposed to the host. On macOS and Windows, minikube is recommended.  Prerequisites  Install Docker. Install KIND.  Prepare the KIND cluster To prepare the KIND cluster for use with Verrazzano, you must create the cluster and then install and configure MetalLB in that cluster.\nCreate the KIND cluster KIND images are prebuilt for each release. To find images suitable for a given release, check the release notes for your KIND version (check with kind version) where you’ll find a complete listing of images created for a KIND release.\nThe following example references a Kubernetes v1.18.8-based image built for KIND v0.9.0. Replace that image with one suitable for the KIND release you are using.\n$ kind create cluster --config - \u003c\u003cEOF kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane image: kindest/node:v1.18.8@sha256:f4bcc97a0ad6e7abaf3f643d890add7efe6ee4ab90baeb374b4f41a4c95567eb kubeadmConfigPatches: - | kind: ClusterConfiguration apiServer: extraArgs: \"service-account-issuer\": \"kubernetes.default.svc\" \"service-account-signing-key-file\": \"/etc/kubernetes/pki/sa.key\" EOF Install and configure MetalLB By default, KIND does not provide an implementation of network load balancers (Services of type LoadBalancer). MetalLB offers a network load balancer implementation.\nTo install MetalLB:\n$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/namespace.yaml $ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/metallb.yaml $ kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=\"$(openssl rand -base64 128)\" For further details, see the MetalLB installation guide.\nMetalLB is idle until configured. Configure MetalLB in Layer 2 mode and give it control over a range of IP addresses in the kind Docker network. In versions v0.7.0 and earlier, KIND uses Docker’s default bridge network; in versions v0.8.0 and later, it creates its own bridge network in KIND.\nTo determine the subnet of the kind Docker network in KIND v0.8.0 and later:\n$ docker inspect kind | jq '.[0].IPAM.Config[0].Subnet' -r 172.18.0.0/16 To determine the subnet of the kind Docker network in KIND v0.7.0 and earlier:\n$ docker inspect bridge | jq '.[0].IPAM.Config[0].Subnet' -r 172.17.0.0/16 For use by MetalLB, assign a range of IP addresses at the end of the kind network’s subnet CIDR range.\n$ kubectl apply -f - \u003c\u003c-EOF apiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: my-ip-space protocol: layer2 addresses: - 172.18.0.230-172.18.0.250 EOF Image caching to speed up install If you are experimenting with Verrazzano and expect that you may need to delete the KIND cluster and later, install Verrazzano again on a new KIND cluster, then you can follow these steps to ensure that the image cache used by containerd inside KIND is preserved across clusters. Subsequent installs will be faster than the first install, because they will not need to pull the images again.\n1. Create a named Docker volume that will be used for the image cache, and note its Mountpoint path. In this example, the volume is named containerd.\n$ docker volume create containerd $ docker volume inspect containerd #Sample output is shown { \"CreatedAt\": \"2021-01-11T16:27:47Z\", \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/containerd/_data\", \"Name\": \"containerd\", \"Options\": {}, \"Scope\": \"local\" } 2. Specify the Mountpoint path obtained, as the hostPath under extraMounts in your KIND configuration file, with a containerPath of /var/lib/containerd, which is the default containerd image caching location inside the KIND container. An example of the modified KIND configuration is shown in the following create cluster command:\n$ kind create cluster --config - \u003c\u003cEOF kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane image: kindest/node:v1.18.8@sha256:f4bcc97a0ad6e7abaf3f643d890add7efe6ee4ab90baeb374b4f41a4c95567eb kubeadmConfigPatches: - | kind: ClusterConfiguration apiServer: extraArgs: \"service-account-issuer\": \"kubernetes.default.svc\" \"service-account-signing-key-file\": \"/etc/kubernetes/pki/sa.key\" extraMounts: - hostPath: /var/lib/docker/volumes/containerd/_data containerPath: /var/lib/containerd #This is the location of the image cache inside the KIND container EOF Next steps To continue, see the Installation Guide.\n","excerpt":"KIND is a tool for running local Kubernetes clusters using Docker container “nodes”. Follow these instructions to prepare a KIND cluster for running Verrazzano.\nNOTE KIND is not recommended for use on …","ref":"/docs/setup/platforms/kind/kind/","title":"KIND"},{"body":"minikube quickly sets up a local Kubernetes cluster on macOS, Linux, and Windows. Follow these instructions to prepare a minikube cluster for running Verrazzano.\nPrerequisites  Install minikube. Install a driver:  On macOS or Windows, select a VM-based driver, not Docker. Oracle Linux 7, deploying WebLogic or Coherence applications requires the kvm2 driver because the Docker driver requires a kernel patch.    Prepare the minikube cluster To prepare the minikube cluster for use with Verrazzano, you must create the cluster and then expose services of type LoadBalancer by using the minikube tunnel command.\nCreate the minikube cluster Create a minikube cluster using a supported Kubernetes version and appropriate driver. On Linux hosts, the default driver is acceptable; on macOS, hyperkit is recommended.\n$ minikube start \\  --kubernetes-version=v1.18.8 \\  --driver=hyperkit \\  --memory=16G \\  --disk-size=30G \\  --cpus=4 \\  --extra-config=apiserver.service-account-signing-key-file=/var/lib/minikube/certs/sa.key \\  --extra-config=apiserver.service-account-issuer=kubernetes/serviceaccount \\  --extra-config=apiserver.service-account-api-audiences=api Run minikube tunnel minikube exposes Kubernetes services of type LoadBalancer with the minikube tunnel command.\nNote that the ip command is required by minikube tunnel. You may need to add /sbin to your PATH environment variable.\nRun a tunnel in a separate terminal from minikube:\n$ minikube tunnel Next steps To continue, see the Installation Guide.\n","excerpt":"minikube quickly sets up a local Kubernetes cluster on macOS, Linux, and Windows. Follow these instructions to prepare a minikube cluster for running Verrazzano.\nPrerequisites  Install minikube. …","ref":"/docs/setup/platforms/minikube/minikube/","title":"minikube"},{"body":"Prepare for the generic install To use a generic Kubernetes implementation, there are two main areas you can configure: ingress and storage.\n Ingress Storage    You can achieve ingress configuration using Helm overrides. For example, to use the nginx-controller for ingress on KIND, apply the following customization to the Verrazzano CRD.\nspec: components: ingress: nginxInstallArgs: - name: controller.kind value: DaemonSet - name: controller.hostPort.enabled value: \"true\" - name: controller.nodeSelector.ingress-ready value: \"true\" setString: true - name: controller.tolerations[0].key value: node-role.kubernetes.io/master - name: controller.tolerations[0].operator value: Equal - name: controller.tolerations[0].effect value: NoSchedule   By default, each Verrazzano install profile has different storage characteristics. Some components have external storage requirements (expressed through PersistentVolumeClaim declarations in their resources/helm charts):\n MySQL Elasticsearch Prometheus Grafana  By default, the prod profile uses 50Gi persistent volumes for each of the above services, using the default storage class for the target Kubernetes platform. The dev profile uses ephemeral emptyDir storage by default. However, you can customize these storage settings within a profile as desired.\nTo override these settings, customize the Verrazzano install resource by defining a VolumeSource on the defaultVolumeSource field in the install CR, which can be one of:\n emptyDir persistentVolumeClaim  Configuring emptyDir for the defaultVolumeSource forces all persistent volumes created by Verrazzano components in an installation to use ephemeral storage unless otherwise overridden. This can be useful for development or test scenarios.\nYou can use a persistentVolumeClaim to identify a volumeClaimSpecTemplate in the volumeClaimSpecTemplates section via the claimSource field. A volumeClaimSpecTemplate is a named PersistentVolumeClaimSpec configuration. A volumeClaimSpecTemplate can be referenced from more than one component; it merely identifies configuration settings and does not result in a direct instantiation of a persistent volume. The settings are used by referencing components when creating their PersistentVolumeClaims at install time.\nIf the component supports it, then you can override the defaultVolumeSource setting at the component level by defining a supported VolumeSource on that component. At present, only the keycloak/mysql component supports a volumeSource field override.\nExamples The following example shows how to define a dev profile with different persistence settings for the monitoring components and the Keycloak/MySQL instance.\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: kind-verrazzano-with-persistence spec: profile: dev defaultVolumeSource: persistentVolumeClaim: claimName: default # Use the \"default\" volume template components: keycloak: mysql: volumeSource: persistentVolumeClaim: claimName: mysql # Use the \"mysql\" PVC template for the MySQL volume configuration volumeClaimSpecTemplates: - metadata: name: default # \"default\" is a known template name, and will be used by Verrazzano components by default if no other template is referenced explicitly spec: resources: requests: storage: 2Gi - metadata: spec: resources: requests: storage: 5Gi # default The following example shows how to define a dev profile where all resources use emptyDir by default.\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: storage-example-dev spec: profile: dev defaultVolumeSource: emptyDir: {} # Use ephemeral storage for dev mode for all Components    Next steps To continue, see the Installation Guide.\n","excerpt":"Prepare for the generic install To use a generic Kubernetes implementation, there are two main areas you can configure: ingress and storage.\n Ingress Storage    You can achieve ingress configuration …","ref":"/docs/setup/platforms/generic/generic/","title":"Generic Kubernetes"},{"body":"Verrazzano is an end-to-end enterprise container platform for deploying cloud-native and traditional applications in multi-cloud and hybrid environments. It is made up of a curated set of open source components – many that you may already use and trust, and some that were written specifically to pull together all of the pieces that make Verrazzano a cohesive and easy to use platform.\nVerrazzano includes the following capabilities:\n Hybrid and multicluster workload management Special handling for WebLogic, Coherence, and Helidon applications Multicluster infrastructure management Integrated and pre-wired application monitoring Integrated security DevOps and GitOps enablement  NOTE This is a developer preview release of Verrazzano. You should install Verrazzano only in a Kubernetes cluster that can be safely deleted when your evaluation is complete.  Select Quick Start to get started.\nVerrazzano release versions and source code are available at https://github.com/verrazzano/verrazzano. This repository contains a Kubernetes operator for installing Verrazzano and example applications for use with Verrazzano.\n","excerpt":"Verrazzano is an end-to-end enterprise container platform for deploying cloud-native and traditional applications in multi-cloud and hybrid environments. It is made up of a curated set of open source …","ref":"/docs/","title":"Welcome to Verrazzano"},{"body":"","excerpt":"","ref":"/search/","title":"Search Results"},{"body":"  #td-cover-block-0 { background-image: url(/featured-background_hu3fcc2fb45a2768e8c72dcbce1f48d58b_164050_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/featured-background_hu3fcc2fb45a2768e8c72dcbce1f48d58b_164050_1920x1080_fill_q75_catmullrom_top.jpg); } }  Verrazzano Enterprise Container Platform Learn More  View Repository   A hybrid multi-cloud Kubernetes based Enterprise Container Platform for running both cloud-native and traditional applications.\n\n          Application Lifecycle Management Use Open Application Model constructs to describe, deploy, and update multi-component application systems across clusters in multiple clouds, including clusters on premises.\n   Integrated Monitoring Verrazzano can provision a full monitoring stack for your application, including Elasticsearch, Kibana, Prometheus and Grafana. Your applications are automatically wired up to send logs and metrics into the monitoring tools.\n   Security Built in security with encryption, certificate management, authentication and authorization services and network policies.\n       Contributions welcome! We do a Pull Request contributions workflow on GitHub. New users are always welcome!\nRead more …\n   Connect with us on Slack! For project discussions.\nRead more …\n   Follow us on Twitter! For announcement of latest features etc.\nRead more …\n    ","excerpt":"  #td-cover-block-0 { background-image: url(/featured-background_hu3fcc2fb45a2768e8c72dcbce1f48d58b_164050_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { …","ref":"/","title":"Verrazzano Enterprise Container Platform"}]
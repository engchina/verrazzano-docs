[{"body":"","excerpt":"","ref":"/docs/concepts/","title":"Concepts"},{"body":"Prerequisites The Quick Start assumes that you have already installed a Kubernetes cluster. For instructions on preparing Kubernetes platforms for installing Verrazzano, see Platform Setup. For detailed installation instructions, see the Installation Guide.\nVerrazzano requires the following:\n A Kubernetes cluster and a compatible kubectl.  Verrazzano has been tested only on the following versions of Kubernetes: 1.17.x and 1.18.x. Other versions have not been tested and are not guaranteed to work.   At least 2 CPUs, 100GB disk storage, and 16GB RAM available on the Kubernetes worker nodes.  Install the Verrazzano platform operator Verrazzano provides a Kubernetes operator to manage the life cycle of Verrazzano installations. The operator works with a custom resource defined in the cluster. You can install, uninstall, and update Verrazzano installations by updating the Verrazzano custom resource. The Verrazzano platform operator controller will apply the configuration from the custom resource to the cluster for you.\nTo install the Verrazzano platform operator, follow these steps:\n  Deploy the Verrazzano platform operator.\nkubectl apply -f https://github.com/verrazzano/verrazzano/releases/latest/download/operator.yaml   Wait for the deployment to complete.\n$ kubectl -n verrazzano-install rollout status deployment/verrazzano-platform-operator deployment \"verrazzano-platform-operator\" successfully rolled out   Confirm that the operator pod is correctly defined and running.\n$ kubectl -n verrazzano-install get pods NAME READY STATUS RESTARTS AGE verrazzano-platform-operator-59d5c585fd-lwhsx 1/1 Running 0 114s   Install Verrazzano You install Verrazzano by creating a Verrazzano custom resource in your Kubernetes cluster. Verrazzano currently supports a default production (prod) profile and a development (dev) profile suitable for evaluation.\nThe development profile has the following characteristics:\n Magic (xip.io) DNS Self-signed certificates Shared observability stack used by the system components and all applications Ephemeral storage for the observability stack (if the pods are restarted, you lose all of your logs and metrics) Single-node, reduced memory Elasticsearch cluster  To install Verrazzano, follow these steps:\n  Install Verrazzano with its dev profile.\nkubectl apply -f - \u003c\u003cEOF apiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: example-verrazzano spec: profile: dev EOF   Wait for the installation to complete.\nkubectl wait \\  --timeout=20m \\  --for=condition=InstallComplete \\  verrazzano/example-verrazzano   (Optional) View the installation logs.\nThe Verrazzano operator launches a Kubernetes job to install Verrazzano. You can view the installation logs from that job with the following command:\nkubectl logs -f \\  $( \\  kubectl get pod \\  -l job-name=verrazzano-install-example-verrazzano \\  -o jsonpath=\"{.items[0].metadata.name}\" \\  )   Deploy an example application The Hello World Helidon example application provides a simple Hello World REST service written with Helidon. For more information and the code of this application, see the Verrazzano examples.\nTo deploy the Hello World Helidon example application, follow these steps:\n  Create a namespace for the example application and add a label identifying the namespace as managed by Verrazzano.\n$ kubectl create namespace hello-helidon $ kubectl label namespace hello-helidon verrazzano-managed=true   Apply the hello-helidon resources to deploy the application.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-comp.yaml $ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-app.yaml   Wait for the application to be ready.\n$ kubectl wait --for=condition=Ready pods --all -n hello-helidon --timeout=300s pod/hello-helidon-workload-977cbbc94-z22ls condition met This creates the Verrazzano OAM component application resources for the example, waits for the pods in the hello-helidon namespace to be ready.\n  Save the host name of the load balancer exposing the application’s REST service endpoints.\n$ HOST=$(kubectl get gateway hello-helidon-hello-helidon-appconf-gw -n hello-helidon -o jsonpath='{.spec.servers[0].hosts[0]}')   Get the default message.\n$ curl -sk -X GET \"https://${HOST}/greet\" {\"message\":\"Hello World!\"}   Uninstall the example application To uninstall the Hello World Helidon example application, follow these steps.\n  Delete the Verrazzano application resources.\n$ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-comp.yaml $ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-app.yaml   Delete the example namespace.\n$ kubectl delete namespace hello-helidon namespace \"hello-helidon\" deleted   Verify that the hello-helidon namespace has been deleted.\n$ kubectl get ns hello-helidon Error from server (NotFound): namespaces \"hello-helidon\" not found   Uninstall Verrazzano To uninstall Verrazzano, follow these steps:\n  Delete the Verrazzano custom resource.\nkubectl delete verrazzano example-verrazzano  NOTE This command blocks until the uninstall has completed. To follow the progress, you can view the uninstall logs.    (Optional) View the uninstall logs.\nThe Verrazzano operator launches a Kubernetes job to delete the Verrazzano installation. You can view the uninstall logs from that job with the following command:\nkubectl logs -f \\  $( \\  kubectl get pod \\  -l job-name=verrazzano-uninstall-example-verrazzano \\  -o jsonpath=\"{.items[0].metadata.name}\" \\  )   Next steps For more example applications, see Verrazzano Examples.\n","excerpt":"Prerequisites The Quick Start assumes that you have already installed a Kubernetes cluster. For instructions on preparing Kubernetes platforms for installing Verrazzano, see Platform Setup. For …","ref":"/docs/setup/quickstart/","title":"Quick Start"},{"body":"Open Application Model (OAM) is a runtime-agnostic specification for defining cloud native applications; it allows developers to focus on the application instead of the complexities of a particular runtime infrastructure. OAM provides the specification for several file formats and rules for a runtime to interpret. Verrazzano uses OAM to enable the definition of a composite application abstraction and makes OAM constructs available within a VerrazzanoApplication YAML file. Verrazzano provides the flexibility to combine what you want into a multi-cloud enablement. It uses the VerrazzanoApplication as a means to encapsulate a set of components, scopes, and traits, and deploy them on a selected cluster.\nOAM’s workload concept makes it easy to use many different workload types. Verrazzano includes specific workload types with special handling to deploy and manage those types, such as WebLogic, Coherence, and Helidon. OAM’s flexibility lets you create a grouping that is managed as a unit, although each component can be scaled or updated independently.\nHow does OAM work? OAM has five core concepts:\n Workloads - Declarations of the kinds of resources supported by the platform and the OpenAPI schema for that resource. Most Kubernetes CRDs can be exposed as workloads. Standard Kubernetes resource types can also be used (for example, Deployment, Service, Pod, ConfigMap). Components - Wrap a workload resource’s spec data within OAM specific metadata. Application Configurations - Describe a collection of components that comprise an application. This is also where customization (such as environmental) of each component is done. Customization is achieved using scopes and traits. Scopes - Apply customization to several components. Traits - Apply customization to a single component.  ","excerpt":"Open Application Model (OAM) is a runtime-agnostic specification for defining cloud native applications; it allows developers to focus on the application instead of the complexities of a particular …","ref":"/docs/concepts/verrazzanooam/","title":"Verrazzano and the Open Application Model"},{"body":"The Verrazzano custom resource contains the configuration information for an installation. Here is a sample Verrazzano custom resource file that uses OCI DNS. See other examples here.\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: my-verrazzano spec: environmentName: env profile: prod components: certManager: certificate: acme: provider: letsEncrypt emailAddress: emailAddress@domain.com dns: oci: ociConfigSecret: ociConfigSecret dnsZoneCompartmentOCID: dnsZoneCompartmentOcid dnsZoneOCID: dnsZoneOcid dnsZoneName: my.dns.zone.name ingress: type: LoadBalancer The following table describes the spec portion of the Verrazzano custom resource:\n   Field Type Description Required     environmentName string Name of the installation. This name is part of the endpoint access URLs that are generated. The default value is default. No   profile string The installation profile to select. Valid values are prod (production) and dev (development). The default is prod. No   version string The version to install. Valid versions can be found here. Defaults to the current version supported by the Verrazzano platform operator. No   components Components The Verrazzano components. No    Components    Field Type Description Required     certManager CertManagerComponent The cert-manager component configuration. No   dns DNSComponent The DNS component configuration. No   ingress IngressComponent The ingress component configuration. No   istio IstioComponent The Istio component configuration. No    CertManager Component    Field Type Description Required     certificate Certificate The certificate configuration. No    Certificate    Field Type Description Required     acme Acme The ACME configuration. Either acme or ca must be specified. No   ca CertificateAuthority The certificate authority configuration. Either acme or ca must be specified. No    Acme    Field Type Description Required     provider string Name of the Acme provider. Yes   emailAddress string Email address of the user. Yes    CertificateAuthority    Field Type Description Required     secretName string The secret name. Yes   clusterResourceNamespace string The secrete namespace. Yes    DNS Component    Field Type Description Required     oci DNS-OCI OCI DNS configuration. Either oci or external must be specified. No   external DNS-External External DNS configuration. Either oci or external must be specified. No    DNS OCI    Field Type Description Required     ociConfigSecret string Name of the OCI configuration secret. Generate a secret named oci-config based on the OCI configuration profile you want to use. You can specify a profile other than DEFAULT and a different secret name. See instructions by running ./install/create_oci_config_secret.sh. Yes   dnsZoneCompartmentOCID string The OCI DNS compartment OCID. Yes   dnsZoneOCID string The OCI DNS zone OCID. Yes   dnsZoneName string Name of OCI DNS zone. Yes    DNS External    Field Type Description Required     external.suffix string The suffix for DNS names. Yes    Ingress Component    Field Type Description Required     type string The ingress type. Valid values are LoadBalancer and NodePort. The default value is LoadBalancer. Yes   ingressNginxArgs NameValue list The list of argument names and values. No   ports PortConfig list The list port configurations used by the ingress. No    Port Config    Field Type Description Required     name string The port name. No   port string The port value. Yes   targetPort string The target port value. The default is same as the port value. Yes   protocol string The protocol used by the port. TCP is the default. No   nodePort string The nodePort value. No    Name Value    Field Type Description Required     name string The argument name. Yes   value string The argument value. Either value or valueList must be specifed. No   valueList string list The list of argument values. Either value or valueList must be specified. No   setString Boolean Specifies if the value is a string No    Istio Component    Field Type Description Required     istioInstallArgs NameValue list A list of Istio Helm chart arguments and values to apply during the installation of Istio. Each argument is specified as either a name/value or name/valueList pair. No    ","excerpt":"The Verrazzano custom resource contains the configuration information for an installation. Here is a sample Verrazzano custom resource file that uses OCI DNS. See other examples here.\napiVersion: …","ref":"/docs/reference/api/verrazzano/verrazzano/","title":"Verrazzano Custom Resource Definition"},{"body":"Overview Developing and deploying an application to Verrazzano consists of:\n Packaging the application as a Docker image. Publishing the application’s Docker image to a container registry. Applying the application’s Verrazzano components to the cluster. Applying the application’s Verrazzano applications to the cluster.  This guide does not provide the full details for the first two steps. An existing example application Docker image has been packaged and published for use.\nVerrazzano supports application definition using Open Application Model (OAM). Verrrazzano applications are composed of components and application configurations. This document demonstrates creating OAM resources that define an application as well as the steps required to deploy those resources.\nWhat you need   About 10 minutes.\n  Access to an existing Kubernetes cluster with Verrazzano installed.\n  Access to the application’s image in GitHub Container Registry.\nConfirm access using this command to pull the example’s Docker image.\ndocker pull ghcr.io/verrazzano/example-helidon-greet-app-v1:0.1.12-1-20210218160249-d8db8f3   Application development This guide uses an example application which was written with Java and Helidon. For the implementation details, see the Helidon MP tutorial. See the application source code in the Verrazzano examples repository.\nThe example application is a JAX-RS service and implements the following REST endpoints:\n /greet - Returns a default greeting message that is stored in memory. This endpoint accepts the GET HTTP request method. /greet/{name} - Returns a greeting message including the name provided in the path parameter. This endpoint accepts the GET HTTP request method. /greet/greeting - Changes the greeting message to be used in future calls to the other endpoints. This endpoint accepts the PUT HTTP request method and a JSON payload.  The following code shows a portion of the application’s implementation. The Verrazzano examples repository contains the complete implementation. An important detail here is that the application contains a single resource exposed on path /greet.\npackage io.helidon.examples.quickstart.mp; ... @Path(\"/greet\") @RequestScoped public class GreetResource { @GET @Produces(MediaType.APPLICATION_JSON) public JsonObject getDefaultMessage() { ... } @Path(\"/{name}\") @GET @Produces(MediaType.APPLICATION_JSON) public JsonObject getMessage(@PathParam(\"name\") String name) { ... } @Path(\"/greeting\") @PUT @Consumes(MediaType.APPLICATION_JSON) ... public Response updateGreeting(JsonObject jsonObject) { ... } } A Dockerfile is used to package the completed application JAR file into a Docker image. The following code shows a portion of the Dockerfile. The Verrazzano examples repository contains the complete Dockerfile. Note that the Docker container exposes a single port 8080.\nFROMghcr.io/oracle/oraclelinux:7-slim...CMD java -cp /app/helidon-quickstart-mp.jar:/app/* io.helidon.examples.quickstart.mp.MainEXPOSE8080Application deployment When you deploy applications with Verrazzano, the platform sets up connections, network policies, and ingresses in the service mesh, and wires up a monitoring stack to capture the metrics, logs, and traces. Verrazzano employs OAM components to define the functional units of a system that are then assembled and configured by defining associated application configurations.\nVerrazzano components A Verrazzano OAM component is a Kubernetes Custom Resource describing an application’s general composition and environment requirements. The following code shows the component for the example application used in this guide. This resource describes a component which is implemented by a single Docker image containing a Helidon application exposing a single endpoint.\napiVersion:core.oam.dev/v1alpha2kind:Componentmetadata:name:hello-helidon-componentnamespace:hello-helidonspec:workload:apiVersion:core.oam.dev/v1alpha2kind:ContainerizedWorkloadmetadata:name:hello-helidon-workloadnamespace:hello-helidonlabels:app:hello-helidonspec:containers:- name:hello-helidon-containerimage:\"ghcr.io/verrazzano/example-helidon-greet-app-v1:0.1.12-1-20210218160249-d8db8f3\"ports:- containerPort:8080name:httpA brief description of each field of the component:\n apiVersion - Version of the component custom resource definition kind - Standard name of the component custom resource definition metadata.name - The name used to create the component’s custom resource metadata.namespace - The namespace used to create this component’s custom resource spec.workload.kind - ContainerizedWorkload defines a stateless workload of Kubernetes spec.workload.spec.containers - The implementation containers spec.workload.spec.containers.ports - Ports exposed by the container  Verrazzano application configurations A Verrazzano application configuration is a Kubernetes Custom Resource which provides environment specific customizations. The following code shows the application configuration for the example used in this guide. This resource specifies the deployment of the application to the hello-helidon namespace. Additional runtime features are specified using traits, or runtime overlays that augment the workload. For example, the ingress trait specifies the ingress host and path, while the metrics trait provides the Prometheus scraper used to obtain the application related metrics.\napiVersion:core.oam.dev/v1alpha2kind:ApplicationConfigurationmetadata:name:hello-helidon-appconfnamespace:hello-helidonannotations:version:v1.0.0description:\"Hello Helidon application\"spec:components:- componentName:hello-helidon-componenttraits:- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:MetricsTraitspec:scraper:verrazzano-system/vmi-system-prometheus-0- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:IngressTraitmetadata:name:hello-helidon-ingressspec:rules:- paths:- path:\"/greet\"pathType:PrefixA brief description of each field in the application configuration:\n apiVersion - Version of the ApplicationConfiguration custom resource definition kind - Standard name of the application configuration custom resource definition metadata.name - The name used to create this application configuration resource metadata.namespace - The namespace used for this application configuration custom resource spec.components - Reference to the application’s components leveraged to specify runtime configuration spec.components[].traits - The traits specified for the application’s components  To explore traits, we can examine the fields of an ingress trait:\n apiVersion - Version of the OAM trait custom resource definition kind - IngressTrait is the name of the OAM application ingress trait custom resource definition spec.rules.paths - The context paths for accessing the application  Deploy the application The following steps are required to deploy the example application. Steps similar to the apply steps would be used to deploy any application to Verrazzano.\n  Create a namespace for the example application and add a label identifying the namespace as managed by Verrazzano.\n$ kubectl create namespace hello-helidon $ kubectl label namespace hello-helidon verrazzano-managed=true   Apply the application’s component.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-comp.yaml This step causes the validation and creation of the component resource. No other resources or objects are created as a result. Application configurations applied in the future may reference this component resource.\n  Apply the application configuration.\n$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-app.yaml This step causes the validation and creation of the application configuration resource. This operation triggers the activation of a number of Verrazzano operators. These operators create other Kubernetes objects (for example, Deployments, ReplicaSets, Pods, Services, Ingresses) that collectively provide and support the application.\n  Configure the application’s DNS resolution.\nAfter deploying the application, configure DNS to resolve the application’s ingress DNS name to the application’s load balancer IP address. The generated host name is obtained by querying Kubernetes for the gateway:\n$ kubectl get gateway hello-helidon-hello-helidon-appconf-gw -n hello-helidon -o jsonpath='{.spec.servers[0].hosts[0]}' The load balancer IP is obtained by querying Kubernetes for the Istio ingress gateway status:\n$ kubectl get service -n istio-system istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}' DNS configuration steps are outside the scope of this guide. For DNS infrastructure that can be configured and used, see the Oracle Cloud Infrastructure DNS documentation. In some small non-production scenarios, DNS configuration using /etc/hosts or an equivalent may be sufficient.\n  Verify the deployment Applying the application configuration initiates the creation of several Kubernetes objects. Actual creation and initialization of these objects occurs asynchronously. The following steps provide commands for determining when these objects are ready for use.\nNote: Many other Kubernetes objects unrelated to the example application may also exist. Those have been omitted from the lists.\n  Verify the Helidon application pod is running.\n$ kubectl get pods -n hello-helidon | grep '^NAME\\|hello-helidon-workload' NAME READY STATUS RESTARTS AGE hello-helidon-workload-9dfbbfb74-4jm9v 1/1 Running 0 94m The parameter hello-helidon-workload is from the component’s spec.workload.metadata.name value.\n  Verify the Verrazzano application operator pod is running.\n$ kubectl get pods -n verrazzano-system | grep '^NAME\\|verrazzano-application-operator' NAME READY STATUS RESTARTS AGE verrazzano-application-operator-5485967588-lp6cw 1/1 Running 0 8d The namespace verrazzano-system is used by Verrazzano for non-application objects managed by Verrazzano. A single verrazzano-application-operator manages the life cycle of all OAM based applications within the cluster.\n  Verify the Verrazzano monitoring infrastructure is running.\n$ kubectl get pods -n verrazzano-system | grep '^NAME\\|vmi-system' NAME READY STATUS RESTARTS AGE vmi-system-api-6fb4fd57cb-95ttz 1/1 Running 0 8d vmi-system-es-master-0 1/1 Running 0 11h vmi-system-grafana-674b4f5df7-f4f2p 1/1 Running 0 8d vmi-system-kibana-759b854fc6-4tsjv 1/1 Running 0 8d vmi-system-prometheus-0-f6f587664-pfm54 3/3 Running 0 101m vmi-system-prometheus-gw-68c45f84b8-jrxlt 1/1 Running 0 8d These pods in the verrazzano-system namespace constitute a monitoring stack created by Verrazzano for the deployed applications.\nThe monitoring infrastructure comprises several components:\n vmi-system-api - Internal API for configuring monitoring vmi-system-es - Elasticsearch for log collection vmi-system-kibana - Kibana for log visualization vmi-system-grafana - Grafana for metric visualization vmi-system-prometheus - Prometheus for metric collection     Diagnose failures.\nView the event logs of any pod not entering the Running state within a reasonable length of time, such as five minutes.\n$ kubectl describe pod -n hello-helidon hello-helidon-workload-9dfbbfb74-4jm9v Use the specific namespace and name for the pod being investigated.\n  Explore the application Follow these steps to explore the application’s functionality. If DNS was not configured, then use the alternative commands.\n  Save the host name and IP address of the load balancer exposing the application’s REST service endpoints for later.\n$ HOST=$(kubectl get gateway hello-helidon-hello-helidon-appconf-gw -n hello-helidon -o jsonpath='{.spec.servers[0].hosts[0]}') $ ADDRESS=$(kubectl get service -n istio-system istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}') NOTE:\n The value of ADDRESS is used only if DNS has not been configured. The following alternative commands may not work in conjunction with firewalls that validate HTTP Host headers.    Get the default message.\n$ curl -sk -X GET \"https://${HOST}/greet\" {\"message\":\"Hello World!\"} If DNS has not been configured, then use this command.\n$ curl -sk -X GET \"https://${HOST}/greet\" --resolve ${HOST}:443:${ADDRESS}   Get a message for Robert.\n$ curl -sk -X GET \"https://${HOST}/greet/Robert\" {\"message\":\"Hello Robert!\"} If DNS has not been configured, then use this command.\n$ curl -sk -X GET \"https://${HOST}/greet/Robert\" --resolve ${HOST}:443:${ADDRESS}   Update the default greeting.\n$ curl -sk -X PUT \"https://${HOST}/greet/greeting\" -H 'Content-Type: application/json' -d '{\"greeting\" : \"Greetings\"}' If DNS has not been configured, then use this command.\n$ curl -sk -X PUT \"https://${HOST}/greet/greeting\" -H 'Content-Type: application/json' -d '{\"greeting\" : \"Greetings\"}' --resolve ${HOST}:443:${ADDRESS}   Get the new message for Robert.\n$ curl -sk -X GET \"https://${HOST}/greet/Robert\" {\"message\":\"Greetings Robert!\"} If DNS has not been configured, then use this command.\n$ curl -sk -X GET \"https://${HOST}/greet/Robert\" --resolve ${HOST}:443:${ADDRESS}   Access the application’s logs Deployed applications have log collection enabled. These logs are collected using Elasticsearch and can be accessed using Kibana. Elasticsearch and Kibana are examples of infrastructure Verrazzano creates in support of an application as a result of applying an application configuration.\nDetermine the URL to access Kibana using the following commands.\n$ KIBANA_HOST=$(kubectl get ingress -n verrazzano-system vmi-system-kibana -o jsonpath='{.spec.rules[0].host}') $ KIBANA_URL=\"https://${KIBANA_HOST}\" $ echo \"${KIBANA_URL}\" $ open \"${KIBANA_URL}\" The user name to access Kibana defaults to verrazzano during the Verrazzano installation.\nDetermine the password to access Kibana using the following command:\n$ echo $(kubectl get secret -n verrazzano-system verrazzano -o jsonpath={.data.password} | base64 --decode) Access the application’s metrics Deployed applications have metric collection enabled. Grafana can be used to access these metrics collected by Prometheus. Prometheus and Grafana are additional components Verrazzano creates as a result of applying an application configuration.\nDetermine the URL to access Grafana using the following commands.\n$ GRAFANA_HOST=$(kubectl get ingress -n verrazzano-system vmi-system-grafana -o jsonpath='{.spec.rules[0].host}') $ GRAFANA_URL=\"https://${GRAFANA_HOST}\" $ echo \"${GRAFANA_URL}\" $ open \"${GRAFANA_URL}\" The user name to access Grafana is set to the default value verrazzano during the Verrazzano installation.\nDetermine the password to access Grafana using the following command.\n$ echo $(kubectl get secret -n verrazzano-system verrazzano -o jsonpath={.data.password} | base64 --decode) Alternatively, metrics can be accessed directly using Prometheus. Determine the URL for this access using the following commands.\n$ PROMETHEUS_HOST=$(kubectl get ingress -n verrazzano-system vmi-system-prometheus -o jsonpath='{.spec.rules[0].host}') $ PROMETHEUS_URL=\"https://${PROMETHEUS_HOST}\" $ echo \"${PROMETHEUS_URL}\" $ open \"${PROMETHEUS_URL}\" The user name and password for both Prometheus and Grafana are the same.\nApplication removal Run the following commands to delete the application configuration, and optionally the component and namespace.\n  Delete the application configuration.\n$ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-app.yaml The deletion of the application configuration will result in the destruction of all application-specific Kubernetes objects.\n  (Optional) Delete the application’s component.\n$ kubectl delete -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/examples/hello-helidon/hello-helidon-comp.yaml Note: This step is not required if other application configurations for this component will be applied in the future.\n  (Optional) Delete the namespace.\n$ kubectl delete namespace hello-helidon   ","excerpt":"Overview Developing and deploying an application to Verrazzano consists of:\n Packaging the application as a Docker image. Publishing the application’s Docker image to a container registry. Applying …","ref":"/docs/guides/application-deployment-guide/","title":"Application Deployment Guide"},{"body":"v0.11.0: Features\n OAM applications are optionally deployed into an Istio service mesh. Incremental improvements to user-facing roles.  Fixes\n Fixed issue with logging when an application has multiple workload types. Fixed metrics configuration in Spring Boot example application.  v0.10.0: Breaking Changes\n Model/binding files removed; now application deployment done exclusively by using Open Application Model (OAM). Syntax changes for WebLogic and Coherence OAM workloads, now defined using VerrazzanoCoherenceWorkload and VerrazzanoWebLogicWorkload types.  Features\n By default, application endpoints now use HTTPs - when using magic DNS, certificates are issued by cluster issuer, when using OCI DNS certificates are issued using Let’s Encrypt, or the end user can provide certificates. Updated Coherence operator to 3.1.3. Updates for running Verrazzano on Kubernetes 1.19 and 1.20. RBAC roles and role bindings created at install time. Added instance information to status of Verrazzano custom resource; can be used to obtain instance URLs. Upgraded Istio to v1.7.3.  Fixes\n Reduced log level of Elasticsearch; excessive logging could have resulted in filling up disks.  v0.9.0:  Features  Added platform support for installing Verrazzano on Kind clusters. Log records are indexed from the OAM appconfig and component definitions using the following pattern: namespace-appconfig-component. All system and curated components are now patchable. More updates to Open Application Model (OAM) support.    To enable OAM, when you install Verrazzano, specify the following in the Kubernetes manifest file for the Verrazzano custom resource:\nspec: oam: enabled: true v0.8.0  Features  Support for two installation profiles, development (dev) and production (prod). The production profile, which is the default, provides a 3-node Elasticsearch and persistent storage for the Verrazzano Monitoring Instance (VMI). The development profile provides a single node Elasticsearch and no persistent storage for the VMI. The default behavior has been changed to use the system VMI for all monitoring (applications and Verrazzano components). It is still possible to customize one of the profiles to enable the original, non-shared VMI mode. Initial support for the Open Application Model (OAM).   Fixes  Updated Axios NPM package to v0.21.1 to resolve a security vulnerability in the examples code.    v.0.7.0   Features\n Ability to upgrade an existing Verrazzano installation. Added the Verrazzano Console. Enhanced the structure of the Verrazzano custom resource to allow more configurability. Streamlined the secret usage for OCI DNS installations.    Fixes\n Fixed bug where the Verrazzano CR Certificate.CA fields were being ignored. Removed secret used for hello-world; hello-world-application image is now public in ghcr so ImagePullSecrets is no longer needed. Fixed issue #339 (PRs #208 \u0026 #210.)    v0.6.0  Features  In-cluster installer which replaces client-side install scripts. Added installation profiles; in this release, there are two: production and development. Verrazzano system components now emit JSON structured logs.   Fixes  Updated Elasticsearch and Kibana versions (elasticsearch:7.6.1-20201130145440-5c76ab1) and (kibana:7.6.1-20201130145840-7717e73).    ","excerpt":"v0.11.0: Features\n OAM applications are optionally deployed into an Istio service mesh. Incremental improvements to user-facing roles.  Fixes\n Fixed issue with logging when an application has multiple …","ref":"/docs/reference/releasenotes/","title":"Release Notes"},{"body":"This guide describes how to move (“Lift-and-Shift”) an on-premises WebLogic Server domain to a cloud environment running Kubernetes using Verrazzano.\nOverview The Initial steps create a very simple on-premises domain that you will move to Kubernetes. The sample domain is the starting point for the lift and shift process; it contains one application (ToDo List) and one data source. First, you’ll configure the database and the WebLogic Server domain. Then, in Lift and Shift, you will move the domain to Kubernetes with Verrazzano. This guide does not include the setup of the networking that would be needed to access an on-premises database, nor does it document how to migrate a database to the cloud.\nWhat you need   The Git command-line tool and access to GitHub\n  MySQL Database 8.x - a database server\n  WebLogic Server 12.2.1.4.0 - an application server; Note that all WebLogic Server installers are supported except the Quick Installer.\n  Maven - to build the application\n  WebLogic Deploy Tooling (WDT) - v1.9.9 or later, to convert the WebLogic Server domain to and from metadata\n  WebLogic Image Tool (WIT) - v1.9.8 or later, to build the Docker image\n  Initial steps In the initial steps, you create a sample domain that represents your on-premises WebLogic Server domain.\nCreate a database using MySQL called tododb   Download the MySQL image from Docker Hub.\ndocker pull mysql:latest   Start the container database (and optionally mount a volume for data).\ndocker run --name tododb \\ -p 3306:3306 \\ -e MYSQL_USER=derek \\ -e MYSQL_PASSWORD=welcome1 \\ -e MYSQL_DATABASE=tododb \\ -e MYSQL_ROOT_PASSWORD=welcome1 \\ -d mysql:latest  NOTE You should use a more secure password.    Start a MySQL client to change the password algorithm to mysql_native_password.\n Assuming the database server is running, start a database CLI client: docker exec -it tododb mysql -uroot -p  When prompted for the password, enter the password for the root user, welcome1 or whatever password you set when starting the container in the previous step. After being connected, run the ALTER command at the MySQL prompt. ALTER USER 'derek'@'%' IDENTIFIED WITH mysql_native_password BY 'welcome1';   NOTE You should use a more secure password.    Create a WebLogic Server domain   If you do not have WebLogic Server 12.2.1.4.0 installed, install it now.\n  Choose the GENERIC installer from WebLogic Server Downloads and follow the documented installation instructions.\n  Be aware of these domain limitations:\n There are two supported domain types, single server and single cluster. Domains must use:  The default value AdminServer for AdminServerName. WebLogic Server listen port for the Administration Server: 7001. WebLogic Server listen port for the Managed Server: 8001. Note that these are all standard WebLogic Server default values.      Save the installer after you have finished; you will need it to build the Docker image.\n  To make copying commands easier, define an environment variable for ORACLE_HOME that points to the directory where you installed WebLogic Server 12.2.1.4.0. For example:\nexport ORACLE_HOME=$HOME/Oracle/Middleware/Oracle_Home     Use the Oracle WebLogic Server Configuration Wizard to create a domain called tododomain.\n Launch $ORACLE_HOME/oracle_common/common/bin/config.sh. Select Create a new domain. Specify a Domain Location of \u003coracle home\u003e/user_projects/domains/tododomain and click Next. Select the Basic WebLogic Server Domain [wlserver] template and click Next. Enter the password for the administrative user (the examples here assume a password of “welcome1”) and click Next. Accept the defaults for Domain Mode and JDK, and click Next. Select Administration Server and click Next. Ensure that the server name is AdminServer and click Next. Click Create. After it has completed, click Next, then Finish.    To start the newly created domain, run the domain’s start script.\n$ORACLE_HOME/user_projects/domains/tododomain/bin/startWebLogic.sh   Access the Console of the newly started domain with your browser, for example, http://localhost:7001/console, and log in using the administrator credentials you specified.\n  Add a data source configuration to access the database Using the WebLogic Server Administration Console, log in and add a data source configuration to access the MySQL database. During the data source configuration, you can accept the default values for most fields, but the following fields are required to match the application and database settings you used when you created the MySQL database.\n  In the left pane in the Console, expand Services and select Data Sources.\n  On the Summary of JDBC Data Sources page, click New and select Generic Data Source.\n  On the JDBC Data Sources page, enter or select the following information:\n Name: tododb JNDI Name: jdbc/ToDoDB Database Type: MySQL    Click Next and then click Next two more times.\n  On the Create a New JDBC Data Source page, enter the following information:\n Database Name: tododb Host name: localhost Database Port: 3306 Database User Name: derek Password: welcome1 (or whatever password you used) Confirm Password: welcome1    Click Next.\n  Select Test Configuration, and make sure you see “Connection Test Succeeded” in the Messages field of the Console.\n  Click Next.\n  On the Select Targets page, select AdminServer.\n  Click Finish to complete the configuration.\n  Build and deploy the application   Using Maven, build this project to produce todo.war.\nNOTE: You should clone this repo outside of $ORACLE_HOME or copy the WAR file to another location, as WDT may ignore it during the model creation phase.\ngit clone https://github.com/verrazzano/examples.git cd examples/todo-list/ mvn clean package   Using the WebLogic Server Administration Console, deploy the ToDo List application.\n In the left pane in the Console, select Deployments and click Install. Use the navigation links or provide the file path to todo.war typically \u003crepo\u003e/todo-list/target. For example, if you cloned the examples repository in your $HOME directory, the location should be $HOME/examples/examples/todo-list/target/todo.war. Click Next twice, then Finish.  NOTE: The remaining steps assume that the application context is todo.\n  Initialize the database After the application is deployed and running in WebLogic Server, access the http://localhost:7001/todo/rest/items/init REST service to create the database table used by the application. In addition to creating the application table, the init service also will load four sample items into the table.\nIf you get an error here, go back to the Select Targets page in the WebLogic Server Administration Console and make sure that you selected AdminServer as the data source target.\nAccess the application  Access the application at http://localhost:7001/todo/index.html.   Add a few entries or delete some. After verifying the application and database, you may shut down the local WebLogic Server domain.  Lift and Shift steps The following steps will move the sample domain to Kubernetes with Verrazzano.\nCreate a WDT Model  If you have not already done so, download v1.9.9 or later of WebLogic Deploy Tooling (WDT) from GitHub. Unzip the installer weblogic-deploy.zip file so that you can access bin/discoverDomain.sh. To make copying commands easier, define an environment variable for WDT_HOME that points to the directory where you installed WebLogic Deploy Tooling. export WDT_HOME=/install/directory   For example, to get the latest version:\ncurl -OL https://github.com/oracle/weblogic-deploy-tooling/releases/latest/download/weblogic-deploy.zip unzip weblogic-deploy.zip cd weblogic-deploy export WDT_HOME=$(pwd) To create a reusable model of the application and domain, use WDT to create a metadata model of the domain.\n First, create an output directory to hold the generated scripts and models. Then, run WDT discoverDomain. mkdir v8o $WDT_HOME/bin/discoverDomain.sh \\  -oracle_home $ORACLE_HOME \\  -domain_home /path/to/domain/dir \\  -model_file ./v8o/wdt-model.yaml \\  -archive_file ./v8o/wdt-archive.zip \\  -target vz \\  -output_dir v8o   You will find the following files in ./v8o:\n application.yaml - Verrazzano application configuration and component file; you can view a sample generated file here wdt-archive.zip - The WDT archive file containing the ToDo List application WAR file wdt-model.yaml - The WDT model of the WebLogic Server domain vz_variable.properties - A set of properties extracted from the WDT domain model create_k8s_secrets.sh - A helper script with kubectl commands to apply the Kubernetes secrets needed for this domain  NOTE: Due to a bug in WDT v1.9.9, you need to make the following edits (preferably using an editor like vi) to the generated application.yaml file:\n  Delete the line between the copyright headers and the first apiVersion.\n  Delete the empty clusters field from the tododomain Domain component.\n  If you chose to skip the Access the application step and did not verify that the ToDo List application was deployed, then you should verify that you see the todo.war file inside the wdt-archive.zip file. If you do not see the WAR file, there was something wrong in your deployment of the application on WebLogic Server that will require additional troubleshooting in your domain.\nCreate a Docker image At this point, the Verrazzano model is just a template for the real model. The WebLogic Image Tool will fill in the placeholders for you, or you can edit the model manually to set the image name and domain home directory.\n If you have not already done so, download WebLogic Image Tool (WIT) from GitHub. Unzip the installer imagetool.zip file so that you can access bin/imagetool.sh. To make copying commands easier, define an environment variable for WIT_HOME that points to the directory where you installed WebLogic Image Tool. export WIT_HOME=/install/directory   For example, to get the latest WIT tool:\ncurl -OL https://github.com/oracle/weblogic-image-tool/releases/latest/download/imagetool.zip unzip imagetool.zip cd imagetool export export WIT_HOME=$(pwd) You will need a Docker image to run your WebLogic Server domain in Kubernetes. To use WIT to create the Docker image, run imagetool create. Although WIT will download patches and PSUs for you, it does not yet download installers. Until then, you must download the WebLogic Server and Java Development Kit installer manually and provide their location to the imagetool cache addInstaller command.\n# The directory created previously to hold the generated scripts and models. cd v8o $WIT_HOME/bin/imagetool.sh cache addInstaller \\  --path /path/to/installer/jdk-8u231-linux-x64.tar.gz \\  --type jdk \\  --version 8u231 # The installer file name may be slightly different depending on # which version of the 12.2.1.4.0 installer that you downloaded, slim or generic. $WIT_HOME/bin/imagetool.sh cache addInstaller \\  --path /path/to/installer/fmw_12.2.1.4.0_wls_Disk1_1of1.zip \\  --type wls \\  --version 12.2.1.4.0 $WIT_HOME/bin/imagetool.sh cache addInstaller \\  --path /path/to/installer/weblogic-deploy.zip \\  --type wdt \\  --version latest # Paths for the files in this command assume that you are running it from the # v8o directory created during the `discoverDomain` step. $WIT_HOME/bin/imagetool.sh create \\  --tag your/repo/todo:1 \\  --version 12.2.1.4.0 \\  --jdkVersion 8u231 \\  --wdtModel ./wdt-model.yaml \\  --wdtArchive ./wdt-archive.zip \\  --wdtVariables ./vz_variable.properties \\  --resourceTemplates=./application.yaml \\  --wdtModelOnly The imagetool create command will have created a local Docker image and updated the Verrazzano model with the domain home and image name. Check your Docker images for the tag that you used in the create command using docker images from the Docker CLI.\nIf everything worked correctly, it is time to push that image to the container registry that Verrazzano will use to access the image from Kubernetes. You can use the Oracle Cloud Infrastructure Registry (OCIR) as your repository for this example, but most Docker compliant registries should work.\nThe variables in the application.yaml resource template should be resolved with information from the image tool build.\nVerify this by looking in the v8o/application.yaml file to make sure that the image: {{{imageName}}} value has been set with the given --tag value.\nPush the image to your repo.\nNOTE: The image name must be the same as what is in the application.yaml file under spec \u003e workload \u003e spec \u003e image for the tododomain-domain component.\ndocker push your/repo/todo:1 Deploy to Verrazzano After the application image has been created, there are several steps required to deploy a the application into a Verrazzano environment.\nThese include:\n Creating and labeling the tododomain namespace. Creating the necessary secrets required by the ToDo List application. Deploying MySQL to the tododomain namespace. Updating the application.yaml file to use the Verrazzano MySQL deployment and (optionally) expose the WLS Console. Applying the application.yaml file.  The following steps assume that you have a Kubernetes cluster and that Verrazzano is already installed in that cluster.\nLabel the namespace Create the tododomain namespace, and add a label to allow the WebLogic Server Kubernetes Operator to manage it.\nkubectl create namespace tododomain kubectl label namespace tododomain verrazzano-managed=true Create the required secrets If you haven’t already done so, edit and run the create_k8s_secrets.sh script to generate the Kubernetes secrets. WDT does not discover passwords from your existing domain. Before running the create secrets script, you will need to edit create_k8s_secrets.sh to set the passwords for the WebLogic Server domain and the data source. In this domain, there are a few passwords that you need to enter:\n Administrator credentials (for example, weblogic/welcome1) ToDo database credentials (for example, derek/welcome1) Runtime encryption secret (for example, welcome1)  For example:\n# Update \u003cadmin-user\u003e and \u003cadmin-password\u003e for weblogic-credentials create_paired_k8s_secret weblogic-credentials weblogic welcome1 # Update \u003cuser\u003e and \u003cpassword\u003e for tododomain-jdbc-tododb create_paired_k8s_secret jdbc-tododb derek welcome1 # Update \u003cpassword\u003e used to encrypt hashes create_k8s_secret runtime-encryption-secret welcome1 Then run the script:\nsh ./create_k8s_secrets.sh Verrazzano will need a credential to pull the image that you just created, so you need to create one more secret. The name for this credential can be changed in the component.yaml file to anything you like, but it defaults to tododomain-registry-credentials.\nAssuming that you leave the name tododomain-registry-credentials, you will need to run a kubectl create secret command similar to the following:\nkubectl create secret docker-registry tododomain-registry-credentials \\  --docker-server=phx.ocir.io \\  --docker-email=your.name@company.com \\  --docker-username=tenancy/username \\  --docker-password='passwordForUsername' \\  --namespace=tododomain Update the application configuration Update the generated application.yaml file for the todo application to:\n Update the tododomain-configmap component to use the in-cluster MySQL service URL jdbc:mysql://mysql.tododomain.svc.cluster.local:3306/tododb to access the database.  wdt_jdbc.yaml:| resources:JDBCSystemResource:'todo-ds':JdbcResource:JDBCDriverParams:# This is the URL of the database used by the WebLogic Server applicationURL:\"jdbc:mysql://mysql.tododomain.svc.cluster.local:3306/tododb\" (Optional) Add a path in the tododomain-domain IngressTrait to allow access to the WebLogic Server Administration Console.  # WLS console- path:\"/console\"pathType:PrefixThe file application-modified.yaml is an example of a modified application.yaml file. A diff of these two sample files is shown:\n$ diff application.yaml application-modified.yaml 27a28,30 \u003e # WLS console \u003e - path: \"/console\" \u003e pathType: Prefix 105c108 \u003c URL: \"jdbc:mysql://localhost:3306/tododb\" --- \u003e URL: \"jdbc:mysql://mysql.tododomain.svc.cluster.local:3306/tododb\" Deploy MySQL As noted previously, moving a production environment to Verrazzano would require migrating the data as well. While data migration is beyond the scope of this guide, we will still need to include a MySQL instance to be deployed with the application in the Verrazzano environment.\nTo do so, download the mysql-oam.yaml file.\nThen, apply the YAML file:\nkubectl apply -f mysql-oam.yaml Wait for the MySQL pod to reach the Ready state.\n$ kubectl get pod -n tododomain -w NAME READY STATUS RESTARTS AGE mysql-5cfd58477b-mg5c7 0/1 Pending 0 0s mysql-5cfd58477b-mg5c7 0/1 Pending 0 0s mysql-5cfd58477b-mg5c7 0/1 ContainerCreating 0 0s mysql-5cfd58477b-mg5c7 1/1 Running 0 2s Deploy the ToDo List application Finally, run kubectl apply to apply the Verrazzano component and Verrazzano application configuration files to start your domain.\nkubectl apply -f application.yaml This will:\n Create the application component resources for the ToDo List application. Create the application configuration resources that create the instance of the ToDo List application in the Verrazzano cluster.  Wait for the ToDo List example application to be ready.\n$ kubectl wait pod --for=condition=Ready tododomain-adminserver -n tododomain pod/tododomain-adminserver condition met Verify the pods are in the Running state:\n$ kubectl get pod -n tododomain NAME READY STATUS RESTARTS AGE mysql-55bb4c4565-c8zf5 1/1 Running 0 8m tododomain-adminserver 2/2 Running 0 5m Access the application from your browser   Get the EXTERNAL_IP address of the istio-ingressgateway service.\nkubectl get service istio-ingressgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.96.97.98 11.22.33.44 80:31380/TCP,443:31390/TCP 13d The IP address is listed in the EXTERNAL-IP column.\n  Add an entry to /etc/hosts for the application hostname for the ingress gateway external IP.\nTemporarily modify the /etc/hosts file (on Mac or Linux) or c:\\Windows\\System32\\Drivers\\etc\\hosts file (on Windows 10), to add an entry mapping todo.example.com to the ingress gateway’s EXTERNAL-IP address.\nFor example:\n11.22.33.44 tododomain-appconf.tododomain.example.com   Intialize the database by accessing the init URL.\n$ curl http://tododomain-appconf.tododomain.example.com/todo/rest/items/init ToDos table initialized.   Access the application in a browser at http://tododomain-appconf.tododomain.example.com/todo.\n  (Optional) Access the WebLogic Server Administration Console at http://tododomain-appconf.tododomain.example.com/console.\n  ","excerpt":"This guide describes how to move (“Lift-and-Shift”) an on-premises WebLogic Server domain to a cloud environment running Kubernetes using Verrazzano.\nOverview The Initial steps create a very simple …","ref":"/docs/guides/lift-and-shift/","title":"Lift-and-Shift Guide"},{"body":"Prepare for the OCI install   Create the OKE cluster using the OCI Console or some other means.\n  For SHAPE, an OKE cluster with 3 nodes of VM.Standard2.4 OCI compute instance shape has proven sufficient to install Verrazzano and deploy the Bob’s Books example application.\n  Set the following ENV variable:\n   export KUBECONFIG=\u003cpath to valid Kubernetes config\u003e  Create the optional imagePullSecret named verrazzano-container-registry. This step is required when one or more of the Docker images installed by Verrazzano are private. For example, while testing a change to the verrazzano-operator, you may be using a Docker image that requires credentials to access it.   kubectl create secret docker-registry verrazzano-container-registry \\ --docker-username=\u003cusername\u003e \\ --docker-password=\u003cpassword\u003e \\ --docker-server=\u003cdocker server\u003e Next steps To continue, see the Installation Guide.\n","excerpt":"Prepare for the OCI install   Create the OKE cluster using the OCI Console or some other means.\n  For SHAPE, an OKE cluster with 3 nodes of VM.Standard2.4 OCI compute instance shape has proven …","ref":"/docs/setup/platforms/oci/oci/","title":"Oracle Cloud Infrastructure (OCI)"},{"body":"","excerpt":"","ref":"/docs/setup/platforms/","title":"Platform Setup"},{"body":"","excerpt":"","ref":"/docs/setup/","title":"Setup"},{"body":"Known Issues OKE Missing Security List Ingress Rules The install scripts perform a check, which attempts access through the ingress ports. If the check fails, then the install will exit and you will see error messages like this:\nERROR: Port 443 is NOT accessible on ingress(132.145.66.80)! Check that security lists include an ingress rule for the node port 31739.\nOn an OKE install, this may indicate that there is a missing ingress rule or rules. To verify and fix the issue, do the following:\n Get the ports for the LoadBalancer services.  Run kubectl get services -A. Note the ports for the LoadBalancer type services. For example 80:31541/TCP,443:31739/TCP.   Check the security lists in the OCI Console.  Go to Networking/Virtual Cloud Networks. Select the related VCN. Go to the Security Lists for the VCN. Select the security list named oke-wkr-.... Check the ingress rules for the security list. There should be one rule for each of the destination ports named in the LoadBalancer services. In the above example, the destination ports are 31541 \u0026 31739. We would expect the ingress rule for 31739 to be missing because it was named in the ERROR output. If a rule is missing, then add it by clicking Add Ingress Rules and filling in the source CIDR and destination port range (missing port). Use the existing rules as a guide.    ","excerpt":"Known Issues OKE Missing Security List Ingress Rules The install scripts perform a check, which attempts access through the ingress ports. If the check fails, then the install will exit and you will …","ref":"/docs/reference/troubleshooting/","title":"Troubleshooting"},{"body":"Prepare for the OCLNE install Oracle Linux Cloud Native Environment can be installed in several different types of environments. These range from physical, on-premises hardware to virtualized cloud infrastructure. The Oracle Linux Cloud Native Environment installation instructions assume that networking and compute resources already exist. The basic infrastructure requirements are a network with a public and private subnet and a set of hosts connected to those networks.\nOCI example The following is an example of OCI infrastructure that can be used to evaluate Verrazzano installed on Oracle Linux Cloud Native Environment. If other environments are used, the capacity and configuration should be similar.\nYou can use the VCN Wizard of the OCI Console to automatically create most of the described network infrastructure. Additional security lists/rules, as detailed below, need to be added manually. All CIDR values provided are examples and can be customized as required.\nVirtual Cloud Network (for example, CIDR 10.0.0.0/16) Public Subnet (for example, CIDR 10.0.0.0/24)\nSecurity List / Ingress Rules\n   Stateless Destination Protocol Source Ports Destination Ports Type \u0026 Code Description     No 0.0.0.0/0 ICMP   3, 4 ICMP errors   No 10.0.0.0/16 ICMP   3 ICMP errors   No 0.0.0.0/0 TCP All 22  SSH   No 0.0.0.0/0 TCP All 80  HTTP load balancer   No 0.0.0.0/0 TCP All 443  HTTPS load balancer    Security List / Egress Rules\n   Stateless Destination Protocol Source Ports Destination Ports Type \u0026 Code Description     No 10.0.1.0/24 TCP All 22  SSH   No 10.0.1.0/24 TCP All 30080  HTTP load balancer   No 10.0.1.0/24 TCP All 30443  HTTPS load balancer   No 10.0.1.0/24 TCP All 31380  HTTP load balancer   No 10.0.1.0/24 TCP All 31390  HTTPS load balancer    Private Subnet (for example, CIDR 10.0.1.0/24)\nSecurity List / Ingress Rules\n   Stateless Destination Protocol Source Ports Destination Ports Type \u0026 Code Description     No 0.0.0.0/0 ICMP   3, 4 ICMP errors   No 10.0.0.0/16 ICMP   3 ICMP errors   No 10.0.0.0/16 TCP All 22  SSH   No 10.0.0.0/24 TCP All 30080  HTTP load balancer   No 10.0.0.0/24 TCP All 30443  HTTPS load balancer   No 10.0.0.0/24 TCP All 31380  HTTP load balancer   No 10.0.0.0/24 TCP All 31390  HTTPS load balancer   No 10.0.1.0/24UDP All 111  NFS    No 10.0.1.0/24 TCP All 111  NFS   No 10.0.1.0/24 UDP All 2048  NFS   No 10.0.1.0/24 TCP All 2048-2050  NFS   No 10.0.1.0/24 TCP All 2379-2380  Kubernetes etcd   No 10.0.1.0/24 TCP All 6443  Kubernetes API Server   No 10.0.1.0/24 TCP All 6446  MySQL   No 10.0.1.0/24 TCP All 8090-8091  OLCNE Platform Agent   No 10.0.1.0/24 UDP All 8472  Flannel   No 10.0.1.0/24 TCP All 10250-10255  Kubernetes Kublet    Security List / Egress Rules\n   Stateless Destination Protocol Source Ports Destination Ports Type and Code Description     No 10.0.0.0/0 TCP    All egress traffic    DHCP Options\n   DNS Type     Internet and VCN Resolver    Route Tables\nPublic Subnet Route Table Rules\n   Destination Target     0.0.0.0/0 Internet Gateway    Private Subnet Route Table Rules\n   Destination Target     0.0.0.0/0 NAT Gateway   All OCI Services Service Gateway    Internet Gateway\nNAT Gateway\nService Gateway\nThe following compute resources adhere to the guidelines provided in the Oracle Linux Cloud Native Environment Getting Started guide. The attributes indicated (for example, Subnet, RAM, Shape, and Image) are recommendations that have been tested. Other values can be used if required.\nCompute Instances\n   Role Subnet Suggested RAM Compatible VM Shape Compatible VM Image     SSH Jump Host Public 8GB VM.Standard.E2.1 Oracle Linux 7.8   OLCNE Operator Host Private 16GB VM.Standard.E2.2 Oracle Linux 7.8   Kubernetes Control Plane Node Private 32GB VM.Standard.E2.4 Oracle Linux 7.8   Kubernetes Worker Node 1 Private 32GB VM.Standard.E2.4 Oracle Linux 7.8   Kubernetes Worker Node 2 Private 32GB VM.Standard.E2.4 Oracle Linux 7.8   Kubernetes Worker Node 3 Private 32GB VM.Standard.E2.4 Oracle Linux 7.8    Do the OLCNE install Deploy Oracle Linux Cloud Native Environment with the Kubernetes module, following instructions from the Getting Started guide.\n Use a single Kubernetes control plane node. Skip the Kubernetes API load balancer (3.4.3). Use private CA certificates (3.5.3).  Prepare for the Verrazzano install A Verrazzano Oracle Linux Cloud Native Environment deployment requires:\n A default storage provider that supports “Multiple Read/Write” mounts. For example, an NFS service like:  Oracle Cloud Infrastructure File Storage Service. A hardware-based storage system that provides NFS capabilities.   Load balancers in front of the worker nodes in the cluster. DNS records that reference the load balancers.  Examples for meeting these requirements follow.\nPrerequisites Details  Storage Load Balancers DNS   Storage Verrazzano requires persistent storage for several components. This persistent storage is provided by a default storage class. A number of persistent storage providers exist for Kubernetes. This guide will focus on pre-allocated persistent volumes. In particular, the provided samples will illustrate the use of OCI’s NFS File System.\nOCI example Before storage can be exposed to Kubernetes, it must be created. In OCI, this is done using File System resources. Using the OCI Console, create a new File System. Within the new File System, create an Export. Remember the value used for Export Path as it will be used later. Also note the Mount Target’s IP Address for use later.\nAfter the exports have been created, referenced persistent volume folders (for example, /example/pv0001) will need to be created. In OCI, this can be done by mounting the export on one of the Kubernetes worker nodes and creating the folders. In the following example, the value /example is the Export Path and 10.0.1.8 is the Mount Target’s IP Address. The following command should be run on one of the Kubernetes worker nodes. This will result in the creation of nine persistent volume folders. The reason for nine persistent volume folders is covered in the next section.\nsudo mount 10.0.1.8:/example /mnt for x in {0001..0009}; do sudo mkdir -p /mnt/pv${x} \u0026\u0026 sudo chmod 777 /mnt/pv${x}; done Persistent Volumes A default Kubernetes storage class is required by Verrazzano. When using pre-allocated PersistentVolumes, for example NFS, persistent volumes should be declared as following. The value for name may be customized but will need to match the PersistentVolume storageClassName value later.\n Create a default StorageClass cat \u003c\u003c EOF | kubectl apply -f - apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: example-nfs annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer EOF  Create the required number of PersistentVolume resources. The Verrazzano system requires five persistent volumes for itself. The following command creates nine persistent volumes. The value for storageClassName must match the above StorageClass name. The values for name may be customized. The value for path must match the Export Path of the Export from above, combined with the persistent volume folder from above. The value for server must be changed to match the location of your file system server. for n in {0001..0009}; do cat \u003c\u003c EOF | kubectl apply -f - apiVersion: v1 kind: PersistentVolume metadata: name: pv${n} spec: storageClassName: example-nfs accessModes: - ReadWriteOnce - ReadWriteMany capacity: storage: 50Gi nfs: path: /example/pv${n} server: 10.0.1.8 volumeMode: Filesystem persistentVolumeReclaimPolicy: Recycle EOF    Load Balancers Verrazzano on Oracle Linux Cloud Native Environment uses external load balancer services. These will not automatically be provided by Verrazzano or Kubernetes. Two load balancers must be deployed outside of the subnet used for the Kubernetes cluster. One load balancer is for management traffic and the other for application traffic.\nSpecific steps will differ for each load balancer provider, but a generic configuration and an OCI example follow.\nGeneric configuration:  Target Host: Host names of Kubernetes worker nodes Target Ports: See table External Ports: See table Distribution: Round Robin Health Check: TCP     Traffic Type Service Name Type Suggested External Port Target Port     Application istio-ingressgateway TCP 80 31380   Application istio-ingressgateway TCP 443 31390   Management ingress-controller-nginx-ingress-controller TCP 80 30080   Management ingress-controller-nginx-ingress-controller TCP 443 30443    OCI example The following details can be used to create OCI load balancers for accessing application and management user interfaces, respectively. These load balancers will route HTTP/HTTPS traffic from the Internet to the private subnet. If load balancers are desired, then they should be created now even though the application and management endpoints will be installed later.\n Application Load Balancer: Public Subnet  Listeners  HTTP Listener: Protocol TCP, Port 80 HTTPS Listener: Protocol TCP, Port 443   Backend Sets  HTTP Backend Sets:  Health Check: Protocol TCP, Port 31380 Backends: Kubernetes Worker Nodes, Port 31380, Distribution Policy Weighted Round Robin   HTTPS Backend Sets  Health Check: Protocol TCP, Port 31390 Backends: Kubernetes Worker Nodes, Port 31390, Distribution Policy Weighted Round Robin       Management Load Balancer: Public Subnet  Listeners  HTTP Listener: Protocol TCP, Port 80 HTTPS Listener: Protocol TCP, Port 443   Backend Sets  HTTP Backend Sets:  Health Check: Protocol TCP, Port 30080 Backends: Kubernetes Worker Nodes, Port 30080, Distribution Policy Weighted Round Robin   HTTPS Backend Sets  Health Check: Protocol TCP, Port 30443 Backends: Kubernetes Worker Nodes, Port 30443, Distribution Policy Weighted Round Robin         DNS When using the spec.dns.external DNS type, the installer searches the DNS zone you provide for two specific A records. These are used to configure the cluster and should refer to external addresses of the load balancers in the previous step. The A records will need to be created manually.\nNOTE: At this time, the only supported deployment for Oracle Linux Cloud Native Environment is the external DNS type.\n   Record Use     ingress-mgmt Set as the .spec.externalIPs value of the ingress-controller-nginx-ingress-controller service   ingress-verrazzano Set as the .spec.externalIPs value of the istio-ingressgateway service    For example:\n198.51.100.10 A ingress-mgmt.myenv.mydomain.com. 203.0.113.10 A ingress-verrazzano.myenv.mydomain.com. Verrazzano installation will result in a number of management services that need to point to the ingress-mgmt address.\nkeycloak.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. rancher.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. grafana.vmi.system.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. prometheus.vmi.system.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. kibana.vmi.system.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. elasticsearch.vmi.system.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. For simplicity, an administrator may want to create wildcard DNS records for the management addresses:\n*.system.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. OR\n*.myenv.mydomain.com CNAME ingress-mgmt.myenv.mydomain.com. OCI example DNS is configured in OCI by creating DNS zones in the OCI Console. When creating a DNS zone, use these values.\n Method: Manual Zone Name: \u003cdns-suffix\u003e Zone Type: Primary  The value for \u003cdns-suffix\u003e excludes the environment (for example, use the mydomain.com portion of myenv.mydomain.com).\nDNS A records must be manually added to the zone and published using values described above. DNS CNAME records, in the same way.\n  During the Verrazzano install, these steps should be performed on the Oracle Linux Cloud Native Environment operator node.\nEdit the sample Verrazzano custom resource install-olcne.yaml file and provide the configuration settings for your OLCNE environment as follows:\n The value for spec.environmentName is a unique DNS subdomain for the cluster (for example, myenv in myenv.mydomain.com). The value for spec.dns.external.suffix is the remainder of the DNS domain (for example, mydomain.com in myenv.mydomain.com). Under spec.ingress.verrazzano.nginxInstallArgs, the value for controller.service.externalIPs is the IP address of ingress-mgmt.\u003cmyenv\u003e.\u003cmydomain.com\u003e configured during DNS set up. Under spec.ingress.application.istioInstallArgs, the value for gateways.istio-ingressgateway.externalIPs is the IP address of ingress-verrazzano.\u003cmyenv\u003e.\u003cmydomain.com\u003e configured during DNS set up.  You will install Verrazzano using the external DNS type (the example custom resource for OLCNE is already configured to use spec.dns.external).\nSet the following environment variable:\nThe value for \u003cpath to valid Kubernetes config\u003e is typically ${HOME}/.kube/config\nexport KUBECONFIG=$VERRAZZANO_KUBECONFIG Next steps To continue, see the Installation Guide.\n","excerpt":"Prepare for the OCLNE install Oracle Linux Cloud Native Environment can be installed in several different types of environments. These range from physical, on-premises hardware to virtualized cloud …","ref":"/docs/setup/platforms/olcne/olcne/","title":"Oracle Linux Cloud Native Environment (OLCNE)"},{"body":"KIND is a tool for running local Kubernetes clusters using Docker container “nodes”. Follow these instructions to prepare a KIND cluster for running Verrazzano.\nNOTE KIND is not recommended for use on macOS and Windows because the Docker network is not directly exposed to the host. On macOS and Windows, minikube is recommended.  Prerequisites  Install Docker. Install KIND.  Prepare the KIND cluster To prepare the KIND cluster for use with Verrazzano, you must create the cluster and then install and configure MetalLB in that cluster.\nCreate the KIND cluster KIND images are prebuilt for each release. To find images suitable for a given release, check the release notes for your KIND version (check with kind version) where you’ll find a complete listing of images created for a KIND release.\nThe following example references a Kubernetes v1.18.8-based image built for KIND v0.9.0. Replace that image with one suitable for the KIND release you are using.\nkind create cluster --config - \u003c\u003cEOF kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane image: kindest/node:v1.18.8@sha256:f4bcc97a0ad6e7abaf3f643d890add7efe6ee4ab90baeb374b4f41a4c95567eb kubeadmConfigPatches: - | kind: ClusterConfiguration apiServer: extraArgs: \"service-account-issuer\": \"kubernetes.default.svc\" \"service-account-signing-key-file\": \"/etc/kubernetes/pki/sa.key\" EOF Install and configure MetalLB By default, KIND does not provide an implementation of network load balancers (Services of type LoadBalancer). MetalLB offers a network load balancer implementation.\nTo install MetalLB:\nkubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/namespace.yaml kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/metallb.yaml kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=\"$(openssl rand -base64 128)\" For further details, see the MetalLB installation guide.\nMetalLB is idle until configured. Configure MetalLB in Layer 2 mode and give it control over a range of IP addresses in the kind Docker network. In versions v0.7.0 and earlier, KIND uses Docker’s default bridge network; in versions v0.8.0 and later, it creates its own bridge network in KIND.\nTo determine the subnet of the kind Docker network in KIND v0.8.0 and later:\n\u003e docker inspect kind | jq '.[0].IPAM.Config[0].Subnet' -r 172.18.0.0/16 To determine the subnet of the kind Docker network in KIND v0.7.0 and earlier:\n\u003e docker inspect bridge | jq '.[0].IPAM.Config[0].Subnet' -r 172.17.0.0/16 For use by MetalLB, assign a range of IP addresses at the end of the kind network’s subnet CIDR range.\nkubectl apply -f - \u003c\u003c-EOF apiVersion: v1 kind: ConfigMap metadata: namespace: metallb-system name: config data: config: | address-pools: - name: my-ip-space protocol: layer2 addresses: - 172.18.0.230-172.18.0.250 EOF Image caching to speed up install If you are experimenting with Verrazzano and expect that you may need to delete the KIND cluster and later, install Verrazzano again on a new KIND cluster, then you can follow these steps to ensure that the image cache used by containerd inside KIND is preserved across clusters. Subsequent installs will be faster than the first install, because they will not need to pull the images again.\n1. Create a named Docker volume that will be used for the image cache, and note its Mountpoint path. In this example, the volume is named containerd.\ndocker volume create containerd docker volume inspect containerd #Sample output is shown { \"CreatedAt\": \"2021-01-11T16:27:47Z\", \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/containerd/_data\", \"Name\": \"containerd\", \"Options\": {}, \"Scope\": \"local\" } 2. Specify the Mountpoint path obtained, as the hostPath under extraMounts in your KIND configuration file, with a containerPath of /var/lib/containerd, which is the default containerd image caching location inside the KIND container. An example of the modified KIND configuration is shown in the following create cluster command:\nkind create cluster --config - \u003c\u003cEOF kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane image: kindest/node:v1.18.8@sha256:f4bcc97a0ad6e7abaf3f643d890add7efe6ee4ab90baeb374b4f41a4c95567eb kubeadmConfigPatches: - | kind: ClusterConfiguration apiServer: extraArgs: \"service-account-issuer\": \"kubernetes.default.svc\" \"service-account-signing-key-file\": \"/etc/kubernetes/pki/sa.key\" extraMounts: - hostPath: /var/lib/docker/volumes/containerd/_data containerPath: /var/lib/containerd #This is the location of the image cache inside the KIND container EOF Next steps To continue, see the Installation Guide.\n","excerpt":"KIND is a tool for running local Kubernetes clusters using Docker container “nodes”. Follow these instructions to prepare a KIND cluster for running Verrazzano.\nNOTE KIND is not recommended for use on …","ref":"/docs/setup/platforms/kind/kind/","title":"KIND"},{"body":"minikube quickly sets up a local Kubernetes cluster on macOS, Linux, and Windows. Follow these instructions to prepare a minikube cluster for running Verrazzano.\nPrerequisites  Install minikube. Install a driver (on macOS or Windows, select a VM based driver, not Docker).  Prepare the minikube cluster To prepare the minikube cluster for use with Verrazzano, you must create the cluster and then expose services of type LoadBalancer by using the minikube tunnel command.\nCreate minikube cluster Create a minikube cluster using a supported Kubernetes version and appropriate driver. On Linux hosts, the default driver is acceptable, on macOS, hyperkit is recommended.\nminikube start \\  --kubernetes-version=v1.18.8 \\  --driver=hyperkit \\  --memory=16384 \\  --cpus=4 \\  --extra-config=apiserver.service-account-signing-key-file=/var/lib/minikube/certs/sa.key \\  --extra-config=apiserver.service-account-issuer=kubernetes/serviceaccount \\  --extra-config=apiserver.service-account-api-audiences=api Run minikube tunnel minikube exposes Kubernetes services of type LoadBalancer with the minikube tunnel command. Run a tunnel in a separate terminal from minikube:\nminikube tunnel Next steps To continue, see the Installation Guide.\n","excerpt":"minikube quickly sets up a local Kubernetes cluster on macOS, Linux, and Windows. Follow these instructions to prepare a minikube cluster for running Verrazzano.\nPrerequisites  Install minikube. …","ref":"/docs/setup/platforms/minikube/minikube/","title":"minikube"},{"body":" NOTE You should install this developer preview release of Verrazzano only in a cluster that can be safely deleted when your evaluation is complete.  Prerequisites Verrazzano requires the following:\n A Kubernetes cluster and a compatible kubectl. At least 2 CPUs, 100GB disk storage, and 16GB RAM available on the Kubernetes worker nodes.  NOTE: Verrazzano has been tested only on the following versions of Kubernetes: 1.17.x and 1.18.x. Other versions have not been tested and are not guaranteed to work.\nPrepare for the install Before installing Verrazzano, see instructions on preparing the following Kubernetes platforms:\n  OCI Container Engine for Kubernetes\n  OLCNE\n  KIND\n  minikube\n  Generic Kubernetes\n  The following instructions show you how to install Verrazzano in a single Oracle Cloud Infrastructure Container Engine for Kubernetes (OKE) cluster.\nInstall the Verrazzano platform operator Verrazzano provides a platform operator to manage the life cycle of Verrazzano installations. You can install, uninstall, and update Verrazzano installations by updating the Verrazzano custom resource.\nTo install the Verrazzano platform operator, follow these steps:\n  Deploy the Verrazzano platform operator.\nkubectl apply -f https://github.com/verrazzano/verrazzano/releases/latest/download/operator.yaml   Wait for the deployment to complete.\n$ kubectl -n verrazzano-install rollout status deployment/verrazzano-platform-operator deployment \"verrazzano-platform-operator\" successfully rolled out   Confirm that the operator pod is correctly defined and running.\n$ kubectl -n verrazzano-install get pods NAME READY STATUS RESTARTS AGE verrazzano-platform-operator-59d5c585fd-lwhsx 1/1 Running 0 114s   Perform the install For a complete description of Verrazzano configuration options, see the Verrazzano Custom Resource Definition.\nAccording to your DNS choice, xip.io or Oracle OCI DNS, install Verrazzano using one of the following methods.\n xip.io OCI DNS   Install using xip.io\nRun the following commands:\nkubectl apply -f - \u003c\u003cEOF apiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: my-verrazzano EOF kubectl wait --timeout=20m --for=condition=InstallComplete verrazzano/my-verrazzano   Install using OCI DNS\nPrerequisites\n  A DNS zone is a distinct portion of a domain namespace. Therefore, ensure that the zone is appropriately associated with a parent domain. For example, an appropriate zone name for parent domain v8o.example.com domain is us.v8o.example.com.\n  Create an OCI DNS zone using the OCI Console or the OCI CLI.\nCLI example:\noci dns zone create -c \u003ccompartment ocid\u003e --name \u003czone-name-prefix\u003e.v8o.example.com --zone-type PRIMARY   Installation\nInstalling Verrazzano on OCI DNS requires some configuration settings to create DNS records. Edit the Verrazzano custom resource and provide values for the following configuration settings:\n spec.environmentName spec.certificate.acme.emailAddress spec.dns.oci.ociConfigSecret spec.dns.oci.dnsZoneCompartmentOCID spec.dns.oci.dnsZoneOCID spec.dns.oci.dnsZoneName  For the full configuration information for an installation, see the Verrazzano Custom Resource Definition.\nWhen you use the OCI DNS installation, you need to provide a Verrazzano name in the Verrazzano custom resource (spec.environmentName) that will be used as part of the domain name used to access Verrazzano ingresses. For example, you could use sales as an environmentName, yielding sales.us.v8o.example.com as the sales-related domain (assuming the domain and zone names listed previously).\nRun the following commands:\nkubectl apply -f - \u003c\u003cEOF apiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: my-verrazzano spec: environmentName: env profile: prod components: certManager: certificate: acme: provider: letsEncrypt emailAddress: emailAddress@domain.com dns: oci: ociConfigSecret: ociConfigSecret dnsZoneCompartmentOCID: dnsZoneCompartmentOcid dnsZoneOCID: dnsZoneOcid dnsZoneName: my.dns.zone.name ingress: type: LoadBalancer EOF kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/master/operator/config/samples/install-oci.yaml kubectl wait --timeout=20m --for=condition=InstallComplete verrazzano/my-verrazzano    To monitor the console log output of the installation, run the following command:\nkubectl logs -f $(kubectl get pod -l job-name=verrazzano-install-my-verrazzano -o jsonpath=\"{.items[0].metadata.name}\") Verify the install Verrazzano installs multiple objects in multiple namespaces. In the verrazzano-system namespaces, all the pods in the Running state, does not guarantee, but likely indicates that Verrazzano is up and running.\nkubectl get pods -n verrazzano-system verrazzano-admission-controller-84d6bc647c-7b8tl 1/1 Running 0 5m13s verrazzano-cluster-operator-57fb95fc99-kqjll 1/1 Running 0 5m13s verrazzano-monitoring-operator-7cb5947f4c-x9kfc 1/1 Running 0 5m13s verrazzano-operator-b6d95b4c4-sxprv 1/1 Running 0 5m13s vmi-system-api-7c8654dc76-2bdll 1/1 Running 0 4m44s vmi-system-es-data-0-6679cf99f4-9p25f 2/2 Running 0 4m44s vmi-system-es-data-1-8588867569-zlwwx 2/2 Running 0 4m44s vmi-system-es-ingest-78f6dfddfc-2v5nc 1/1 Running 0 4m44s vmi-system-es-master-0 1/1 Running 0 4m44s vmi-system-es-master-1 1/1 Running 0 4m44s vmi-system-es-master-2 1/1 Running 0 4m44s vmi-system-grafana-5f7bc8b676-xx49f 1/1 Running 0 4m44s vmi-system-kibana-649466fcf8-4n8ct 1/1 Running 0 4m44s vmi-system-prometheus-0-7f97ff97dc-gfclv 3/3 Running 0 4m44s vmi-system-prometheus-gw-7cb9df774-48g4b 1/1 Running 0 4m44s Installation profiles Verrazzano supports two installation profiles: development (dev) and production (prod). The production profile, which is the default, provides a 3-node Elasticsearch and persistent storage for the Verrazzano Monitoring Instance (VMI). The development profile provides a single node Elasticsearch and no persistent storage for the VMI.\nTo use the development profile, specify the following in the Kubernetes manifest file for the Verrazzano custom resource:\nspec: profile: dev The install-dev.yaml file provides a template for a dev profile installation.\n(Optional) Install the example applications Example applications are located here.\nTo get the consoles URLs and credentials, see Operations. Uninstall Verrazzano To delete a Verrazzano installation, run the following commands:\n# Get the name of the Verrazzano custom resource kubectl get verrazzano # Delete the Verrazzano custom resource kubectl delete verrazzano \u003cname of custom resource\u003e To monitor the console log of the uninstall, run the following command:\nkubectl logs -f $(kubectl get pod -l job-name=verrazzano-uninstall-my-verrazzano -o jsonpath=\"{.items[0].metadata.name}\") ","excerpt":" NOTE You should install this developer preview release of Verrazzano only in a cluster that can be safely deleted when your evaluation is complete.  Prerequisites Verrazzano requires the following: …","ref":"/docs/setup/install/installation/","title":"Installation Guide"},{"body":"Upgrading an existing Verrazzano installation involves:\n Upgrading the Verrazzano platform operator to the Verrazzano release version to which you want to upgrade. Updating the version of your installed Verrazzano resource to the version supported by the upgraded operator.  Performing an upgrade will upgrade only the Verrazzano components related to the existing installation. Upgrading will not have any impact on running applications.\nNOTE: You may only change the version field during an upgrade; changes to other fields or component configurations are not supported at this time.\nUpgrade the Verrazzano platform operator In order to upgrade an existing Verrazzano installation, you must first upgrade the Verrazzano platform operator.\n  Upgrade the Verrazzano platform operator.\nTo upgrade to the latest version:\nkubectl apply -f https://github.com/verrazzano/verrazzano/releases/latest/download/operator.yaml To upgrade to a specific version:\nkubectl apply -f https://github.com/verrazzano/verrazzano/releases/download/\u003cversion\u003e/operator.yaml where \u003cversion\u003e is the desired version. For example:\nkubectl apply -f https://github.com/verrazzano/verrazzano/releases/download/v0.7.0/operator.yaml   Wait for the deployment to complete.\n$ kubectl -n verrazzano-install rollout status deployment/verrazzano-platform-operator deployment \"verrazzano-platform-operator\" successfully rolled out   Confirm that the operator pod is correctly defined and running.\n$ kubectl -n verrazzano-install get pods NAME READY STATUS RESTARTS AGE verrazzano-platform-operator-59d5c585fd-lwhsx 1/1 Running 0 114s   Upgrade Verrazzano To upgrade Verrazzano:\n  Update the Verrazzano resource to the desired version.\nTo upgrade the Verrazzano components, you must update the version field in your Verrazzano resource spec to match the version supported by the platform operator to which you upgraded and apply it to the cluster.\nThe value of the version field in the resource spec must be a Semantic Versioning value corresponding to a valid Verrazzano release version.\nYou can update the resource by doing one of the following:\na. Editing the YAML file you used to install Verrazzano and setting the version field to the latest version.\nFor example, to upgrade to v0.7.0, your YAML file should be edited to add or update the version field:\napiVersion:install.verrazzano.io/v1alpha1kind:Verrazzanometadata:name:my-verrazzanospec:profile:devversion:v0.7.0Then apply the resource to the cluster (if you have not edited the resource in-place using kubectl edit):\nkubectl apply -f my-verrazzano.yaml b. Editing the Verrazzano resource directly using kubectl and setting the version field directly, for example:\nkubectl edit verrazzano my-verrazzano # Once in the resource editor, add or update the version field to \"version: v0.7.0\", then save.   Wait for the upgrade to complete:\nkubectl wait --timeout=20m --for=condition=UpgradeComplete verrazzano/my-verrazzano   Verify the upgrade Check that all the pods in the verrazzano-system namespace are in the Running state. While the upgrade is in progress, you may see some pods terminating and restarting as newer versions of components are applied.\nFor example:\nkubectl get pods -n verrazzano-system verrazzano-admission-controller-84d6bc647c-7b8tl 1/1 Running 0 5m13s verrazzano-cluster-operator-57fb95fc99-kqjll 1/1 Running 0 5m13s verrazzano-monitoring-operator-7cb5947f4c-x9kfc 1/1 Running 0 5m13s verrazzano-operator-b6d95b4c4-sxprv 1/1 Running 0 5m13s vmi-system-api-7c8654dc76-2bdll 1/1 Running 0 4m44s vmi-system-es-data-0-6679cf99f4-9p25f 2/2 Running 0 4m44s vmi-system-es-data-1-8588867569-zlwwx 2/2 Running 0 4m44s vmi-system-es-ingest-78f6dfddfc-2v5nc 1/1 Running 0 4m44s vmi-system-es-master-0 1/1 Running 0 4m44s vmi-system-es-master-1 1/1 Running 0 4m44s vmi-system-es-master-2 1/1 Running 0 4m44s vmi-system-grafana-5f7bc8b676-xx49f 1/1 Running 0 4m44s vmi-system-kibana-649466fcf8-4n8ct 1/1 Running 0 4m44s vmi-system-prometheus-0-7f97ff97dc-gfclv 3/3 Running 0 4m44s vmi-system-prometheus-gw-7cb9df774-48g4b 1/1 Running 0 4m44s ","excerpt":"Upgrading an existing Verrazzano installation involves:\n Upgrading the Verrazzano platform operator to the Verrazzano release version to which you want to upgrade. Updating the version of your …","ref":"/docs/setup/upgrade/upgrade/","title":"Upgrade Guide"},{"body":"Prepare for the generic install To use a generic Kubernetes implementation, there are two main areas you can configure: ingress and storage.\n Ingress Storage    You can achieve ingress configuration using Helm overrides. For example, to use the nginx-controller for ingress on KIND, apply the following customization to the Verrazzano CRD.\nspec: components: ingress: nginxInstallArgs: - name: controller.kind value: DaemonSet - name: controller.hostPort.enabled value: \"true\" - name: controller.nodeSelector.ingress-ready value: \"true\" setString: true - name: controller.tolerations[0].key value: node-role.kubernetes.io/master - name: controller.tolerations[0].operator value: Equal - name: controller.tolerations[0].effect value: NoSchedule   By default, each Verrazzano install profile has different storage characteristics. Some components have external storage requirements (expressed through PersistentVolumeClaim declarations in their resources/helm charts):\n MySQL Elasticsearch Prometheus Grafana  By default, the prod profile uses 50Gi persistent volumes for each of the above services, using the default storage class for the target Kubernetes platform. The dev profile uses ephemeral emptyDir storage by default. However, you can customize these storage settings within a profile as desired.\nTo override these settings, customize the Verrazzano install resource by defining a VolumeSource on the defaultVolumeSource field in the install CR, which can be one of:\n emptyDir persistentVolumeClaim  Configuring emptyDir for the defaultVolumeSource forces all persistent volumes created by Verrazzano components in an installation to use ephemeral storage unless otherwise overridden. This can be useful for development or test scenarios.\nYou can use a persistentVolumeClaim to identify a volumeClaimSpecTemplate in the volumeClaimSpecTemplates section via the claimSource field. A volumeClaimSpecTemplate is a named PersistentVolumeClaimSpec configuration. A volumeClaimSpecTemplate can be referenced from more than one component; it merely identifies configuration settings and does not result in a direct instantiation of a persistent volume. The settings are used by referencing components when creating their PersistentVolumeClaims at install time.\nIf the component supports it, then you can override the defaultVolumeSource setting at the component level by defining a supported VolumeSource on that component. At present, only the keycloak/mysql component supports a volumeSource field override.\nExamples The following example shows how to define a dev profile with different persistence settings for the monitoring components and the Keycloak/MySQL instance.\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: kind-verrazzano-with-persistence spec: profile: dev defaultVolumeSource: persistentVolumeClaim: claimName: default # Use the \"default\" volume template components: keycloak: mysql: volumeSource: persistentVolumeClaim: claimName: mysql # Use the \"mysql\" PVC template for the MySQL volume configuration volumeClaimSpecTemplates: - metadata: name: default # \"default\" is a known template name, and will be used by Verrazzano components by default if no other template is referenced explicitly spec: resources: requests: storage: 2Gi - metadata: spec: resources: requests: storage: 5Gi # default The following example shows how to define a dev profile where all resources use emptyDir by default.\napiVersion: install.verrazzano.io/v1alpha1 kind: Verrazzano metadata: name: storage-example-dev spec: profile: dev defaultVolumeSource: emptyDir: {} # Use ephemeral storage for dev mode for all Components    Next steps To continue, see the Installation Guide.\n","excerpt":"Prepare for the generic install To use a generic Kubernetes implementation, there are two main areas you can configure: ingress and storage.\n Ingress Storage    You can achieve ingress configuration …","ref":"/docs/setup/platforms/generic/generic/","title":"Generic Kubernetes"},{"body":"Verrazzano is an end-to-end enterprise container platform for deploying cloud-native and traditional applications in multi-cloud and hybrid environments. It is made up of a curated set of open source components – many that you may already use and trust, and some that were written specifically to pull together all of the pieces that make Verrazzano a cohesive and easy to use platform.\nVerrazzano includes the following capabilities:\n Hybrid and multi-cluster workload management Special handling for WebLogic, Coherence, and Helidon applications Multi-cluster infrastructure management Integrated and pre-wired application monitoring Integrated security DevOps and GitOps enablement  NOTE This is a developer preview release of Verrazzano. You should install Verrazzano only in a Kubernetes cluster that can be safely deleted when your evaluation is complete.  Select Quick Start to get started.\nVerrazzano release versions and source code are available at https://github.com/verrazzano/verrazzano. This repository contains a Kubernetes operator for installing Verrazzano and example applications for use with Verrazzano.\n","excerpt":"Verrazzano is an end-to-end enterprise container platform for deploying cloud-native and traditional applications in multi-cloud and hybrid environments. It is made up of a curated set of open source …","ref":"/docs/","title":"Welcome to Verrazzano"},{"body":"","excerpt":"","ref":"/docs/reference/api/","title":"API"},{"body":"Get the consoles URLs Verrazzano installs several consoles. Get the ingress for the consoles with the following command:\nkubectl get ingress -A\nTo get the URL, prefix https:// to the host name returned. For example https://rancher.myenv.mydomain.com\nThe following is an example of the ingresses:\n NAMESPACE NAME HOSTS ADDRESS PORTS AGE cattle-system rancher rancher.myenv.mydomain.com 128.234.33.198 80, 443 93m keycloak keycloak keycloak.myenv.mydomain.com 128.234.33.198 80, 443 69m verrazzano-system verrazzano-operator-ingress api.myenv.mydomain.com 128.234.33.198 80, 443 81m verrazzano-system vmi-system-api api.vmi.system.myenv.mydomain.com 128.234.33.198 80, 443 80m verrazzano-system vmi-system-es-ingest elasticsearch.vmi.system.myenv.mydomain.com 128.234.33.198 80, 443 80m verrazzano-system vmi-system-grafana grafana.vmi.system.myenv.mydomain.com 128.234.33.198 80, 443 80m verrazzano-system vmi-system-kibana kibana.vmi.system.myenv.mydomain.com 128.234.33.198 80, 443 80m verrazzano-system vmi-system-prometheus prometheus.vmi.system.myenv.mydomain.com 128.234.33.198 80, 443 80m verrazzano-system vmi-system-prometheus-gw prometheus-gw.vmi.system.myenv.mydomain.com 128.234.33.198 80, 443 80m Get console credentials You will need the credentials to access the consoles installed by Verrazzano.\nConsoles accessed by the same user name/password  Grafana Prometheus Kibana Elasticsearch  User: verrazzano\nTo get the password:\nkubectl get secret --namespace verrazzano-system verrazzano -o jsonpath={.data.password} | base64 --decode; echo\nThe Keycloak admin console User: keycloakadmin\nTo get the password:\nkubectl get secret --namespace keycloak keycloak-http -o jsonpath={.data.password} | base64 --decode; echo\nThe Rancher console User: admin\nTo get the password:\nkubectl get secret --namespace cattle-system rancher-admin-secret -o jsonpath={.data.password} | base64 --decode; echo\n","excerpt":"Get the consoles URLs Verrazzano installs several consoles. Get the ingress for the consoles with the following command:\nkubectl get ingress -A\nTo get the URL, prefix https:// to the host name …","ref":"/docs/operations/","title":"Operations"},{"body":"","excerpt":"","ref":"/docs/guides/","title":"Guides"},{"body":"","excerpt":"","ref":"/docs/reference/","title":"Reference"},{"body":"","excerpt":"","ref":"/search/","title":"Search Results"},{"body":"  #td-cover-block-0 { background-image: url(/featured-background_hu3fcc2fb45a2768e8c72dcbce1f48d58b_164050_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/featured-background_hu3fcc2fb45a2768e8c72dcbce1f48d58b_164050_1920x1080_fill_q75_catmullrom_top.jpg); } }  Verrazzano Enterprise Container Platform Learn More  View Repository   A hybrid multi-cloud Kubernetes based Enterprise Container Platform for running both cloud-native and traditional applications.\n\n          Application Lifecycle Management Use Open Application Model constructs to describe, deploy, and update multi-component application systems across clusters in multiple clouds, including clusters on premises.\n   Integrated Monitoring Verrazzano can provision a full monitoring stack for your application, including Elasticsearch, Kibana, Prometheus and Grafana. Your applications are automatically wired up to send logs and metrics into the monitoring tools.\n   Security Built in security with encryption, certificate management, authentication and authorization services and network policies.\n       Contributions welcome! We do a Pull Request contributions workflow on GitHub. New users are always welcome!\nRead more …\n   Connect with us on Slack! For project discussions.\nRead more …\n   Follow us on Twitter! For announcement of latest features etc.\nRead more …\n    ","excerpt":"  #td-cover-block-0 { background-image: url(/featured-background_hu3fcc2fb45a2768e8c72dcbce1f48d58b_164050_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { …","ref":"/","title":"Verrazzano Enterprise Container Platform"}]